<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.1.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/avatar.jpg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/avatar.jpg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/avatar.jpg">
  <link rel="mask-icon" href="/images/avatar.jpg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"right","display":"always","padding":18,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"default"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":true,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":false,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="智能客服项目智能客服项目主要有三个模块FAQ、任务型对话和闲聊，用户输入一个句子后是进入FAQ、任务型对话还是闲聊，是由一个大的顶层分类器来决定。 FAQ和闲聊都是单轮对话，所以用户后面的输入都会重新进入到顶层分类器再次进行分类。 任务型对话则是多轮的，如果一个句子被顶层分类器划分到任务型对话中，那么在这个任务结束以前，用户后面的输入都会不经过顶层分类器而直接进入到任务型对话中。判断任务型对话是否">
<meta property="og:type" content="article">
<meta property="og:title" content="智能客服项目">
<meta property="og:url" content="http://yoursite.com/2019/12/08/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/index.html">
<meta property="og:site_name" content="panna cotta">
<meta property="og:description" content="智能客服项目智能客服项目主要有三个模块FAQ、任务型对话和闲聊，用户输入一个句子后是进入FAQ、任务型对话还是闲聊，是由一个大的顶层分类器来决定。 FAQ和闲聊都是单轮对话，所以用户后面的输入都会重新进入到顶层分类器再次进行分类。 任务型对话则是多轮的，如果一个句子被顶层分类器划分到任务型对话中，那么在这个任务结束以前，用户后面的输入都会不经过顶层分类器而直接进入到任务型对话中。判断任务型对话是否">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://yoursite.com/public/images/image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-R2.png">
<meta property="og:image" content="http://yoursite.com/public/images/image/SVM%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0.svg">
<meta property="og:image" content="http://yoursite.com/public/images/image/BiLSTM+CRF&#32;loss.png">
<meta property="og:image" content="http://yoursite.com/public/images/image/self-attention.png">
<meta property="og:image" content="http://yoursite.com/public/images/image/self-attention-2.png">
<meta property="article:published_time" content="2019-12-07T16:00:00.000Z">
<meta property="article:modified_time" content="2022-07-11T03:40:07.156Z">
<meta property="article:author" content="Shijiao DENG">
<meta property="article:tag" content="智能客服">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="stacking">
<meta property="article:tag" content="LR">
<meta property="article:tag" content="SVM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/public/images/image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-R2.png">

<link rel="canonical" href="http://yoursite.com/2019/12/08/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>智能客服项目 | panna cotta</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">panna cotta</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">诗娇的博客</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-news">

    <a href="/news/" rel="section"><i class="fa fa-bullhorn fa-fw"></i>news</a>

  </li>
        
            
  <li class="menu-item menu-item-docs">

    <a href="/docs/" rel="section"><i class="fa fa-book fa-fw"></i>docs</a>

  </li>


      
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/12/08/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="Shijiao DENG">
      <meta itemprop="description" content="记录工作，记录生活">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="panna cotta">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          智能客服项目
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2019-12-08 00:00:00" itemprop="dateCreated datePublished" datetime="2019-12-08T00:00:00+08:00">2019-12-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2022-07-11 11:40:07" itemprop="dateModified" datetime="2022-07-11T11:40:07+08:00">2022-07-11</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/" itemprop="url" rel="index"><span itemprop="name">项目总结</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="智能客服项目"><a href="#智能客服项目" class="headerlink" title="智能客服项目"></a>智能客服项目</h2><p>智能客服项目主要有三个模块FAQ、任务型对话和闲聊，用户输入一个句子后是进入FAQ、任务型对话还是闲聊，是由一个大的顶层分类器来决定。</p>
<p>FAQ和闲聊都是单轮对话，所以用户后面的输入都会重新进入到顶层分类器再次进行分类。</p>
<p>任务型对话则是多轮的，如果一个句子被顶层分类器划分到任务型对话中，那么在这个任务结束以前，用户后面的输入都会不经过顶层分类器而直接进入到任务型对话中。<br>判断任务型对话是否结束有两种方式：</p>
<ul>
<li>一是这个任务已经完成</li>
<li>二是虽然任务还未完成，但是模型判断需要跳出任务，此时直接结束任务返回通过顶层分类器进行模块判别的进程。</li>
</ul>
<p>在所有的任务中，我们都需要先为这个任务建立一个baseline，通常为相关任务常用的相对简单的模型。</p>
<span id="more"></span>


<hr>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>在NLP中，词嵌入是一个非常重要和基本的点。不同于图片领域，可以直接将图片色彩值、像素值等直接拿来进行学习，在NLP领域中，字、词是需要转换成数值，然后才能用一些算法来进行学习，而这个将词&#x2F;字转换成数值的方法，就叫做词嵌入 word embedding。</p>
<p>最简单的词嵌入就是ont-hot方法了，但是在都拥有大数据集的情况下，one-hot不仅使得词维度过大，而且并不能够表示文档、字词之间的表意关系。</p>
<p>词嵌入如此重要，我们在词嵌入方向也做了很多研究，并且现在已有很多种效果很好的词嵌入的方法。</p>
<ol>
<li><p>最开始自己在为法国农业银行做智能客服时使用的tf-idf就是一个比较经典的词嵌入方法，但是它仍然有维度过大和不能表示词表意之间的关系</p>
</li>
<li><p>我们这里主要要讲的就是我们经常用到的word2vec词向量方法。word2vec词向量主要有两种计算方法，一种是skip-gram，另一种就是CBOW</p>
<ol>
<li>skip-gram</li>
</ol>
<p>   skip-gram是通过在已知当前词的情况下，预测前n个词和后n个词</p>
<p>   当前词作为中心词时，因此要预测前n个词和后n个词，所以是从2*n个词的地方来进行参数优化，即进行学习，所以得到的词向量更准确一些，因此对于一些低频词较多的情况下，可以酌情使用这种方法，但是计算量会更大一些</p>
<ol start="2">
<li>CBOW</li>
</ol>
<p>   CBOW是在已知前面n个词和后面n个词的情况下，对当前词进行预测</p>
<p>   通过前n个词和后n个词来预测当前词，学习效率快，但是相对而言，优化的次数要少，因此得到的词向量会相对没有那么精确，但是因为计算量相对较小，而且这种方法得到的词向量已经满足常规使用需求了，所以这种方法的词向量被使用的更频发</p>
<ol start="3">
<li><p>两者都是通过对目标函数的优化，来优化隐藏层的参数W，而这个W就是我们的词向量</p>
</li>
<li><p>这两种计算方法其实计算量都特别大，所以有两种优化方法，一种是负采样，一种是softmax方法，我并没有细看</p>
</li>
</ol>
</li>
</ol>
<p>其实并没有规避one-hot维度大、没有词表意之间练习的问题，我们只是把这个问题提前解决了之后，再拿来使用，所以这应该也算是迁移学习吧</p>
<hr>
<h3 id="顶层分类器"><a href="#顶层分类器" class="headerlink" title="顶层分类器"></a>顶层分类器</h3><hr>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><p>FAQ经历的漫长的演变。</p>
<ol>
<li><p>第一版</p>
<ul>
<li>最开始做的FAQ就是tf-idf加上余弦相似度</li>
<li>缺点：<ul>
<li>词包模型，没有句子中词汇出现的顺序信息</li>
<li>有的词汇出现频繁，但并不是很重要的词汇；相反有的词汇出现频率相对较小，但是确实非常重要的判别词汇，所以tf-idf给予字词的权重有待优化</li>
</ul>
</li>
<li>导致：<ul>
<li>有些句子可能在距离上更接近A句子，其实是相似于B句子，比如句式相似，只有一两个重点字不同</li>
</ul>
</li>
<li>为了优化这个方法对词汇权重计算的偏差问题（重要词汇的权重不够大），又找出了所有的重要词汇，并且计算每个句子中重要词汇的杰卡德系数</li>
<li>最后结合余弦相似度和杰卡德系数来判断两个句子的相似性</li>
<li>此时因为数据量小，句子相对简单，top1值并不高60%，但是top3值可以达到95%，剩下的95%是因为数据库中不存在相关语句</li>
</ul>
</li>
<li><p>第二版</p>
<ul>
<li>后来用的是深度学习加上余弦相似</li>
<li>深度学习主要是用word2vec和LSTM得到句子的embedding</li>
<li>有目标句子Anchor，同类（答案相同的）问题中抽取一个正例Positive，非同类问题中抽取一个负例Negative</li>
<li>d(a,p) &lt;&#x3D; d(a,n)，所以d(a,p)-d(a,n) &lt;&#x3D; 0，所以d(a,p)-d(a,n)+margin &lt;&#x3D; 0</li>
<li>对于新输入的句子，训练好的模型唯一的用途就是embedding，用模型embedding出来的数值计算余弦相似度进行排序</li>
<li>缺点：<ul>
<li>因为负采样的原因不稳定，每一对&lt;a,p,n&gt;中，正采样可以用的样本过少，负采样样本较多，且大概率负样本很容易满足d(a,p)&lt;&#x3D;d(a,n)的限制，因为一般情况下，负样本都比正样本离目标样本远的多</li>
</ul>
</li>
</ul>
</li>
<li><p>第三版</p>
<ul>
<li><p>loss中使用am-softmax。以前的<strong>loss</strong>是简单的取交叉熵，现在是</p>
<ul>
<li>对两个vector都做了归一化，即求内积变为求余弦相似度</li>
<li>增加类间距离，缩小类内距离：每个样本所属类的距离，必须<strong>远</strong>小于它跟其它类的距离。这个<strong>远</strong>可以是<strong>到其它类距离的1&#x2F;2</strong>，可以是<strong>到其它类的距离减去某个值: dist()-m</strong>，等。这里用的是第二种，即余弦相似度减去某个值m</li>
<li>为了让概率P更均匀的分布在0-1之间，对余弦相似度进行了s倍的缩放</li>
</ul>
</li>
<li><p>用了triplet loss中online采样的变体：</p>
<ul>
<li>假设包含B个图片的banch有P个不同的人组成，每人有K个图片，即B&#x3D;PK。两种在线采样策略分别是：<ul>
<li>batch all: 取目标样本下所有的正样本和所有的负样本（PK*(K-1)*(P-1)K个triplet，PK为所有样本数，K-1为正样本树，(P-1)K为负样本树）</li>
<li>batch hard: 取目标样本下距离最小的负样本和距离最大的正样本（PK个triplet）</li>
</ul>
</li>
<li>这两个采样策略，一个太复杂，计算量太大，一个太极端，不稳定。所以我们取了中和的方法，即<strong>取目标样本下每个类别中距离最小的m个负样本和距离最大的m个正样本，然后一一对应来计算（PK * m * (P-1)m个triplet with m &lt;&lt; K）</strong></li>
<li>offline采样：这个方法不够高效，因为最初要把所有的训练数据喂给神经网络，而且每过1个或几个epoch，可能还要重新对negative examples进行分类</li>
</ul>
</li>
<li><p>triplet loss中的分类是以问答相似&#x2F;相同为标准的小类大约3500个，还有一个大类，即我们把FAQ问题整体分了个类，约840个类，FAQ整体数据大概10W。我们在得到一个新的输入句子时，会从840个类中分别抽取3个query，来计算相似度，我们取平均值最高的5个类，然后在只取这5个类中所有的问题来和新输入的句子进行相似度匹配，来得到需要输出的答案。</p>
<ul>
<li>第一次类的寻找，是为了节约时间，不让输入的query和10W个句子暴力匹配</li>
<li>第二次在5个类中找到需要的结果，是对具体回答的答案的寻找</li>
<li>是否直接输出找到的最匹配的答案，还有一个阈值来决定</li>
<li>如果我们对第一选项不确定，即没有超过相关阈值，那么我们可以推荐前三个相似度最高的问题，让用户选择</li>
<li>用户的选择，能够为我们带来新的数据，帮助优化模型</li>
<li>如果所有问题都没有超过推荐阈值，那么需要用户重新输入</li>
</ul>
</li>
<li><p>第三版使用了triplet loss改进后，top1值提高到了87%，top3值提高到了94%</p>
</li>
</ul>
</li>
</ol>
<h4 id="相似度计算方法"><a href="#相似度计算方法" class="headerlink" title="相似度计算方法"></a>相似度计算方法</h4><p>最长公共子序列、最长公共子串和编辑距离，都用的是动态规划，代码中需要注意：</p>
<ol>
<li>矩阵的建立</li>
<li>循环时，A，B指针和矩阵下标的对应</li>
</ol>
<h5 id="levenshtein"><a href="#levenshtein" class="headerlink" title="levenshtein"></a>levenshtein</h5><p>当i&#x3D;0 or j&#x3D;0,res[i][j]&#x3D;0<br>res[i][j]&#x3D;min{res[i-1][j]+1,res[i][j-1]+1,res[i-1][j-1]+flag}, flag&#x3D;0 if A[i]&#x3D;B[j], flag&#x3D;1 if A[i]!&#x3D;B[j]</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">levenshtein</span>(<span class="params">seq1, seq2</span>):</span><br><span class="line">    size_x = <span class="built_in">len</span>(seq1) + <span class="number">1</span></span><br><span class="line">    size_y = <span class="built_in">len</span>(seq2) + <span class="number">1</span></span><br><span class="line">    matrix = np.zeros ((size_x, size_y))</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(size_x):</span><br><span class="line">        matrix [x, <span class="number">0</span>] = x</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> xrange(size_y):</span><br><span class="line">        matrix [<span class="number">0</span>, y] = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">1</span>, size_x):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> xrange(<span class="number">1</span>, size_y):</span><br><span class="line">            <span class="keyword">if</span> seq1[x-<span class="number">1</span>] == seq2[y-<span class="number">1</span>]:</span><br><span class="line">                matrix [x,y] = <span class="built_in">min</span>(</span><br><span class="line">                    matrix[x-<span class="number">1</span>, y] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x-<span class="number">1</span>, y-<span class="number">1</span>],</span><br><span class="line">                    matrix[x, y-<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                matrix [x,y] = <span class="built_in">min</span>(</span><br><span class="line">                    matrix[x-<span class="number">1</span>,y] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x-<span class="number">1</span>,y-<span class="number">1</span>] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x,y-<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">    <span class="built_in">print</span> (matrix)</span><br><span class="line">    <span class="keyword">return</span> (matrix[size_x - <span class="number">1</span>, size_y - <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h5 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h5><p>当i&#x3D;0 or j&#x3D;0,res[i][j]&#x3D;0<br>当A[i]&#x3D;B[j],res[i][j]&#x3D;res[i-1][j-1]+1<br>当A[i]!&#x3D;B[j],res[i][j]&#x3D;0</p>
<h5 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h5><p>当i&#x3D;0 or j&#x3D;0,res[i][j]&#x3D;0<br>当A[i]&#x3D;B[j],res[i][j]&#x3D;res[i-1][j-1]+1<br>当A[i]!&#x3D;B[j],res[i][j]&#x3D;max{res[i-1][j],res[i][j-1]}</p>
<hr>
<h3 id="任务型对话"><a href="#任务型对话" class="headerlink" title="任务型对话"></a>任务型对话</h3><p>任务型对话主要有时效运费、查单、转寄退回等几个场景，这里主要用时效运费的场景来说明项目过程。</p>
<p>在时效运费场景中，NLU主要有两个模块，意图识别和NER。</p>
<h4 id="意图识别"><a href="#意图识别" class="headerlink" title="意图识别"></a>意图识别</h4><p>意图识别总共有8类，总数据量是1万5，留了1500个数据做测试集，3000个数据做验证集。</p>
<p>最开始使用的是stacking，后来随着数据量的增加，改用深度学习。<br>深度学习测试了cnn+rnn和cnn，发现cnn的效果比cnn+rnn好，应该是：</p>
<ul>
<li><p>数据本身都是比较短的句子，且序列性不强，所以rnn的优势没有体现出来</p>
</li>
<li><p>只有cnn更好的保存了提取的特征，优化了分类效果</p>
</li>
</ul>
<h5 id="如何优化分类问题"><a href="#如何优化分类问题" class="headerlink" title="如何优化分类问题"></a>如何优化分类问题</h5><h6 id="类别F1值分析"><a href="#类别F1值分析" class="headerlink" title="类别F1值分析"></a>类别F1值分析</h6><p>分析整体F1值，看主要影响效果的是哪一个&#x2F;哪几个类，然后针对性分析，提高优化效果。<br>针对F1值过低的类别，仔细分析错误分类的数据，找出原因</p>
<h6 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h6><p>根据学习曲线，可以找出当前模型训练出最佳结果所需要的最少的数据数，可以避免资源的浪费，也可以排除结果不佳可能是数据集不够的困惑。</p>
<h5 id="说一下stacking以及它在这个项目中的应用"><a href="#说一下stacking以及它在这个项目中的应用" class="headerlink" title="说一下stacking以及它在这个项目中的应用"></a>说一下stacking以及它在这个项目中的应用</h5><p>stacking中我们用了两层分类器，第一层有三个基分类器RF，NB，LR，第二层分类器是线性核函数的SVM，folder&#x3D;3，</p>
<ol>
<li><p>先用tf-idf提取特征</p>
</li>
<li><p>第二层的输入为第一层的输出，不包括原始数据</p>
</li>
<li><p>第二层分类器如果是更简单的分类器，效果更好，比如我们用的是线性核函数SVM。</p>
<ul>
<li><p>可能因为上一层的几个基分类器多维度提取特征已经比较复杂，所以第二层的分类器过于复杂会造成过拟合。</p>
</li>
<li><p>输出是概率，更符合分类的要求</p>
</li>
</ul>
</li>
<li><p>较小数据集上stacking的表现并不如意，有时甚至比单独的分类器的效果要差</p>
</li>
<li><p>基分类器中如果有效果特别差的，可以将其移除，可能优化最后的结果</p>
</li>
<li><p>stacking的预测需要时间相对较长</p>
</li>
</ol>
<h6 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h6><p>逻辑回归是一个判别模型<br>通过对激活函数sigmoid的引入，使得线性回归中的$W^T · x$映射到(0,1)之间，使逻辑回归成为一个概率预测问题，参数就是$W^T$<br>为了使逻辑回归公式$P(y|x) &#x3D; p_1^y·p_0^{1-y}$最大，我们引入最大似然估计</p>
<p>逻辑回归假设数据符合伯努利分布，所以是一个参数模型。<br>通过引入sigmoid函数，将输出映射到[0,1]之间，</p>
<p>逻辑回归本来是二元分类，但是也可以用作多元分类。<br>如果逻辑回归中引入正则化，我们需要对特征进行标准化，在这标准化也可以加快训练。</p>
<p>最大似然(max最大化问题) 可以导出 loss funcition(min Cross Entropy最小化问题)</p>
<p>方法：</p>
<ul>
<li><p>one VS all，选择计算结果最高的那个类。</p>
</li>
<li><p>引入softmax</p>
</li>
</ul>
<p>缺点：样本不均衡，因为1 VS all</p>
<ol>
<li>损失函数</li>
</ol>
<p>欠拟合：增加特性，增加数据<br>过拟合：正则化，dropout，提前停止训练，减少模型复杂度</p>
<p><strong>在统计学中对变量进行线行回归分析，采用最小二乘法进行参数估计时，R平方为回归平方和与总离差平方和的比值，表示总离差平方和中可以由回归平方和解释的比例，这一比例越大知越好，模型越精确，回归效果越显著。R平方介于0~1之间，越接近1，回归拟合效果越好，一道般认为超过0.8的模型拟合优度比较高。</strong><br><img src="/public/images/image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-R2.png" alt="逻辑回归-R2"></p>
<h6 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h6><p>今天终于感觉自己终于懂了SVM的一点皮毛。</p>
<ol>
<li><p><strong>SVM：Support Vector Machine支持向量机</strong></p>
<p> SVM一般用于二元分类，但是因为one-vs-all的存在，我们也可以用它来进行多元分类。</p>
<p> SVM的效果一般特别好，特点是计算量比较大，但是他对数据的分布没有要求，它通过最大化到超平面最近点的距离，来进行分类。</p>
</li>
<li><p><strong>Python &gt;&gt; sklearn中的SVM：</strong></p>
<p> SVC: Support Vector Classification 支持向量用于分类<br> SVR: Support Vector Regression 支持向量用于回归<br> LinearSVC: Linear SVC 核函数为<strong>线性核函数</strong>的支持向量</p>
</li>
<li><p><strong>SVM几种常用核函数：</strong></p>
<p><img src="/../../public/images/image/SVM%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0.svg" alt="核函数"></p>
</li>
<li><p><strong>核函数的选择</strong>（吴恩达教授说）：</p>
<ul>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel (“rbf”)</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ul>
</li>
<li><p>参考视频：<br><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av75892058/">SVM核函数–表示这个视频真的很好</a></p>
</li>
<li><p>自己对核函数的总结：</p>
<ul>
<li>核函数最重要的作用就是映射，不同的核函数提供不同的映射方法</li>
<li>核函数和normalization有点相似，都是对点的映射，只是目标和方式不同，normalization是为了让数据分布更均匀，而SVM中的核函数是为了让数据分布更开散、更易分割</li>
<li>核函数的加入，是的SVM的计算量增大，尤其是使用高斯核函数和多项核函数的时候</li>
<li>在文本分类项目中的stacking方法中，我们使用的是LinearSVC</li>
<li>一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果</li>
<li>自己只是了解皮毛，对于核函数公式的推导，如何知道一个核函数是否是有效核函数等，都还不是太清楚</li>
</ul>
</li>
</ol>
<h5 id="介绍一下CNN以及在这个项目中的应用"><a href="#介绍一下CNN以及在这个项目中的应用" class="headerlink" title="介绍一下CNN以及在这个项目中的应用"></a>介绍一下CNN以及在这个项目中的应用</h5><p>在CNN中，<strong>卷积</strong>的作用是<strong>特征提取</strong>（比如图像中的边缘检测），<strong>池化</strong>的作用是压缩特征数，对<strong>特征进行降维</strong>。</p>
<p>在CNN分类模型中，我们简单的使用了三个卷积核(大小为filter_size*embedding_size)，加入了L2正则化。</p>
<p><strong>如何让CNN也能解决序列问题</strong></p>
<ul>
<li>CNN在卷基层后，还是保留了句子的顺序关系的，但是在经过max pooling对特征降维后，就打乱了这种顺序，所以我们其实可以去掉max pooling，只让CNN做卷积</li>
<li>或者也可以在CNN中加入位置信息</li>
</ul>
<h5 id="碰到的问题"><a href="#碰到的问题" class="headerlink" title="碰到的问题"></a>碰到的问题</h5><ol>
<li><p>样本不均衡</p>
<ul>
<li>过采样：不易太过，否则容易过拟合，但是预测效果很差</li>
<li>欠采样：当数据样本足够多时才能使用</li