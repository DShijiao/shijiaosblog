<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h2 id="Hexo-github"><a href="#Hexo-github" class="headerlink" title="Hexo + github"></a>Hexo + github</h2><h3 id="基本需求"><a href="#基本需求" class="headerlink" title="基本需求"></a>基本需求</h3><ol>
<li><p><code>github</code>上创建仓库，仓库名必须严格是<code>username.github.io</code></p>
</li>
<li><p>创建本地写博客的hexo文件夹MyHexo，然后进入文件夹，执行命令行<code>hexo init</code>，如果报错，会提示输入<code>npm install hexo --save</code></p>
</li>
<li><p>进入根目录下的<code>_config.yml</code>，增加</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:username&#x2F;username.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行命令<code>hexo g</code>，如果报错，一般是因为没有安装<code>git</code>，执行<code>npm install hexo-deployer-git --save</code>安装<code>hexo</code>下的<code>git</code>，然后重新<code>hexo g</code></p>
</li>
<li><p>执行<code>hexo d</code>，如果没有关联<code>git</code>和<code>hexo</code>，会自动提醒输入</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Username for &#39;https:&#x2F;&#x2F;github.com&#39;:</span><br><span class="line">Password for &#39;https:&#x2F;&#x2F;github.com&#39;:</span><br></pre></td></tr></table></figure>
<p> 这里我用了<code>ssh</code>钥匙。</p>
<ul>
<li>执行命令<code>ssh-keygen -t rsa -C github_de_email@gmail.com</code></li>
<li>进入<code>~/.ssh</code>复制文件<code>id_rsa.pub</code>里的公匙，进入到<code>github</code>里的<code>setting</code>，进入<code>SSH and GPG keys</code>，新建SSH钥匙<code>new SSH key</code>，粘贴公匙，<code>title</code>可以随意取或者不取，然后确定</li>
<li>运行<code>ssh -T git@github.com</code>，如果看到<code>Hi username! You&#39;ve successfully authenticated, but GitHub does not provide shell access.</code>证明SSH连接成功。</li>
</ul>
</li>
<li><p>自己博客的域名为 <a href="https://shijiaod.github.io/" target="_blank" rel="noopener">https://shijiaod.github.io/</a><br>有时<code>hexo d</code>后页面没有变化，可以尝试重启电脑…</p>
</li>
</ol>
<a id="more"></a>

<h4 id="踩过的坑之血与泪的教训"><a href="#踩过的坑之血与泪的教训" class="headerlink" title="踩过的坑之血与泪的教训"></a>踩过的坑之血与泪的教训</h4><ol>
<li><p><strong>⚠️ 注 ⚠️</strong>：硬是弄了一天才发现，是仓库名见错了，所以怎么<code>hexo d</code>都报错，说<code>ERROR: Repository not found.</code>，后来发现仓库名后面应该跟上<code>github.io</code>，即完整仓库名应该是<code>username.github.io</code>而不是<code>username</code>！</p>
</li>
<li><p>当时在<code>hexo d</code>一直报错要输入<code>username</code>和<code>password</code>时，尝试使用<code>git remote add origin</code>来连接<code>github</code>，但是更换到正确的仓库名后，发现自己删除掉以前建立的<code>origin</code>也不影响，可见这里其实不需要更多的步骤</p>
</li>
<li><p>执行命令<code>ssh -T git@github.com</code>后出现</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The authenticity of host &#39;github.com (13.229.188.59)&#39; can&#39;t be established.</span><br><span class="line">RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)?</span><br></pre></td></tr></table></figure>
<p> 是因为没有配置<code>ssh</code>，或者本地缺少一个文件夹（什么文件夹我也不清楚…）<br> 如果是没有配置<code>ssh</code>见上方如何配置，如果是后者，输入<code>yes</code>，而非回车或者<code>y</code></p>
</li>
<li><p>在升级npm，安装hexo下的git，重新安装hexo时，都会报错</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">No receipt for &#39;com.apple.pkg.CLTools_Executables&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">No receipt for &#39;com.apple.pkg.DeveloperToolsCLILeo&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">No receipt for &#39;com.apple.pkg.DeveloperToolsCLI&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">gyp: No Xcode or CLT version detected!</span><br><span class="line">gyp ERR! configure error</span><br><span class="line">gyp ERR! stack Error: &#96;gyp&#96; failed with exit code: 1</span><br><span class="line">gyp ERR! stack     at ChildProcess.onCpExit (&#x2F;usr&#x2F;local&#x2F;lib&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;node-gyp&#x2F;lib&#x2F;configure.js:351:16)</span><br><span class="line">gyp ERR! stack     at ChildProcess.emit (events.js:210:5)</span><br><span class="line">gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal&#x2F;child_process.js:272:12)</span><br><span class="line">gyp ERR! System Darwin 19.3.0</span><br><span class="line">gyp ERR! command &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;node&quot; &quot;&#x2F;usr&#x2F;local&#x2F;lib&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;node-gyp&#x2F;bin&#x2F;node-gyp.js&quot; &quot;rebuild&quot;</span><br><span class="line">gyp ERR! cwd &#x2F;Users&#x2F;shijiaodeng&#x2F;Documents&#x2F;MyBlog&#x2F;shijiaod&#x2F;node_modules&#x2F;nunjucks&#x2F;node_modules&#x2F;fsevents</span><br><span class="line">gyp ERR! node -v v12.13.1</span><br><span class="line">gyp ERR! node-gyp -v v5.0.5</span><br><span class="line">gyp ERR! not ok</span><br></pre></td></tr></table></figure>
<p> 是升级了mac的原因（到版本10.15.3），所以下载并安装Xcode并同意它的相关协议，问题就解决了。具体详见<a href="https://segmentfault.com/a/1190000021394623?utm_source=tag-newest" target="_blank" rel="noopener">这片文章</a></p>
</li>
</ol>
<h3 id="内容更丰富的博客"><a href="#内容更丰富的博客" class="headerlink" title="内容更丰富的博客"></a>内容更丰富的博客</h3><h4 id="博客主题"><a href="#博客主题" class="headerlink" title="博客主题"></a>博客主题</h4><ol>
<li><p><code>hexo</code>根目录下安装主题nexT<code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code></p>
</li>
<li><p><code>hexo</code>根目录下修改文件<code>_config.yml</code>中的主题为<code>nexT</code>：<code>theme: next</code></p>
</li>
<li><p>到next主题下更改配置文件<code>/hexo/theme/next/_config.yml</code>中的<code>scheme: pisce</code>，里面有四种主题可以选，<code>pisce</code>是经典的旁边有小栏框的格式</p>
</li>
<li><p>我喜欢把<code>sidebar</code>放在右边:<code>/hexo/theme/next/_config.yml</code>中改为</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line"># Sidebar Position.</span><br><span class="line"># position: left</span><br><span class="line">position: right</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入主题目录下的languages文件夹中，<code>cp zh-CN.yml zh-Hans.yml</code>，然后再进入<code>hexo</code>根目录下修改<strong>语言</strong>、名字等一些基本信息（我也不知道为什么要改成<code>zh-Hans</code>而不是直接用<code>zh-CN</code>…）</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Site</span><br><span class="line">title: 诗娇的博客</span><br><span class="line">subtitle: &#39;&#39;</span><br><span class="line">description: &#39;&#39;</span><br><span class="line">keywords:</span><br><span class="line">author: Shijiao DENG</span><br><span class="line">language: zh-Hans</span><br><span class="line">timezone: &#39;&#39;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在<code>/hexo/theme/next/_config.yml</code>中配置<code>avatar</code>设置图像</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: #&#x2F;images&#x2F;avatar.gif  #头像图片路径 图片放置在hexo&#x2F;public&#x2F;images</span><br><span class="line">  rounded: false  #是否显示圆形头像，true表示圆形，false默认</span><br><span class="line">  opacity: 0.7  #透明度0~1之间</span><br><span class="line">  rotated: false  #是否旋转 true表示旋转，false默认</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li><p>进入主题目录下的<code>_config.yml</code>修改主页目录，可以更改配置里的上下顺序来更改他们在主页各自排列的顺序，比如<code>about</code>本来在<code>home</code>下面，我们把它移到了最下面</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F; || home</span><br><span class="line">  #about: &#x2F;about&#x2F; || user</span><br><span class="line">  tags: &#x2F;tags&#x2F; || tags</span><br><span class="line">  categories: &#x2F;categories&#x2F; || th</span><br><span class="line">  archives: &#x2F;archives&#x2F; || archive</span><br><span class="line">  schedule: &#x2F;schedule&#x2F; || calendar</span><br><span class="line">  #sitemap: &#x2F;sitemap.xml || sitemap</span><br><span class="line">  #commonweal: &#x2F;404&#x2F; || heartbeat</span><br><span class="line">  about: &#x2F;about&#x2F; || user</span><br></pre></td></tr></table></figure>
<p> 也可以自己增加一个目录内容，不过这些配置都要与主题目录下的languages文件中对应的<code>yml</code>文档里配置相关联。比如你在站点根目录中的配置文件设置<code>language</code>为<code>zh-Hans</code>，那么就要进入到主题目录下的languages文件中修改<code>zh-Hans.yml</code>，这样才能显示出菜单项新增的中文内容（以something为例子）</p>
</li>
<li><p>前面通过修改next主题下的<code>_config.yml</code>文件中的<code>menu</code>选项，可以在主页面的菜单栏添加”标签”选项，但是此时点击”标签”，跳转的页面会显示”page not found”。此时我们要新建一个页面</p>
<p> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A201.png" alt="图一"><br> 在新建的<code>index.md</code>文件中添加<code>type: &quot;tags&quot;</code><br> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A202.png" alt="图二"><br> <code>categories</code>等同理</p>
</li>
<li><p>设置头像：</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: &#x2F;images&#x2F;avatar.gif  #头像图片路径 图片放置在next&#x2F;source&#x2F;images</span><br><span class="line">  rounded: false  #是否显示圆形头像，true表示圆形，false默认</span><br><span class="line">  opacity: 0.7  #透明度0~1之间</span><br><span class="line">  rotated: false  #是否旋转 true表示旋转，false默认</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>/hexo/theme/next/_config.yml</code>的<code>menu_settings</code>如果设置<code>icon: false</code>则无前面的图标，<code>badges: true</code>则标签都会显示数字</p>
</li>
<li><p>社交设置<code>/hexo/theme/next/_config.yml</code>里的<code>social</code></p>
</li>
<li><p>自动在文章中生成目录，在文章最前面加入<code>[toc]</code>，具体设置如下，但自己试了没有效果</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: false #是否启动侧边栏</span><br><span class="line">  number: true  #自动将列表编号添加到toc。</span><br><span class="line">  wrap: false #true时是当标题宽度很长时，自动换到下一行</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="添加站内搜索"><a href="#添加站内搜索" class="headerlink" title="添加站内搜索"></a>添加站内搜索</h4><ol>
<li>安装站内搜索插件 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></li>
<li>在根目录下的<code>_config.yml</code>添加 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></li>
<li>在<code>themes/next/_config.yml</code>文件中搜索<code>local_search</code>,进行设置 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true  # 设置为true</span><br><span class="line">  trigger: auto  # auto &#x2F;  manual，auto 自动搜索、manual：按回车[enter ]键手动搜索</span><br><span class="line">  top_n_per_article: 3 # 每篇博客显示搜索的结果数</span><br><span class="line">  unescape: true</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="不显示整篇文章"><a href="#不显示整篇文章" class="headerlink" title="不显示整篇文章"></a>不显示整篇文章</h4><p>在文章中想要显示的部分后面加上<code>&lt;!--more--&gt;</code>，从这里开始，后面文章会隐藏，只会显示”阅读全文”按钮</p>
<h4 id="更多惊喜"><a href="#更多惊喜" class="headerlink" title="更多惊喜"></a>更多惊喜</h4><p>还有很多细节，可参见以下两个网站：</p>
<p><a href="https://www.jianshu.com/p/3a05351a37dc" target="_blank" rel="noopener">网站一</a></p>
<p><a href="https://eirunye.github.io/2018/09/15/Hexo搭建GitHub博客—打造炫酷的NexT主题—高级—四/#more" target="_blank" rel="noopener">网站二</a></p>
<h2 id="如何把本地写好的-md文件推倒github网站上"><a href="#如何把本地写好的-md文件推倒github网站上" class="headerlink" title="如何把本地写好的.md文件推倒github网站上"></a>如何把本地写好的.md文件推倒github网站上</h2><ul>
<li>进入到根目录（<code>/Documents/xxxx.github.io/</code>）下，输入命令行<code>hexo g</code> (<code>hexo generate</code>)生成静态页面</li>
<li>输入命令<code>hexo d</code> (<code>hexo deploy</code>)上传到<code>github</code>上<br>这时就可以在网站上看到刚刚写的博客了。<br>如果想要先本地预览效果，可以使用<code>hexo s</code> (<code>hexo server</code>)</li>
</ul>
<h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h3><h3 id="添加评论等"><a href="#添加评论等" class="headerlink" title="添加评论等"></a>添加评论等</h3><h3 id="增加归档页面文章数目"><a href="#增加归档页面文章数目" class="headerlink" title="增加归档页面文章数目"></a>增加归档页面文章数目</h3>]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown语法</title>
    <url>/2020/02/19/Markdown%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><ol>
<li>安装插件<code>Markdown All in One</code>，包含了最常用的<code>Markdown</code>优化。</li>
<li>安装插件<code>Markdown Preview Github Styling</code>，专门针对<code>github pages</code>的预览，功能有限，但是正是我需要的</li>
<li>如果不是针对<code>github</code>的预览，则可以安装<code>Markdown Preview Enhanced</code>，这个插件应用更普遍</li>
</ol>
<a id="more"></a>

<h2 id="Markdown语法"><a href="#Markdown语法" class="headerlink" title="Markdown语法"></a>Markdown语法</h2><ol>
<li><p><strong>标题</strong>：标题有共有六个等级，在前面加上一到六个 “#”</p>
</li>
<li><p><strong>正文</strong>：正文中想要换行，必须要多跳一行，如果在代码中只换一行，那么其实没有换行</p>
</li>
<li><p><strong>代码</strong>：正文中的代码块，在前后加上”`”，如果是一段代码段落，前后分别加上”```”</p>
<p> <strong>注：可以根据不同的语言配置不同的代码着色</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wenzi</span><span class="params">(self, n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>列表</strong></p>
<p> 有序列表：输入数字 + 一个点”.” + 一个空格</p>
<p> 无序列表：输入”-“/“*”/“+” + 一个空格</p>
<ul>
<li>无序列表<ul>
<li>与前面的表示符号无关<ul>
<li>只与缩紧行数相关</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>加粗、倾斜和删除</strong></p>
<p> <strong>加粗</strong></p>
<p> <em>倾斜</em></p>
<p> <del>删除线</del></p>
</li>
<li><p><strong>引用</strong></p>
<blockquote>
<p>此处是引用</p>
</blockquote>
<blockquote>
<p>此处是多层引用第一层</p>
<p>此处是多层引用第二层</p>
</blockquote>
<blockquote>
<p>多层嵌套第一层</p>
<blockquote>
<p>多层嵌套第二层</p>
<blockquote>
<p>多层嵌套第三层</p>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><p><strong>插入连接</strong></p>
<p> <a href="https://markdown-zh.readthedocs.io/en/latest/" target="_blank" rel="noopener">Markdown中文文档</a></p>
</li>
<li><p><strong>插入图片</strong></p>
<p> <img src="images/avatar.jpeg" alt="avatar"></p>
<ul>
<li>可以在图片上传到<code>github</code>后，用github上图片的地址链接，这样网页上可以正常显示</li>
<li>可以把包含图片的文件夹docs放到本地/hexo/public/images里，用相对路径/public/images/docs/images.jpg来调用</li>
<li>更多方式参考<a href="https://fuhailin.github.io/Hexo-images/" target="_blank" rel="noopener">这个文章</a></li>
</ul>
</li>
<li><p><strong>文字的个性设置</strong>：可以直接用html语法对正文进行编辑，达到想要的展示效果</p>
<ul>
<li><p>居中：</p>
  <center>这一行居中</center>
</li>
<li><p>换色 + 变化大小：</p>
<p>  接下来就是见证奇迹的时刻<br>  <font color="#989898"> 我爱用的注释颜色 </font><br>  <font color="#FF0000"> 我可以设置这一句的颜色哈哈 </font><br>  <font size=6> 我还可以设置这一句的大小嘻嘻 </font><br>  <font size=5 color="#FF0000"> 我甚至可以设置这一句的颜色和大小呵呵</font> </p>
</li>
<li><p>分段、分行<br>  分段：<code>&lt;p&gt;&lt;/p&gt;</code><br>  分行：<code>&lt;br&gt;</code></p>
</li>
<li><p>有序、无序列表</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">    &lt;li&gt;无序列表&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ul&gt;</span><br><span class="line"></span><br><span class="line">&lt;ol&gt;</span><br><span class="line">    &lt;li&gt;有序列表&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ol&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>数学公式</strong>：主题目录下<code>/hexo/theme/next/_config.yml</code>设置<code>mathjax</code>里的<code>enable: true</code><br>并且在需要使用数学公式的博客里打开公式开关：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>python的点滴</title>
    <url>/2020/02/19/python%E7%82%B9%E6%BB%B4/</url>
    <content><![CDATA[<h2 id="一些函数的使用"><a href="#一些函数的使用" class="headerlink" title="一些函数的使用"></a>一些函数的使用</h2><h3 id="基本数据类型的时间复制度"><a href="#基本数据类型的时间复制度" class="headerlink" title="基本数据类型的时间复制度"></a>基本数据类型的时间复制度</h3><p><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A61.png" alt="图片一"></p>
<a id="more"></a>

<p><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A62.png" alt="图片二"><br><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A63.png" alt="图片三"></p>
<h3 id="map-zip"><a href="#map-zip" class="headerlink" title="map, zip"></a>map, zip</h3><p>map和zip配合使用，实现矩阵转置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原矩阵</span><br><span class="line">matrix &#x3D; [</span><br><span class="line">  [1,2,3],</span><br><span class="line">  [4,5,6],</span><br><span class="line">  [7,8,9]</span><br><span class="line">]</span><br><span class="line">class Solution:</span><br><span class="line">    def rotate(self, matrix):  </span><br><span class="line">        matrix[:] &#x3D; map(list,zip(*matrix))</span><br><span class="line">        return matrix</span><br><span class="line">print(Solution().rotate(matrix))</span><br><span class="line">[</span><br><span class="line">    [1,4,7],</span><br><span class="line">    [2,5,8],</span><br><span class="line">    [3,6,9]</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>更多关于<code>map</code>和<code>zip</code>的解释，请看<a href="https://blog.csdn.net/shawpan/article/details/69663710" target="_blank" rel="noopener">一行代码搞定矩阵旋转——python</a>。</p>
<h3 id="defaultdict-function-factory"><a href="#defaultdict-function-factory" class="headerlink" title="defaultdict(function_factory)"></a>defaultdict(function_factory)</h3><p><strong>defaultdict</strong></p>
<p>dict subclass that calls a factory function to supply missing values</p>
<p><code>defaultdict</code>构建的是一个类似<code>dictionary</code>的对象，其中<code>keys</code>值自行确定赋值，但是<code>values</code>的类型是<code>fucntion_factory</code>的类实例，而且<strong>具有默认值</strong>。比如<code>defaultdict(list)</code>创建一个<code>dict</code>,对于任何一个还不存在的<code>dict[newkey]</code>都已经有一个默认<code>list</code>使得<code>dict[newkey].append</code>可以直接运行而不需要先运行<code>dict[newkey]=[]</code>。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; [(&#39;yellow&#39;, 1), (&#39;blue&#39;, 2), (&#39;yellow&#39;, 3), (&#39;blue&#39;, 4), (&#39;red&#39;, 1)]</span><br><span class="line">import collections</span><br><span class="line">d &#x3D; collections.defaultdict(list)</span><br><span class="line">for k, v in s:</span><br><span class="line">    d[k].append(v)</span><br><span class="line">list(d.items())</span><br><span class="line">[(&#39;blue&#39;, [2, 4]), (&#39;red&#39;, [1]), (&#39;yellow&#39;, [1, 3])]</span><br></pre></td></tr></table></figure>

<h3 id="pandas聚合和分组运算之groupby"><a href="#pandas聚合和分组运算之groupby" class="headerlink" title="pandas聚合和分组运算之groupby"></a>pandas聚合和分组运算之groupby</h3><p>根据<code>key1</code>的值<code>a</code>,<code>b</code>来分组求<code>data1</code>的平均值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df</span><br><span class="line">data1     data2 key1 key2</span><br><span class="line">0 -0.410673  0.519378    a  one</span><br><span class="line">1 -2.120793  0.199074    a  two</span><br><span class="line">2  0.642216 -0.143671    b  one</span><br><span class="line">3  0.975133 -0.592994    b  two</span><br><span class="line">4 -1.017495 -0.530459    a  one</span><br><span class="line">&gt;&gt;&gt; grouped &#x3D; df[&#39;data1&#39;].groupby(df[&#39;key1&#39;])</span><br><span class="line">&gt;&gt;&gt; grouped</span><br><span class="line">&lt;pandas.core.groupby.SeriesGroupBy object at 0x04120D70&gt;</span><br><span class="line">&gt;&gt;&gt; grouped.mean()</span><br><span class="line">key1</span><br><span class="line">a      -1.182987</span><br><span class="line">b       0.808674</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>

<p>根据<code>cpucore-</code>的类别，求所有数值形式列的平均值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; data_mac.groupby(&#39;cpucore&#39;).mean()</span><br><span class="line">cpucore-	time_transform_bert	time_sess_run_bert	time_to_dict_bert	time_all_bert			</span><br><span class="line">1	0.003124	1.550403	0.000032	1.553559</span><br><span class="line">2	0.001235	0.730950	0.000034	0.732218</span><br><span class="line">3	0.001565	0.487936	0.000037	0.489538</span><br><span class="line">4	0.001103	0.323777	0.000029	0.324909</span><br><span class="line">5	0.001185	0.289874	0.000033	0.291093</span><br><span class="line">6	0.001226	0.284265	0.000033	0.285524</span><br><span class="line">0	0.001225	0.284462	0.000032	0.285719</span><br></pre></td></tr></table></figure>

<h3 id="is和-的区别"><a href="#is和-的区别" class="headerlink" title="is和==的区别"></a>is和==的区别</h3><p>在Python中一切都是对象。</p>
<p>Python中对象包含的三个基本要素，分别是：</p>
<ul>
<li>id(身份标识)</li>
<li>type(数据类型)</li>
<li>value(值)</li>
</ul>
<p>对象之间比较是否相等可以用<code>==</code>，也可以用<code>is</code>。</p>
<p><code>is</code> 和 <code>==</code> 都是对对象进行比较判断作用的，但对对象比较判断的内容并不相同。下面来看看具体区别在哪</p>
<ul>
<li><p><strong><code>is</code> 比较的是两个对象的 <code>id</code> 值是否相等，也就是比较两个对象是否为同一个实例对象，是否指向同一个内存地址</strong></p>
</li>
<li><p><strong><code>==</code> 比较的是两个对象的内容是否相等，默认会调用对象的 <code>__eq__()</code> 方法</strong></p>
</li>
</ul>
<p><strong>==比较操作符和is同一性运算符区别</strong></p>
<p>==是python标准操作符中的比较操作符，用来比较判断两个对象的value(值)是否相等。</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python3.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a </span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b == a</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a[:]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b == a</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="python中一些看似简单的运算需求"><a href="#python中一些看似简单的运算需求" class="headerlink" title="python中一些看似简单的运算需求"></a>python中一些看似简单的运算需求</h2><h3 id="求list的绝对值"><a href="#求list的绝对值" class="headerlink" title="求list的绝对值"></a>求list的绝对值</h3><p>求<code>list l = [-1,2,3,-4,-5,-6]</code>的绝对值的三种方法</p>
<ol>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list_abs &#x3D; list(map(abs, l))</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list_abs &#x3D; [abs(i) for i in l]</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">list_abs &#x3D; list(np.abs(l))</span><br></pre></td></tr></table></figure>
</li>
</ol>
]]></content>
      <categories>
        <category>-周边辅助</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>智能客服项目</title>
    <url>/2019/12/08/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<h2 id="智能客服项目"><a href="#智能客服项目" class="headerlink" title="智能客服项目"></a>智能客服项目</h2><p>智能客服项目主要有三个模块FAQ、任务型对话和闲聊，用户输入一个句子后是进入FAQ、任务型对话还是闲聊，是由一个大的顶层分类器来决定。</p>
<p>FAQ和闲聊都是单轮对话，所以用户后面的输入都会重新进入到顶层分类器再次进行分类。</p>
<p>任务型对话则是多轮的，如果一个句子被顶层分类器划分到任务型对话中，那么在这个任务结束以前，用户后面的输入都会不经过顶层分类器而直接进入到任务型对话中。<br>判断任务型对话是否结束有两种方式，一是这个任务已经完成，二是虽然任务还未完成，但是模型判断需要跳出任务，则直接结束任务返回通过顶层分类器进行模块判别的进程。</p>
<a id="more"></a>

<h3 id="顶层分类器"><a href="#顶层分类器" class="headerlink" title="顶层分类器"></a>顶层分类器</h3><h4 id="full-connection"><a href="#full-connection" class="headerlink" title="full connection"></a>full connection</h4><h4 id="online-offline采样"><a href="#online-offline采样" class="headerlink" title="online/offline采样"></a>online/offline采样</h4><h4 id="LOSS"><a href="#LOSS" class="headerlink" title="LOSS"></a>LOSS</h4><p>最开始使用的是余弦相似度</p>
<p>缺点：</p>
<ul>
<li><p>有些句子可能在距离上更接近A类句子，其实却属于B类，比如句式相似，只有一两个重点字不同</p>
</li>
<li><p>太过依赖句子本身的向量，而没有和拉开和不同类别句子间的距离</p>
</li>
</ul>
<p>之后是triplet loss</p>
<p>缺点：</p>
<ul>
<li>因为负采样的原因不稳定，每一对&lt;a,p,n&gt;中，正采样可以用的样本过少，负采样样本较多，且不能保证负采样样本的稳定性</li>
</ul>
<p>然后是am-softmax</p>
<p>为什么？</p>
<h3 id="任务型对话"><a href="#任务型对话" class="headerlink" title="任务型对话"></a>任务型对话</h3><p>任务型对话主要有时效运费、查单、转寄退回等几个场景，这里主要用时效运费的场景来说明项目过程。</p>
<p>在时效运费场景中，NLU主要有两个模块，意图识别和NER。</p>
<h4 id="意图识别"><a href="#意图识别" class="headerlink" title="意图识别"></a>意图识别</h4><p>意图识别总共有8类，总数据量是1万5，留了1500个数据做测试集，3000个数据做验证集。</p>
<p>最开始使用的是stacking，后来随着数据量的增加，改用深度学习。<br>深度学习测试了cnn+rnn和cnn，发现cnn的效果比cnn+rnn好，应该是：</p>
<ul>
<li><p>数据本身都是比较短的句子，且序列性不强，所以rnn的优势没有体现出来</p>
</li>
<li><p>cnn特征提取的效果更好，优化了分类效果</p>
</li>
</ul>
<h5 id="说一下stacking以及它在这个项目中的应用"><a href="#说一下stacking以及它在这个项目中的应用" class="headerlink" title="说一下stacking以及它在这个项目中的应用"></a>说一下stacking以及它在这个项目中的应用</h5><p>stacking中我们用了两层分类器，第一层有三个基分类器，第二层分类器是LR，folder=3，</p>
<ol>
<li><p>第二层的输入为第一层的输出，不包括原始数据</p>
</li>
<li><p>第二层分类器如果是更简单的分类器，效果更好，比如我们用的是罗辑回归。</p>
<ul>
<li><p>可能因为上一层的几个基分类器多维度提取特征已经比较复杂，所以第二层的分类器过于复杂会造成过拟合。</p>
</li>
<li><p>输出是概率，更符合分类的要求</p>
</li>
</ul>
</li>
<li><p>较小数据集上stacking的表现并非非常突出，比单独的分类器效果稍微好一点点，但是消耗要大的很多</p>
</li>
<li><p>基分类器中如果有效果特别差的，可以将其移除，可能优化最后的结果</p>
</li>
<li><p>stacking的预测需要时间相对较长</p>
</li>
<li><p>第一层基分类器用的是RF，NB，KNN</p>
</li>
</ol>
<p>代码找到了，直接运行写心得吧！然后完成几个经典机器学习的复习并记录。</p>
<h6 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h6><p>逻辑回归假设数据符合伯努利分布，所以是一个参数模型。<br>通过引入sigmoid函数，将输出映射到[0,1]之间，使逻辑回归成为一个概率预测问题，但正因它对sigmoid的引入，逻辑回归对极值</p>
<p>逻辑回归本来是二元分类，但是也可以用作多元分类。<br>如果逻辑回归中引入正则化，我们需要对特征进行标准化，在这标准化也可以加快训练。</p>
<p>方法：</p>
<ul>
<li><p>one VS all，选择计算结果最高的那个类。</p>
</li>
<li><p>引入softmax</p>
</li>
</ul>
<p>缺点：样本不均衡，因为1 VS all</p>
<ol>
<li>损失函数</li>
</ol>
<p>欠拟合：增加特性，增加数据<br>过拟合：正则化，dropout，提前停止训练，减少模型复杂度</p>
<h6 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h6><h5 id="介绍一下CNN以及在这个项目中的应用"><a href="#介绍一下CNN以及在这个项目中的应用" class="headerlink" title="介绍一下CNN以及在这个项目中的应用"></a>介绍一下CNN以及在这个项目中的应用</h5><h5 id="碰到的问题"><a href="#碰到的问题" class="headerlink" title="碰到的问题"></a>碰到的问题</h5><ol>
<li><p>样本不均衡</p>
<p> 除了样本不均衡本身带来的训练问题，在测试集中，因为样本严重不均衡，且需要尽量保证测试集和训练集的分布一样，所以可能存在测试集中某些类只有几个数据，导致结果随机性较大不可靠。</p>
</li>
<li><p>阈值难以确定</p>
</li>
<li><p>任务结束前如何判断用户是否更改意图，跳出当前场景</p>
<p> 我们增加了一个类others，包含所有肯定不属于当前场景的对话数据，比如：“我要投诉”，“今天天气适合登山”等等。</p>
<p> 对于需要跳出当前场景的对话，如果我们没有跳出，比不需要跳出场景对话但是却跳出了的结果要更严重，所以我们主要要考虑others类的准确率，其次是F1值和召回率。</p>
<p> 这里为了使得others相对于召回率有更大的准确率，我们增加了这个类的权重。如果训练时间足够长，是否增加权重最后的结果都是相同的，但是增加权重后，模型会优先保证others这一类的准确率，所以我们可以让训练停在others有较大准确率、且总体结果比较符合我们要求的时候。即牺牲别的类的准确率来优先保证others的准确率。</p>
</li>
<li><p>过拟合</p>
<p> 表现为训练集的F1值不断上升但是验证集的变化不大。原因是数据量很多，但是类别很少，且句子都很简短。</p>
<p> 解决方法：</p>
<ul>
<li>加入L2正则化：加入L2正则化初期效果很好，后面慢慢还是会过拟合</li>
<li>停止训练：在发现训练集不断上升但是验证集结果变化不大时，及时停止训练。</li>
</ul>
</li>
</ol>
<h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><h4 id="LSTM-CRF"><a href="#LSTM-CRF" class="headerlink" title="LSTM+CRF"></a>LSTM+CRF</h4><h5 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h5><h4 id="Lattice-LSTM"><a href="#Lattice-LSTM" class="headerlink" title="Lattice+LSTM"></a>Lattice+LSTM</h4><h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><h3 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h3><p>有三个输入：字向量，位置向量，句子向量<br>15%的词mask，这15%中有80%是真的用MASK来代替，10%是原有单词，10%是随机选择的单词<br>优点：</p>
<ul>
<li>只有80%的字为MASK是因为微调时，输入数据中不会有MASK</li>
<li>10%错误是因为这样模型不能100%确定当前字一定是正确的，所以迫使模型更多的依赖上下文</li>
</ul>
<h4 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h4><h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><h3 id="闲聊"><a href="#闲聊" class="headerlink" title="闲聊"></a>闲聊</h3><p>闲聊用的是seq2seq，loss中加入了anti-lm来减少无意义回复，加入了问句于回答的余弦相似性来减少重复问题的回答</p>
<h4 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h4><h4 id="anti-lm"><a href="#anti-lm" class="headerlink" title="anti-lm"></a>anti-lm</h4><h5 id="最大互信息"><a href="#最大互信息" class="headerlink" title="最大互信息"></a>最大互信息</h5><h4 id="attention-LSTM"><a href="#attention-LSTM" class="headerlink" title="attention+LSTM"></a>attention+LSTM</h4><h5 id="attention-1"><a href="#attention-1" class="headerlink" title="attention"></a>attention</h5><h5 id="transformer-1"><a href="#transformer-1" class="headerlink" title="transformer"></a>transformer</h5><h3 id="概述历程"><a href="#概述历程" class="headerlink" title="概述历程"></a>概述历程</h3><h4 id="法国"><a href="#法国" class="headerlink" title="法国"></a>法国</h4><h4 id="顺丰"><a href="#顺丰" class="headerlink" title="顺丰"></a>顺丰</h4><h3 id="几大模块"><a href="#几大模块" class="headerlink" title="几大模块"></a>几大模块</h3><h2 id="待写文章"><a href="#待写文章" class="headerlink" title="待写文章"></a>待写文章</h2><ol>
<li><p>过拟合欠拟合</p>
</li>
<li><p>样本不均衡</p>
</li>
<li><p>batch norm详述及优缺点</p>
</li>
<li><p>batch norm和layer norm的不同</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <url>/2019/12/07/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
    <content><![CDATA[<h2 id="什么是梯度消失-梯度爆炸"><a href="#什么是梯度消失-梯度爆炸" class="headerlink" title="什么是梯度消失/梯度爆炸"></a>什么是梯度消失/梯度爆炸</h2><p>梯度消失或梯度爆炸是指在神经网络权重更新的反向传播中，$\frac{\partial Loss}{\partial W}$过小或过大的现象</p>
<h2 id="梯度消失-梯度爆炸会产生什么问题"><a href="#梯度消失-梯度爆炸会产生什么问题" class="headerlink" title="梯度消失/梯度爆炸会产生什么问题"></a>梯度消失/梯度爆炸会产生什么问题</h2><p>梯度爆炸会导致权重的更新过大（表现为LOSS出现Nan）<br>梯度消失会导致权重的更新过慢或者几乎不更新</p>
<a id="more"></a>

<h2 id="为什么会有梯度消失和梯度爆炸"><a href="#为什么会有梯度消失和梯度爆炸" class="headerlink" title="为什么会有梯度消失和梯度爆炸"></a>为什么会有梯度消失和梯度爆炸</h2><p>一种是任务梯度消失/梯度爆炸的原因是反向传播过程中的叠成导致的<br>另一种认为反向传播不该背这个锅，反向传播求导只是一种求最优解的方式，最根本的原因还是网络结构的问题（线性和非线性公式连成组合）</p>
<blockquote>
<p>看到<a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">有个文章</a>说我们不应该纠结于梯度消失和梯度爆炸，应该从不同的角度看LSTM，比如选择性、信息不变性等等，觉得这是个很好的想法。当然，他对于RNN和DNN中梯度消失和梯度爆炸的看法也相对详细正确，下面会细说。</p>
</blockquote>
<p>无论是因为网络结构还是反向传播，不可否认的是梯度消失和梯度传播确实来源于对权重求导后很多小于1的数值的连乘，下面我门主要看一下为什么会产生这些小于1的数值。</p>
<h3 id="几个激活函数"><a href="#几个激活函数" class="headerlink" title="几个激活函数"></a>几个激活函数</h3><p>在进入反向传播求导前，先认识一下神经网络中我们经常用到的激活函数</p>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>sigmoid 常见于CNN中，在RNN和LSTM中一般不会用到sigmoid<br>sigmoid的公式为$S(x)=\frac{1}{1+e^{-x}}$<br>其函数和导数图如下</p>
<p><img src="./images/image/sigmoid.png" alt="sigmoid"></p>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>LSTM和RNN中会用到tanh，详情可以看上一篇文章<br>tanh的公式为$tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{x}}$<br>其函数和导数图如下</p>
<p><img src="/images/image/tanh.png" alt="tanh"></p>
<h4 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h4><p>Relu主要是为了解决梯度消失和梯度爆炸的问题<br>其公式为：<br>$<br>relu(x)=\begin{cases}<br>0, x&lt;0 \\<br>x, x&gt;0<br>\end{cases}<br>$</p>
<p>其函数和导数图如下</p>
<p><img src="/images/image/relu.jpg" alt="relu"></p>
<p>relu的优点：</p>
<ul>
<li>解决了梯度消失/梯度爆炸的问题</li>
<li>计算速度快</li>
<li>加速了网络训练（这点我有点怀疑，是指计算速度快呢还是指效果更好，更容易收敛找到最优解呢？如果是第二点，如何证明的？）<br>relu缺点：</li>
<li>负数部分恒为0，导致一些神经元无法激活（但是有很多relu的改进版已经解决了这个问题，比如leakrelu）</li>
<li>输出不是以0为中心</li>
</ul>
<blockquote>
<p><strong>问题：如果大于0的函数都返回其本身，不是失去了使用非线性函数的意义吗？</strong></p>
</blockquote>
<h3 id="为什么会产生梯度消失和梯度爆炸"><a href="#为什么会产生梯度消失和梯度爆炸" class="headerlink" title="为什么会产生梯度消失和梯度爆炸"></a>为什么会产生梯度消失和梯度爆炸</h3><p>DNN和RNN中的梯度消失和梯度爆炸是不同的</p>
<h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><p>写比较麻烦，下次有时间再看看专门推一遍公式写一遍吧，这里直接贴吧：</p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播w导数解释"></p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADb%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播b导数解释"></p>
<p>没有手动推导，所以曲折的理解过程如下：</p>
<ol>
<li><p>我当时看到两个博客里的公式不一样很困惑，后来发现是自己粗心，因为第一个是对W求导，第二个是对b求导。但是看到很多博客里其实都写的是第二种情况。<br>其实更应该考虑的是第一种情况，因为在神经网络中，bias是为了防止wx等于零起的辅助作用，真正有意义的是讨论W。</p>
</li>
<li><p>虽然真正有意义的是讨论W，但是图二中对b求导的公式其实和真正对w求导的公式差不多，因为他们前面的所有结果都是相同的，唯一不同的是最后一步对b求导结果是1，而对w求导结果是$\delta’(z)$，也就是图一中的$f_1$。所以图二中的公式可以看作是图一公式的详细版。所以在反向传播中，决定导数数值的不仅是激活函数的函数，也有W本身。</p>
</li>
</ol>
<p>如上所示，我们可以看到$loss$对$W_1$求导，其实是$\delta’(z) \cdot w_i$的连乘，在上面我们已经了解到，sigmoid和tanh的导数，都小于零，w一般都会初始化为均值为0，方差为1的数值，我们可以有$\vert \delta’(z) \cdot w_i \vert&lt;1$($\vert \delta’(z) \cdot w_i \vert&lt;\frac{1}{4}$当激活函数为sigmoid时)。因此会有梯度消失。</p>
<p>当然，当w特别大是，我们会有$\vert \delta’(z) \cdot w_i \vert&gt;1$，就会导致梯度爆炸。</p>
<p>从上我们可以看到DNN中的梯度爆炸或者梯度消失，本质是大于1或者小于1的数的<strong>连乘</strong>（注意这里不是幂次方，RNN中才是幂次方，下面会细说）导致的。</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>RNN中很重要的一点是权重共享。一样贴图</p>
<p><img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="rnn反向传播w导数解释"></p>
<p>这里有几个点需要解释：</p>
<ul>
<li><p>为什么DNN中就连乘，但是RNN的导数中有加法呢？<br>DNN中写出的公式是<strong>每一层</strong>$w_i$前的求导公式<br>RNN这里写出的是<strong>所有层数</strong>的$w_x$前的求导公式的和，因为RNN是权重共享，所以对$W_x$的更新就是每一层需要对$W_x$更新的和。<br>因此第三层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%89%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第三层导数">)，第二层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第二层导数">)，第一层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%80%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第一层导数"></p>
</li>
<li><p>公式中$\frac{\partial S_i}{\partial S_i-1}$和DNN中的$\frac{\partial f_i}{\partial f_i-1}$是一样的性质，唯一不同的是这里$\frac{\partial S_i}{\partial S_i-1}=\frac{\partial S_i-1}{\partial S_i-2}=tanh’\cdot W_s$，所以这里其实是一个小于1的数的<strong>幂次方</strong></p>
</li>
</ul>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p>关于DNN和RNN的梯度消失和梯度爆炸，<a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">这里</a>解释的非常详细。</p>
<p>我也自己来总结以下加深印象</p>
<ul>
<li><p>DNN中每层都有不同的参数，所以每一层都各是各的梯度。RNN中因为权重共享，所以梯度为所有层数梯度的总和（上面提到过）</p>
</li>
<li><p><strong>每一层中</strong>：DNN是小于1的梯度的<strong>连乘</strong>，而RNN中因为$\frac{\partial S_i}{\partial S_i-1}=\frac{\partial S_i-1}{\partial S_i-2}$的关系，所以是<strong>幂次方</strong></p>
</li>
<li><p>RNN中的梯度不会真正的消失，它是每一层梯度的和，RNN中的梯度消失主要是指：离的越远，梯度的传递越弱，所以梯度被邻近的梯度主导，导致模型难以学到远距离信息的长期依赖性问题</p>
</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>附加一点LSTM的</p>
<ul>
<li><p>在LSTM中，有很多条路径，cell state这条路径扮演的角色有点像ResNet残差连接，使前面的数据可以不受中间数据的影响传输到后面。</p>
</li>
<li><p>如果去除cell state这条路径，LSTM中该消失还是消失，该爆炸也爆炸，但是有了cell state，<em>梯度=消失梯度+cell state的梯度</em>，我们看到如果cell state的梯度不消失，就可以一定程度上缓解梯度消失，在第一版没有遗忘门的LSTM中，这里的cell state的梯度=1，后来加入了一个遗忘门，但是因为tanh的取值在-1和1之间，并且多数情况趋于1或者-1，所以加入遗忘门后，LSTM也可以一定程度上缓解梯度消失问题。</p>
</li>
</ul>
<h5 id="为什么说是缓解呢"><a href="#为什么说是缓解呢" class="headerlink" title="为什么说是缓解呢"></a>为什么说是缓解呢</h5><ul>
<li><p>我们不能保证每一层都有前面传过来的残差连接，即我们不能保证哪几层cell state的门就一定是0（怎么解释有待再详细思考）</p>
</li>
<li><p>cell state趋于1或者-1，且有不趋于1和-1的时候，这个时候梯度也是小于1的数</p>
</li>
<li><p><strong>那如果每一层的门的数值不一样，是不是也会导致$\frac{\partial S_i}{\partial S_i-1} \neq \frac{\partial S_i-1}{\partial S_i-2}$？所以这里其实也不是幂次方的关系？</strong></p>
</li>
</ul>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><ol>
<li><p>梯度爆炸一般的解决方法是使用gradient clipping(梯度裁剪)，即通过设置梯度的值域范围来限制梯度过大。</p>
</li>
<li><p>L2正则化：$Loss=(y-W^tx)^2+\alpha \cdot \Vert W \Vert^2$，如果权重过大，可以通过Loss的减小来一定程度控制W过大。</p>
</li>
</ol>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>在神经网络中，梯度消失出现的更多一些，也更难解决。</p>
<ol>
<li><p>用tanh取代sigmoid激活函数。因为tanh导数的范围是-1到1，相对sigmoid的导数0到$\frac{1}{4}$，tanh可以相对sigmoid在一定程度缓解梯度消失</p>
</li>
<li><p>使用relu激活函数，当x&gt;0时，relu导数是1。但是觉得他会引起一些其他问题，并且在x&gt;0时为线性函数，所以有点失去了使用激活函数的意义，上面已经提到过了</p>
</li>
<li><p>batchnorm(简称BN)非常复杂，以后专门弄一篇研究一下。在反向传播中$\delta’(z) \cdot w_i$有W的存在，我们通过强行拉回偏离的W的分布到均值为0方差为1的正太分布上，消除了W带来的放大和缩小的问题，来一定程度缓解梯度消失（梯度爆炸）的问题</p>
<p> 有文章说这里是通过BN改变激活函数的输入，使其落在激活函数比较敏感的区域，让输入的微小变化造成Loss较大的变化来缓解梯度消失，这一点还要详细思考一下。</p>
<p> a. 我们用BN对数据进行缩放，强行改变数据输入，是否会对模型有影响？</p>
<p> b. BN中，如果batch size过小，会导致数据不准确，过大会对内存有要求</p>
<p> c. 这里norm的是输入x还是权重w？：两个都不是，而是激活函数的输入$Wx+b$</p>
</li>
<li><p>残差结构：LSTM中cell state的应用，就类似于残差结构，可以一定程度上让在序列前面的信息不受影响的（较完整的）传输到后面的序列中</p>
</li>
<li><p>谨慎选择随机初始化权重，如xavier初始化</p>
</li>
</ol>
<h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><p>非常感谢<a href="https://zhuanlan.zhihu.com/p/76772734" target="_blank" rel="noopener">这篇文章</a>，很全面的解释了很多我的疑惑。</p>
<h3 id="hexo-markdown"><a href="#hexo-markdown" class="headerlink" title="hexo-markdown"></a>hexo-markdown</h3><ol>
<li><p>又一个新发现，在markdown中，如果使用<code>$</code>，公式会和字在同一层展示，如果使用<code>$$</code>，公式会自己单独用一行展示。如下：</p>
<ul>
<li><code>$</code>：$y=f(x)$</li>
<li><code>$$</code>：$$y=f(x)$$</li>
</ul>
</li>
<li><p>尝试了很多插入图片的方法。只有复制github上图片连接最好用，这样在markdown，首页和文章中都可以正确显示图片。如果只是在<code>_config.yml</code>中修改<code>post_asset_folder:true</code>，再使用相对路径来显示图片，那么图片只能在文章首页正确显示，但是不能在文章中正确显示，并且在markdown中的相对路径和在网页上的，不一定是一样的。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
        <tag>DNN</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2019/12/06/LSTM/</url>
    <content><![CDATA[<h2 id="LSTM来源"><a href="#LSTM来源" class="headerlink" title="LSTM来源"></a>LSTM来源</h2><p>LSTM由RNN变化而来，所以在介绍LSTM前，我们先了解一下RNN。</p>
<p>RNN是一类用来处理序列数据的神经网络，当前状态不仅受到当前输入$x_t$的影响，也受前面状态$h_{t-1}$的影响</p>
<p>$h_t=\delta (W^{h_{t-1}} \cdot h_{t-1} + W^{x_t} \cdot x_t)$<br>$y=\delta (W^y \cdot h_t)$</p>
<pre><code>ignore bais here</code></pre><p>且神经元是共享权重W的，即:<br>$W^{h_{t-1}}=W^{h_{t}}$<br>$W^{x_{t-1}}=W^{x_t}$<br>$W^{y-1}=W^y$</p>
<a id="more"></a>

<h2 id="RNN问题"><a href="#RNN问题" class="headerlink" title="RNN问题"></a>RNN问题</h2><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>RNN在实际应用中又一个很大的问题，就是长期依赖性问题，即一个输出$y_t$主要受到其前面几个输入和状态的影响，对较长时间前的输入和状态则非常不敏感，这是由于反向传播中梯度消失引起的。<br>为了解决这一问题，我们引入了LSTM（门机制）。</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>我们在计算RNN反向传播的梯度时会发现，因为RNN是一个共享权重的神经网络，所以当权重W可以特征分解为$W=Q \cdot \Lambda \cdot Q^T$且Q为正交矩阵时，则梯度包含特征值矩阵的幂次方，因此当特征值&gt;1时，则会梯度爆炸，反之则会梯度消失。<br>梯度爆炸比较容易察觉，一般表现为loss变为Nan<br>梯度消失则相对比较难以察觉，一般表现为loss几乎不变，但是loss几乎不变不代表就是产生了梯度消失</p>
<h2 id="LSTM基本描述"><a href="#LSTM基本描述" class="headerlink" title="LSTM基本描述"></a>LSTM基本描述</h2><p>LSTM一定程度上解决了RNN梯度消失和梯度爆炸的问题，得益于三个门的引入</p>
<h3 id="LSTM的三个门"><a href="#LSTM的三个门" class="headerlink" title="LSTM的三个门"></a>LSTM的三个门</h3><p>在LSTM中引入了三个门：输入门，输出门和遗忘门</p>
<p>遗忘门：控制了上一个cell的状态$C_{t-1}$中有多少信息进入到当前状态$C_t$中<br>输入门：控制了有多少信息会被保存到cell状态$C_t$中<br>输出门：控制了当前cell的状态$C_t$(cell state)有多少信息会输出到下一个神经元（即隐藏单元输出hidden state）</p>
<p><img src="/images/image/LSTM3-chain.png" alt="LSTM"></p>
<h3 id="LSTM特点"><a href="#LSTM特点" class="headerlink" title="LSTM特点"></a>LSTM特点</h3><p>LSTM一定程度上将RNN中反向传播中梯度中的叠成关系转换成叠加关系，所以缓解了梯度消失和梯度爆炸的问题。</p>
<h3 id="LSTM和GRU的比较"><a href="#LSTM和GRU的比较" class="headerlink" title="LSTM和GRU的比较"></a>LSTM和GRU的比较</h3><p>LSTM有三个门，GRU只有两个（更新门update和相关门revelance），所以LSTM的运算更为复杂。<br>GRU是在LSTM后出来的，为了缓解LSTM的计算问题，但是在一些实际应用中，GRU的效果并不比LSTM差，所以应用越来越广泛。</p>
<p><img src="/images/image/GRU.png" alt="GRU"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>关于RNN的长期依赖性问题，看了很多文章，一直不能确到底是因</p>
<ul>
<li><p>$h_t=Q^T \cdot \Lambda^t \cdot Q \cdot h_{(0)}$导致的在<strong>前向传播</strong>中$h_{(0)}$对$h_t$的影响因为幂次方的关系而非常小</p>
</li>
<li><p>还是因为梯度消失导致的长期依赖关系.</p>
<p>直到看到Andrew Ng在<a href="https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns" target="_blank" rel="noopener">视频</a>中明确指出：RNN的长期依赖性问题来源于<strong>梯度消失</strong>，并且在<a href="http://pelhans.com/2019/04/24/deepdive_tensorflow-note9/#长期依赖问题" target="_blank" rel="noopener">文章</a>中看到公式$\Delta_{h^{(0)}}$的求解，才理解：RNN的长期依赖性问题来源于反向传播中因共享权重W产生的$\Delta$的幂次方，造成了梯度消失，而使得当前状态对前面序列的依赖过小。</p>
</li>
</ul>
</li>
<li><p>在LSTM中有几种激活函数：</p>
<ul>
<li>sigmoid主要用在门的计算中，因为sigmoid很容易趋近于0或者1，所以使得门可以控制保存（等于1时）或不保存（等于0时）信息</li>
<li>softmax一般用于输出y之前</li>
<li>tanh用于求$\tilde{C}^t$和$h_{t}$的时候</li>
</ul>
</li>
<li><p>hexo中并不支持LaTex格式写公式的问题见<a href="http://stevenshi.me/2017/06/26/hexo-insert-formula/" target="_blank" rel="noopener">此文章</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>长期依赖性</tag>
      </tags>
  </entry>
  <entry>
    <title>Chitchat</title>
    <url>/2019/01/16/chitchat/</url>
    <content><![CDATA[<h2 id="Chitchat"><a href="#Chitchat" class="headerlink" title="Chitchat"></a>Chitchat</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>chitchat持续的时间很长了。每次进步不大，但是至少都在前进。因为持续时间长，每次想法和改动的地方比较多，也是学到了不少东西，想要把这些都记录下来，所以开了这个文档。</p>
<p>chitchat 目前已经自己改进到v1.5了。虽然每一次改动的内容并不大，但是每一次都是一个尝试。</p>
<a id="more"></a>

<h4 id="到目前为止，每个version的改进简介"><a href="#到目前为止，每个version的改进简介" class="headerlink" title="到目前为止，每个version的改进简介"></a>到目前为止，每个version的改进简介</h4><ul>
<li><p>v1.1 是所有以前的版本，但是因为隔的太久，当时又没有记录，所以都不太记得了。</p>
</li>
<li><p>v1.2 是自己重构代码，除此之外，和第一个版本的差别不大。</p>
</li>
<li><p>v1.3 是在<code>seq2seq</code>的基础上加入了<code>bert</code>的<code>get_pooled_output</code>。</p>
</li>
<li><p>v1.4</p>
<ul>
<li>v1.4 这个版本在v1.2版本上改变了loss。因为在同时训练了v1.3版本和v1.2版本后进行比较，发现相同时间内训练出来的结果（因为版本v1.3中加入了<code>bert</code>，所以相同时间内训练出来的<code>epoch</code>数相差很大），v1.2版本的更好。因为考虑到时间成本，所以v1.4版本是在v1.2版本的基础上进行改进的。具体内容可以看v1.4的<code>README.md</code>。</li>
<li>v1.4里也加入了一些“问候”，“再见”等的规则。</li>
</ul>
</li>
<li><p>v1.5 <code>seq2seq</code>中在<code>train</code>时，一般用上一步正确的输出作为下一步的输入，但是这里改成用上一步真实的输出作为下一步的输入。具体内容可见这个文件夹下的《变形seq2seq，即在training时，上一步的输出成为下一步的输入》</p>
</li>
<li><p>又把<code>learning_rate</code>调小了，在训练了100个epoch，但是结果还是很糟糕。</p>
</li>
<li><p>v1.6 尝试在v1.4的基础上把<code>layer</code>从3改成6</p>
</li>
</ul>
<h4 id="还有的问题"><a href="#还有的问题" class="headerlink" title="还有的问题"></a>还有的问题</h4><ul>
<li>回答和问题太相似</li>
<li>无意义回答过多</li>
<li>对话内容大多无意义，无法深入交谈具体事情</li>
</ul>
<h4 id="一些想法和接下来前进的方向"><a href="#一些想法和接下来前进的方向" class="headerlink" title="一些想法和接下来前进的方向"></a>一些想法和接下来前进的方向</h4><ul>
<li>增加<code>layer</code>层数，比如6，让模型更好的拟合 (v1.6)</li>
<li>试一试用明翔的转化模型，把一批聊天节目等的语音转换乘文字后用来作为数据训练？</li>
</ul>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>chitchat</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>protobuf</title>
    <url>/2019/01/16/protobuf/</url>
    <content><![CDATA[<h4 id="protobuf"><a href="#protobuf" class="headerlink" title="protobuf"></a>protobuf</h4><ul>
<li><code>protobuf</code>：把消息序列化的工具</li>
<li><code>rpc</code>： 远程方法调用</li>
<li><code>redis</code>：缓存</li>
<li><code>mysql</code>：持久储存</li>
</ul>
<h4 id="搭建微服务实例"><a href="#搭建微服务实例" class="headerlink" title="搭建微服务实例"></a>搭建微服务实例</h4><p><a href="https://blog.goodaudience.com/ml-client-server-using-grpc-in-python-3cba7693d1f5" target="_blank" rel="noopener">ML Client/Server Using gRPC in Python – Good Audience</a></p>
<h4 id="安装Protocol-Buffers："><a href="#安装Protocol-Buffers：" class="headerlink" title="安装Protocol Buffers："></a>安装Protocol Buffers：</h4><ol>
<li>ruby -e “$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;" target="_blank" rel="noopener">https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</a></li>
<li>brew install protobuf</li>
<li>安装gRPC：pip install grpcio</li>
<li>安装gRPC：pip install grpcio-tools</li>
</ol>
<a id="more"></a>

<h4 id="编译proto"><a href="#编译proto" class="headerlink" title="编译proto"></a>编译proto</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m grpc_tools.protoc -I .&#x2F;protos --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;intent.proto</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-I</code><br>指定生成的<code>pb2.py</code>和<code>pb2_grpc.py</code>文件的储存路径</li>
<li><code>./protos/intent.proto</code><br>需要编译的<code>.proto</code>文件的位置</li>
</ul>
<p>如果文件原来的位置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I .&#x2F;protos&#x2F; --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br><span class="line">- intent_pb2.py</span><br><span class="line">- intent_pb2_grpc.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I . --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- intent_pb2.py</span><br><span class="line">- intent_pb2_grpc.py</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br></pre></td></tr></table></figure>

<p>没弄明白为什么，不过为了方便函数之间的调用，一般使用第一种方法。</p>
<h4 id="http-VS-grpc"><a href="#http-VS-grpc" class="headerlink" title="http VS grpc"></a>http VS grpc</h4><ol>
<li><code>http</code>和<code>grpc</code>都是通过<code>tcp</code>运行。</li>
<li>不同的是<code>grpc</code>会将信息转换成二进制格式，因此传输更快，所需内存更小，效率更高。<br>但是因为<code>grpc</code>中，双方都需要编写相同的<code>.proto</code>，彼此传输的格式要固定，因此要求相对更严格一些。而http则没有这种要求，双方并没有格式的约定，所以<code>http</code>也更大一些，速度相对更慢一些。</li>
<li><code>http</code>通过<code>&quot;\\r\\n&quot;</code>来分隔信息。<code>grpc</code>则要通过<code>.proto</code>文件里的<code>message</code>严格规范信息的数目、类型等。</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow实战应用</title>
    <url>/2019/01/16/tensorflow/</url>
    <content><![CDATA[<h2 id="tensorflow-重新加载并继续训练模型"><a href="#tensorflow-重新加载并继续训练模型" class="headerlink" title="tensorflow 重新加载并继续训练模型"></a>tensorflow 重新加载并继续训练模型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ckpt_file &#x3D; tf.train.latest_checkpoint(path_save_model)</span><br><span class="line">if ckpt_file:</span><br><span class="line">    saver.restore(sess, ckpt_file)</span><br></pre></td></tr></table></figure>

<p>这里<code>ckpt_file</code>是文件<code>checkpoint</code>里<code>model_checkpoint_path</code>的值。<br>只要<code>model_checkpoint_path</code>指定的<code>checkpoint</code>在路径下存在，就会直接加载这个<code>checkpoint</code>继续训练。否则就会重新从<code>0</code>开始。</p>
<a id="more"></a>

<h3 id="tensorboard"><a href="#tensorboard" class="headerlink" title="tensorboard"></a>tensorboard</h3><p><code>log/</code>中会因为重新加载，有几个分开的文件，但是对于同一个变量（如<code>loss</code>）会在同一个图内显示所有其随着<code>steps</code>变化的曲线。<br><img src="/images/image/tensorboard.png" alt="tensorboard_tf.train.latest_checkpoint.png"></p>
<hr>
<h2 id="用tensorflow实现seq2seq的细节问题"><a href="#用tensorflow实现seq2seq的细节问题" class="headerlink" title="用tensorflow实现seq2seq的细节问题"></a>用tensorflow实现seq2seq的细节问题</h2><h3 id="1-maximum-iteration"><a href="#1-maximum-iteration" class="headerlink" title="1. maximum_iteration"></a>1. maximum_iteration</h3><p>在<code>dynamic_decode</code>中有参数<code>maximum_iteration</code>。</p>
<p>这个参数在<code>training</code>的时候并没有什么作用，因为在<code>training</code>时，<code>TrainingHelper</code>中有参数<code>sequence_length</code>，在解码到<code>sequence_length</code>的长度后，<code>TrainingHelper</code>会通过参数<code>finished</code>来告诉模型，解码已经结束。</p>
<p>在<code>inference</code>的时候，则会因为没有<code>sequence_length</code>的限制，而会不知道该在哪里停止，或者解码的句子过长。所以<code>maximum_iteration</code>在<code>inference</code>的时候，最好是有设定的。</p>
<h3 id="2-具体的seq2seq模型的输入和输出实例"><a href="#2-具体的seq2seq模型的输入和输出实例" class="headerlink" title="2. 具体的seq2seq模型的输入和输出实例"></a>2. 具体的seq2seq模型的输入和输出实例</h3><p><code>encoder_inputs</code>：length = [3, 4]</p>
<pre><code>&lt;sos&gt;, a, b, c, &lt;eos&gt;, &lt;pad&gt;
&lt;sos&gt;, c, b, a, d, &lt;eos&gt;</code></pre><p><code>decoder_inputs</code>: length = [2, 3]</p>
<pre><code>&lt;sos&gt;, e, f, &lt;eos&gt;
&lt;sos&gt;, e, f, g</code></pre><p><code>decoder_outputs</code>: length = [2, 3]</p>
<pre><code>e, f, &lt;eos&gt;, &lt;eos&gt;
e, f, g, &lt;eos&gt;</code></pre><p>可以看出，<code>&lt;sos&gt;</code>是不会出现在<code>decoder_outputs</code>里面的，无论是在<code>training</code>还是<code>inference</code>里，<code>decoder</code>的第一个输入都是<code>&lt;sos&gt;</code>，<code>decoder</code>的第一个输出就直接是句子的第一个字/词。</p>
<hr>
<h2 id="变形seq2seq，即在training时，上一步的输出成为下一步的输入。"><a href="#变形seq2seq，即在training时，上一步的输出成为下一步的输入。" class="headerlink" title="变形seq2seq，即在training时，上一步的输出成为下一步的输入。"></a>变形seq2seq，即在training时，上一步的输出成为下一步的输入。</h2><p><strong>用<code>GreedyEmbeddingHelper</code>替换<code>TrainingHelper</code></strong></p>
<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>在seq2seq的training中，我们一般用<code>target</code>，即上一步正确的输出，作为下一步的输入。但是因为这样，同样一句话在train时的输出，后infer时的输出是完全不一样的（同事说是因为模型训练的还不够好，这点有待考证）。为了使自己在infer时的输出更可控，我改变了train时的模型，下一步的输入不再是<code>target</code>（上一步的<strong>正确输出</strong>），而是上一步模型的<strong>真实输出</strong>。</p>
<h3 id="2-困难"><a href="#2-困难" class="headerlink" title="2. 困难"></a>2. 困难</h3><ul>
<li>这样最大的困难就是在<code>sequence_loss</code>的计算时会因为shape不同而报错。<br>因为少了y_label的约束，真实输出的句子长度是完全不可控的，但是在计算<code>sequence_loss</code>时计算的是两者<code>label</code>的熵（<code>nn_ops.sparse_softmax_cross_entropy_with_logits</code>），所以我们需要统一两者的<code>shape</code>。</li>
</ul>
<ul>
<li>真实输出<code>self.decoder_logits_train=outputs.rnn_outputs</code>。<code>self.decoder_logits_train</code>的<code>shape</code>是<code>[batch_size, sequence_len, vocab_size]</code>。这里的<code>sequence_len</code>实际上是这个<code>batch_size</code>中，真实输出的<strong>最长句子的长度</strong>。<br>同样，这个<code>batch_size</code>的<code>targets</code>，也会有个最长句子的长度<code>self.max_decode_length</code>。我们只需要让每个<code>batch_size</code>中的<code>sequence_len=self.max_decode_length</code>就可以了。<br>如果<code>sequence_len</code>太短，就加到<code>self.max_decode_length</code>的长度，如果太长，就用<code>tf.slice</code>给切断。<br>但是问题又来了：<ul>
<li>因为<code>self.decoder_logits_train.shape[1]</code> <code>type</code>的问题，又因为它的值是<code>None</code>，想要比较它和<code>self.max_decode_length</code>非常困难</li>
<li>就算比较了两者的值，想要把<code>self.decoder_logits_train</code>的第二维<code>tf.concat</code>到<code>self.max_decode_length</code>也要进行<code>tf.subtract(self.max_decode_length, self.decoder_logits_train.shape[1])</code>的运算。也会增加困难</li>
</ul>
</li>
</ul>
<h3 id="3-解决方案"><a href="#3-解决方案" class="headerlink" title="3. 解决方案"></a>3. 解决方案</h3><p>好在我们在数据整理的时候去除了过长的句子，使得<code>targets</code>的最大长度不会超过50。所以<code>self.max_decode_length</code>不会大于50。</p>
<p>我们可以直接<code>tf.concat</code> <code>self.decoder_logits_train</code>和一个<code>shape</code>为<code>[batch_size, 50, vocab_size]</code>的矩阵，然后用<code>tf.slice</code>把第二维按<code>self.max_decoce_length</code>来切断。这样无论<code>self.decoder_logits_train</code>是过长还是过短，我们都能得到最终想要的结果。</p>
<h3 id="4-遗留问题"><a href="#4-遗留问题" class="headerlink" title="4.遗留问题"></a>4.遗留问题</h3><ul>
<li>因为这里我们知道<code>self.max_decode_length</code>的最大长度，所以可以取巧，如果不知道的话，可能更麻烦一些。</li>
<li>当句子过短时，会因为后面都是用<code>&lt;pad&gt;</code>补上，因此可以计算出正确的<code>loss</code>。<br>但是当句子过长时，因为我们把<code>self.max_decode_length</code>后面部分全部截断，因此这里<code>loss</code>只计算了前面一部分的句子，后面部分的句子并不可控（可能后面部分非常长，但是仍然只有截断时的那个字符的差别产生<code>loss</code>——即<strong>真实输出</strong>文字，<strong>正确输出</strong>是”\<eos>“）。<br>所以<strong>可能</strong>会导致模型生成特别特别长的句子，但是只有前半部分是我们想要的。</li>
</ul>
<hr>
<h2 id="tf分布式-–-DistributionStrategy"><a href="#tf分布式-–-DistributionStrategy" class="headerlink" title="tf分布式 – DistributionStrategy"></a>tf分布式 – DistributionStrategy</h2><p>以下内容摘抄自：<br><a href="http://jcf94.com/2018/10/21/2018-10-21-tfunpacking9/" target="_blank" rel="noopener">TensorFlow 拆包（九）：High Level APIs \| Chenfan Blog</a></p>
<p><strong>看一下官方文档中对 <code>DistributionStrategy</code> 的设计思想。</strong></p>
<p>首先是一些底层的概念：</p>
<ul>
<li>Wrapped values：跨设备相关的变量可以被封装为两种类别，PerDevice 对象表示的变量在每个设备上的值不同，Mirrored 对象表示的变量在每个设备上的值都相同</li>
<li>Unwrapping and merging：考虑前面提过的这个函数 <code>call_for_each_tower(fn, w)</code>，<code>fn</code> 是模型函数，<code>w</code> 代表一些 Wrapped values。这个函数的调用过程中就包含了变量的 unwrapping 和 merging，假定在设备 <code>d0</code> 上 <code>fn(w0)</code> 得到的结果是 <code>(x, a, v0)</code>，在设备 <code>d1</code> 上 <code>fn(w1)</code> 得到的结果是 <code>(x, b, v1)</code>。首先在调用函数之前，<code>w</code> 需要被解包变成 <code>w0</code> 和 <code>w1</code> 然后分别调用 <code>fn</code> 函数。返回的结果有三种情况，第一个值都返回了一个相同的对象 <code>x</code>，则最终 <code>merge</code> 之后还是对象 <code>x</code>；第二个值是每个设备不一样的，则 <code>merge</code> 之后是一个 <code>PerDevice</code> 对象（其实就是个设备和对应值的 <code>map</code>）；第三个值是每个设备返回的分别是一组 <code>Mirrored</code> 对象的成员，则 <code>merge</code> 之后是一个 <code>Mirrored</code> 对象。所以 <code>call_for_each_tower(fn, w)</code> 在这里返回得到的就是一组 <code>(x, PerDevice{...}, Mirrored{...})</code></li>
<li>Tower context vs. Cross-tower context：Tower context 指的是对每个设备的封装上下文，通常对每个设备分别跑一遍模型函数就需要这种封装；Cross-tower context 指的是跨设备的封装上下文，比如说像 <code>reduce()</code> 这种所有设备共同参与的一个操作就需要这种封装</li>
<li>Worker devices vs. parameter devices：负责计算的设备和存参数的设备，没啥好说的。</li>
</ul>
<p>更新一个变量的常规操作如下：</p>
<ol>
<li>把输入数据集封装在 <code>d.distribute_dataset()</code> 中，然后创建一个 iterator</li>
<li>对每一个设备共同调用 d.call_for_each_tower() 来分别创建网络模型，并且最终各自得到一组梯度/变量对：<code>d0</code> 上有 <code>{(g0, v0), (g1, v1), ...}</code>，<code>d1</code> 上有 <code>{(g&#39;0, v0), (g&#39;1, v1), ...}</code> 等等这样</li>
<li>调用<code>d.reduce(VariableAggregation.SUM, t, v)</code> 或者 <code>d.batch_reduce()</code> 来对梯度求和，并且对应到各自的变量上：<code>{(Sum(g0, g&#39;0), v0), (Sum(g1, g&#39;1), v1), ...}</code></li>
<li>调用 <code>d.update(v)</code> 来对每一个变量进行更新</li>
</ol>
<p>3、4 两步如果用 <code>Optimizer</code> 中的 <code>apply_gradients()</code> 方法可以自动完成（……这就是 <code>Optimizer</code> 后来加进去那部分代码的作用），或者在一个 <code>Cross-tower context</code> 中调用 <code>_distributed_apply()</code> 方法也可以。常规的网络层都应该在 <code>Tower context</code> 中被调用。</p>
]]></content>
      <categories>
        <category>实战问题</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>docker</title>
    <url>/2019/01/16/docker/</url>
    <content><![CDATA[<h2 id="docker-挂载"><a href="#docker-挂载" class="headerlink" title="docker 挂载"></a>docker 挂载</h2><p>前面是本地文件，后面是镜像文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus&#x3D;3 -v &#x2F;Users&#x2F;shijiao&#x2F;Documents&#x2F;Bert&#x2F;:&#x2F;shijiao&#x2F;mkl conda-origin bash</span><br></pre></td></tr></table></figure>

<p><code>--cpus=8</code>设置的是cpu利用百分比，而不是个数，因为报错中，<code>Range of CPUs</code>是小数，而不是整数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus&#x3D;8 -v &#x2F;Users&#x2F;shijiao&#x2F;Documents&#x2F;Bert&#x2F;:&#x2F;shijiao&#x2F;mkl conda-origin bash</span><br><span class="line">Error response from daemon: Range of CPUs is from 0.01 to 6.00, as there are only 6 CPUs available.</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<hr>
<h2 id="SF-配置并使用-Docker"><a href="#SF-配置并使用-Docker" class="headerlink" title="SF 配置并使用 Docker"></a>SF 配置并使用 Docker</h2><h3 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h3><ol>
<li><p>Homebrew 安装命令：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">brew cask install docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 Docker：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 --name webserver nginx</span><br></pre></td></tr></table></figure>

<p> 打开 <code>http://localhost</code>，看到 <code>Welcome to nginx!</code> 说明 Docker 安装成功！</p>
<ul>
<li>-d: 让容器在后台运行</li>
<li>-p: 将容器内部使用的网络端口映射到我们使用的主机上</li>
</ul>
</li>
<li><p>停止 Nginx 服务器并删除：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker stop webserver</span><br><span class="line">docker rm webserver</span><br></pre></td></tr></table></figure>
</li>
<li><p>国内镜像加速</p>
<p> Docker -&gt; Preferences -&gt; Daemon -&gt; Registry mirrors 添加：<code>https://registry.docker-cn.com</code></p>
</li>
<li><p>测试加速器</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker info</span><br></pre></td></tr></table></figure>

<p> 如有以下配置内容，说明加速器配置成功：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">Registry Mirrors:</span><br><span class="line">    https://registry.docker-cn.com/</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h3><blockquote>
<p>个人偏好：仅配置一个镜像 (我以<strong><em>湖人总冠军</em></strong>命名 <code>lakers_champion</code>，不包含任何代码数据)，仅仅把需要安装的package封装好<br>这样就避免不用每个项目都给它单独弄一个镜像，10G+占空间啊！</p>
</blockquote>
<ol>
<li><p>本地build镜像</p>
<p> 新建 Dockerfile，e.g.,</p>
 <figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> okwrtdsh/anaconda3:pytorch-<span class="number">10.0</span>-cudnn7</span><br><span class="line"><span class="keyword">MAINTAINER</span> kun.xie@sfmail.sf-express.com</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install --upgrade pip \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install tqdm \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install jieba \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install gensim \\</span></span><br><span class="line"><span class="bash">&amp;&amp; apt-get install -y vim</span></span><br><span class="line">&amp;&amp; apt-get install -y git</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com tensorflow tensorboardX</span></span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple cupy pynvrtc --trusted-host mirrors.aliyun.com</span><br><span class="line">&amp;&amp; pip install git+https://github.com/salesforce/pytorch-qrnn</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com torchtext</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com jieba gensim</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com opencc-python-reimplemented</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span></span><br><span class="line">&amp;&amp; conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> conda install jupyter -y --quiet</span></span><br><span class="line">&amp;&amp; conda install pytorch torchvision -c pytorch</span><br><span class="line">&amp;&amp; conda install scikit-learn -y --quiet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD . /kxie</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir kxie</span></span><br><span class="line"><span class="comment">### 其实以上安装的package可以简单点写到文件requirements.txt中，以下一行命令就能完成安装操作 -- 因为懒没做！ ###</span></span><br><span class="line"><span class="comment"># RUN pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com -r ./requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /kxie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [ <span class="string">"/bin/bash"</span> ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>加了 命令 <code>CMD [&quot;/bin/bash&quot;]</code>后，我们直接<code>docker run -it name_of_mirror</code>后就能直接进入<code>/bin/bash/</code>了。</p>
</li>
<li><p>我们也可以在运行时指定别的命令，如<code>docker run -it name_of_mirror cat /etc/os-release</code>，就是用<code>cat /etc/os-release</code>替换了<code>/bin/bash</code>命令。</p>
<p>随后在当前目录下 <code>build</code>：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">docker build -t 10.202.107.19/sfai/lakers_champion .</span><br></pre></td></tr></table></figure>
<p>可以查看本地已经build好的镜像：<code>docker images</code></p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br><span class="line">REPOSITORY                                            TAG                 IMAGE ID               CREATED                   SIZE</span><br><span class="line">10.202.107.19/sfai/lakers_champion   latest               25473e876cd6        About an hour ago   11.1GB</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>把本地build好的镜像push到Harbor</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">docker login 10.202.107.19</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
</li>
<li><p>在server上把镜像从Harbor中pull下来</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ssh 01366808@10.202.90.201</span></span><br><span class="line">sudo docker pull 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>

<p> <code>docker images</code> 同样可以查看server上目前已有的镜像</p>
</li>
<li><p>server上运行</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">sudo nvidia-docker run -i -t --rm -p &lt;随便设置_1&gt;:8888 -p &lt;随便设置_2&gt;:6006 -v /HDATA/1/01366808/shijiao/test_map:/shijiao 10.202.107.19/sfai/tf1.12-cuda10-cudnn7-speech:latest bash</span><br></pre></td></tr></table></figure>
 <p><font color="#989898">
 <ul>
 <li>`<随便设置_1>` & `<随便设置_2>` 按个人喜好自行更改，避免冲突</li>
 <li>我把所有的代码以及数据放在了server上个人文件夹`~/kxie/`里面</li>
 <li>之后打开Jupyter Notebook页面的时候由于映射关系，能看到所有数据以及代码！很方便！！</li>
 </ul>
 </font></p>

<p> 运行 <code>ternsorboard</code> &amp; <code>Jupyter Notebook</code>：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash -</span></span><br><span class="line">nohup tensorboard --host=0.0.0.0 --logdir=/data/<span class="built_in">log</span>/kxie &amp;</span><br><span class="line">/opt/conda/bin/jupyter notebook  --ip=<span class="string">'*'</span> --no-browser --allow-root --NotebookApp.token= --notebook-dir=<span class="string">'/shijiao'</span></span><br></pre></td></tr></table></figure>
<p> 这样就可以打开网页 <code>http://10.202.90.201:&lt;随便设置_1&gt;</code> 跑代码了！Jupyter的可视化还是很方便的，从本地copy paste也容易！</p>
<p> <strong>具体命令分析，可以看新工作笔记第二页</strong></p>
</li>
</ol>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h3 id="想修改镜像内容怎么办？"><a href="#想修改镜像内容怎么办？" class="headerlink" title="想修改镜像内容怎么办？"></a>想修改镜像内容怎么办？</h3><p>进入docker镜像里面改：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion /bin/bash </span><br><span class="line">修改内容 <span class="comment"># 然后退出并记住 ***Container ID***</span></span><br><span class="line">docker commit &lt;ID&gt; 10.202.107.19/sfai/lakers_champion</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">server上重新pull: sudo docker pull 10.202.107.19/sfai/lakers_champion</span><br></pre></td></tr></table></figure>

<p>e.g., 修改本地镜像，使得能够使用<code>fzf</code>:</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion</span><br><span class="line"><span class="comment"># after enter into this image:</span></span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/junegunn/fzf.git ~/.fzf</span><br><span class="line">~/.fzf/install</span><br><span class="line">vi .vimrc</span><br><span class="line"><span class="built_in">set</span> rtp+=~/.fzf</span><br><span class="line"><span class="built_in">exit</span> <span class="comment"># and remember container id</span></span><br><span class="line">docker commit &lt;container id&gt; 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/junegunn/fzf" target="_blank" rel="noopener">GitHub - junegunn/fzf: A command-line fuzzy finder</a></p>
<p>将本地文件添加到镜像中：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker cp &lt;<span class="built_in">local</span> <span class="built_in">source</span>&gt; &lt;Container ID&gt;:&lt;dest&gt;</span><br><span class="line"><span class="comment"># 然后同上commit</span></span><br></pre></td></tr></table></figure>

<p>Terminal过段时间就会中断 <code>timed out: Connection to 10.202.90.201 closed.</code> 如何重新进入正在后台运行的Docker？<br>(当然也有办法永不中断，自行Google)</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ssh 01366808@10.202.90.201</span></span><br><span class="line">sudo docker <span class="built_in">exec</span> -it &lt;ID&gt; bash</span><br></pre></td></tr></table></figure>

<p>可以用 <code>docker ps</code> 查看Docker进程以及ID</p>
<p>删除镜像：<code>sudo docker rmi &lt;ID&gt;</code></p>
<p>结束运行的容器：<code>sudo docker stop &lt;ID&gt;</code></p>
<p>修改镜像名字或者tag：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker tag &lt;old_name:old_tag&gt; &lt;new_name:new_tag&gt;</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">docker tag &lt;ID&gt; &lt;new_name:new_tag&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Sync"><a href="#Sync" class="headerlink" title="Sync"></a>Sync</h3><p><font color="#989898">好像server上不连网的，没法Git同步，影响效率！</font></p>

<ol>
<li><p>从本地scp到服务器上:</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">scp -r &lt;<span class="built_in">source</span>&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>rsync</code>：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">rsync -av -e ssh --exclude=<span class="string">'excluded dir'</span> &lt;<span class="built_in">source</span>&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>

<p> <code>--exclude=&#39;excluded dir&#39;</code>：本地<source>中有些目录的内容是你不想传到server上的</p>
 <p><font color="#989898">
 <ul>
 <li>一般情况：本地修改完代码，使用上面rysnc命令同步到server上 (或者简单点通过Jupyter copy paste)，然后跑吧！</li>
 <li>交换 source dest 位置实现反向同步</li>
 </ul>
 </font></p>
</li>
</ol>
]]></content>
      <categories>
        <category>实战问题</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
