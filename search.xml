<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>ChitChat任务</title>
    <url>/2021/08/19/interview/ChitChat%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li><p>数据如何</p>
<ol>
<li>闲聊原本数据是120万左右，后来经过数据清洗后，剩下80万左右的数据。</li>
<li>数据清洗：<ol>
<li>删除重复对话</li>
<li>将多轮对话剪切成单轮对话</li>
<li>删除回复过短的回答，或者不需要的回答/问题（如 字符，笑脸符等）</li>
<li>删除过长对话（30字以内）</li>
<li>统一回复（如 哈哈哈哈哈哈 =&gt; 哈哈哈哈哈，如 我不知道啊，哈哈哈 =&gt; 我不知道啊）、</li>
<li>用规则过滤了一遍脏话</li>
</ol>
</li>
</ol>
</li>
<li><p>背景如何</p>
</li>
<li><p>模型如何</p>
<ol>
<li>“问候”和“再见”等，是用规则写的，大致过滤一遍输入</li>
<li>如果没有匹配到“问候”或者“再见”，就会进入到模型生成回复</li>
<li>模型是seq2seq+attention</li>
<li>attention是哪个attention，对结果影响并不大</li>
<li>把数据分为几块，比如5-10个字的句子一起，10-20个字的句子一起，等等，然后让句子从短到长进行输入，一是让训练更稳定，二是能一定程度上加快速度</li>
</ol>
</li>
</ol>
<ol start="4">
<li><p>准确率如何</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>无意义回答过多 =&gt; 加入anti-LM</li>
<li>相似回答过多 =&gt; 加入余弦相似到loss中，有一定的效果，但是很难统计，因为可能对某个测试集，效果明显，对某些测试集，效果就不是那么明显</li>
<li>简单问题正确回答的百分比 =&gt; “问候”等对话进行了规则处理</li>
<li>过于简短回答的百分比 =&gt; 数据集上进行了处理</li>
</ol>
<p>还有一个隐藏问题，生成式模型，一般都有training和inference时，输入不匹配的问题，即training时输入的是正确答案的编码，inference输入的是模型的上一步产生的输入<br>解决方法：</p>
<ol>
<li>只用输出作为下一步的输入，很难训练出语句通顺的句子，因此可以将两种方式交替训练</li>
<li>引入对齐模型alignment model。通过计算输入位置s和输出位置t的匹配程度，获取一个匹配程度分布的矩阵（attention），ast计算了在当前状态t，相对于前一个状态t-1，不同encode中的hs的重要程度。此矩阵衡量了输出单词t和单词s的对齐程度。上下文向量ct来源于ast，表示t时刻，encoder根据对齐程度得到的期望上下文向量</li>
</ol>
</li>
<li><p>怎么解决（trick或者模型）</p>
</li>
<li><p>为什么这么解决</p>
</li>
<li><p>效果怎么样<br>闲聊的评估是带有非常大的主观性的，而且场景不同，用户不同，对相同结果的满意度也会不一样，所以我们对模型的评估希望从更客观、更基础的地方来进行评估和改进：</p>
<ol>
<li>无意义回答的频率/百分比</li>
<li>相似回答的频率/百分比</li>
<li>正确回答的百分比</li>
<li>简单问题正确回答的百分比</li>
<li>相同问题，是否能给出相同答案</li>
<li>过于简短回答的百分比</li>
<li>对话内容大多无意义，无法深入交谈具体事情</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li>模型优化</li>
<li>数据集质量优化+数据集扩充</li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
</li>
</ol>
<p>闲聊：</p>
<ol>
<li>SF的闲聊中使用的是人工评价指标</li>
<li>虽然ppl被认为是可以评估生成的文本，但其实效果并不是特别好<ol>
<li>Perplexity的影响因素（这些是听报告了解的）：<ol>
<li>训练数据集越大，PPL会下降得更低，1billion dataset和10万dataset训练效果是很不一样的；</li>
<li>数据中的标点会对模型的PPL产生很大影响，一个句号能让PPL波动几十，标点的预测总是不稳定；</li>
<li>预测语句中的“的，了”等词也对PPL有很大影响，可能“我借你的书”比“我借你书”的指标值小几十，但从语义上分析有没有这些停用词并不能完全代表句子生成的好坏。</li>
</ol>
</li>
<li>需要具体了解一下ppl，出自哪里，怎么计算，如何评价生成的文本而非模型</li>
<li>可以看一下上次做的ppt</li>
</ol>
</li>
<li>20年google出了另一种评价指标BLEURT</li>
<li><a href="https://cloud.tencent.com/developer/news/468354" target="_blank" rel="noopener">人机对话关键技术及挑战</a><br>文章提到了好几个闲聊重的优化技术，比如learning to start for sequence to sequence,Retrieval-Enhanced Adversarial Training For Neural Response Generation, etc.</li>
</ol>
<p>好好研究一下<a href="https://www.infoq.cn/profile/90B7FCCBE83037/publish" target="_blank" rel="noopener">滴滴技术</a></p>
]]></content>
  </entry>
  <entry>
    <title>DM任务</title>
    <url>/2021/08/19/interview/DM%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何</li>
<li>背景如何</li>
<li>模型如何</li>
<li>准确率如何</li>
<li>碰到哪些问题</li>
<li>怎么解决（trick或者模型）</li>
<li>为什么这么解决</li>
<li>效果怎么样</li>
<li>还有哪些可以改进的</li>
<li>现有哪些更好的方法吗（新的论文）</li>
</ol>
<p>对话机器人中碰到的问题：</p>
<ol>
<li>解决语言的多样性和歧义性问题</li>
<li>槽位模块中如何提高抽取模型的复用性</li>
<li>解决实体消歧问题</li>
<li>上下文理解</li>
<li>场景可移植性</li>
<li>多轮（ner可以加到多轮里，比如学生教学场景，可以聊ner和dm结合rasa的设计）</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>FAQ任务</title>
    <url>/2021/08/19/interview/FAQ%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><h3 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h3><ol>
<li>数据如何<br>10W数据，1600个标准问（5.6w数据是自己造的），每个标准问对应n个相似问<br>最多的能有两千左右的相似样例，较少的只有上百个</li>
</ol>
<p>数据不是标注，是本来业务就已经有的归类数据</p>
<ol start="2">
<li><p>背景如何</p>
<ul>
<li>一句话进入到系统后，先用faq进行判别，如果是比如「我要寄快递」这个标准问，那么就转入到多轮，如果是其他的faq中对应问题，有两种可能，一种是特别肯定，就直接回复，一种是不太肯定，就推荐top3进行。</li>
<li>faq做的就是对所有问题的初步判断</li>
<li>（这里和vipkid里就不一样了，因为vipkid里用的是bert模型，很难判断第一个回复是不是和第二个回复差距很大，最开始设置的差别是0.0001，后来改用意图分类来进行筛选，如果意图不能判断，再给到faq，提高直答的准确率）</li>
</ul>
</li>
<li><p>模型如何<br>期间尝试过很多模型，用stacking，使用svm，使用双塔模型，使用lstm，最后发现lstm效果最好（在回复比率相当的情况下，直答率最高，较svm高12个百分点，直答准确率要低1个百分点）</p>
<ol>
<li>双塔模型+triplet loss<ul>
<li>其实双塔模型效果也还不错，尤其是优化了triplet loss之后，我们使用正负样本三元组，对于每一个样本，我们取最远的k个正样本，和最近的k个负样本，然后使用triplet loss进行计算</li>
<li>loss中的距离，尝试了常用的余弦距离和欧式距离，发现欧式距离效果更好。</li>
<li>每5个epoch更新一次距离最近的正样本和负样本，直答率确实提升了3%，但是训练时的整体性能下降很严重，对线上影响不大，线上还是过一个模型计算出embedding，然后找出最相似的句子即可（召回+精排，性能允许，直接精排，但是其实可以加一个召回）</li>
</ul>
</li>
<li>使用am softmax<ul>
<li>为了更好的区分类间距离，我们使用了am softmax，softmax指数右上角本来应该是x^cos(delta)，改为s(cos(delta)-m)，m就是为了增加目标类别间的距离，s是为了控制让好的样本有一个更高的梯度，来进一步缩小类内间的距离</li>
<li>相比triplet loss而言，am softmax再能达到相同效果的情况下，更节省算力</li>
</ul>
</li>
<li>两者相对简单的使用lsmt效果都有2%左右的提升，但是资源耗费不同<ul>
<li>triplet loss是通过计算相似度训练出得embedding，因此应该更适用于相似度的计算</li>
<li>这里最后还是需要进行分类的，加上am softmax更节省算力，在效果几乎相同的情况下，我们选择am softmax</li>
</ul>
</li>
</ol>
</li>
<li><p>准确率如何<br>最后结果，直答准确率，85.9%，直答率91.3%，默认回复比率2.3%</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>也会有数据问题<ol>
<li>最大的类别有700多个样本，最小的类别只有几十个样本</li>
<li>我们做了数据增广，过采样加上数据增强</li>
</ol>
</li>
<li>召回率和精排效果的提升<ol>
<li>主要是对模型的尝试，希望从模型方面更好的提升直答率和直答准确率</li>
<li>最初用了baseline是svm模型（svm模型对数据样本的分布并不敏感，只取最贴近边界的样本计算loss）</li>
<li>改用lstm后直答率提升了12%到89%，但是准确率下降了1%</li>
<li>改用am softmax整体效果又提升了，增加了阈值之后，直答准确率85.9%，直答率91.3%</li>
</ol>
</li>
<li>某些样本很难区分<ol>
<li>所以尝试了triplet loss和am softmax，针对性解决问题</li>
<li>都有一定的效果提升</li>
</ol>
</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li></li>
</ol>
</li>
</ol>
<ol start="7">
<li>现有哪些更好的方法吗（新的论文）</li>
</ol>
<ol start="8">
<li><p>碰到哪些问题</p>
<ol start="4">
<li>样本不平衡 —— 数据增强</li>
<li>提升直答率和精排的准确率 —— 模型的改进</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
</li>
<li><p>现有哪些更好的方法吗（新的论文）<br>SBert</p>
</li>
</ol>
<h3 id="VIPKid"><a href="#VIPKid" class="headerlink" title="VIPKid"></a>VIPKid</h3><ol>
<li><p>数据如何</p>
<ol>
<li>总共2w数据，200个类别</li>
<li>因为用的三元组重新构造的数据，某个样本有多个正样本和负样本。我们在当前类中，匹配当前问题坐在类别中的其他问题为正样本，随机选择其他类的问题为负样本，最后获得80w的三元组数据组对</li>
<li></li>
</ol>
</li>
<li><p>背景如何</p>
<ol>
<li>在intent不能够区分当前语句的意图时，faq进行处理并进行兜底</li>
</ol>
</li>
<li><p>模型如何</p>
<ol>
<li>最开始用的bert模型<ol>
<li>在对数据进行三元组处理以后，就讲数据放入bert中的nsp，进行而分类匹配，这个为基础模型，但是也不是特别差，F1值能到89.6%</li>
</ol>
</li>
<li>因为NSP初衷不是用来分类的，所以我们尝试使用sbert模型，只能bert来做一个embedding的提取<ol>
<li>先是只取的cls的embedding，尝试了triplet loss等通过相似度计算loss的方法，最后发现triplet loss效果最好</li>
<li>之后又我们比较了cls的embedding，第一层输出的embedding，最后一层token的embedding，然后计算相似度进行训练，发现最后一层token的embedding进行concat后计算相似度效果最好，能够再提升1.5个百分点的效果</li>
<li>最后效果是93%</li>
</ol>
</li>
<li></li>
</ol>
</li>
<li><p>准确率如何</p>
</li>
<li><p>碰到哪些问题</p>
</li>
<li><p>怎么解决（trick或者模型）</p>
</li>
<li><p>为什么这么解决</p>
</li>
<li><p>效果怎么样</p>
</li>
<li><p>还有哪些可以改进的</p>
</li>
<li><p>现有哪些更好的方法吗（新的论文）<br>3w数据，200个类，最开始是直接上的bert模型，然后使用nsp来预测两个句子是否是同一类，即对每个类别做个二分类。<br>效果不是特别好，89%<br>分析是bert的NSP并不适合分类，因为我们只是微调，没有重新训练bert，之前都是判别两个句子是否是前后文，与我们的情况不同<br>所以我们尝试使用sbert模型。其实这个模型的主题思想是使用bert来产生embedding，然后在对每个句子的embedding进行softmax，triplet loss等各种尝试处理，最后发现普通的triplet loss效果就不错，能够让f1值达到93%，等于是优化了解码的embedding</p>
</li>
</ol>
<p>Sbert<br>学习曲线<br>bert结果两集分化，如何平衡<br>triplet loss<br>am softmax</p>
]]></content>
  </entry>
  <entry>
    <title>INTENT任务</title>
    <url>/2021/08/19/interview/INTENT%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何</li>
</ol>
<h4 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h4><p>共有15000条数据，10个类别（如「信息」，「询问到达时间」，「询问费用」，「询问到达时间——是否」，「询问费用——是否」，「肯定」，「否定」，「」等） + 1个「others」类<br>其中将近5000是数据增强的</p>
<p>样本最多的类别others，样本数可达5000<br>样本最少的类别比如「否定」，样本数只有100多（最后两少的三个类别，取了三倍过采样，过多和过少都会降低F1值）</p>
<ol start="2">
<li><p>背景如何<br>比如其中一个多轮场景是「寄快递」</p>
</li>
<li><p>模型如何<br>baseline上的是rnn<br>后来改为cnn<br>也同bert比较了，bert出了收敛较cnn更快一些外，最终效果并不比cnn好</p>
</li>
<li><p>准确率如何<br>最后F1达到95%，达到业务要求</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>F1值不够95%<ol>
<li>数据增强</li>
<li>错误样本分析（尤其对准确率很低的类别，针对行进行分析）<ol>
<li>我们通过错误样本分析，找到了几个准确率特别低的样本，进行过采样，提升了效果</li>
</ol>
</li>
</ol>
</li>
<li>样本不均衡的问题<ol>
<li>低样本的三个类别进行了三倍过采样</li>
<li>数据增强（尝试了使用bert的mask进行数据增强）</li>
</ol>
</li>
<li>多轮之间通过意图跳转<ol>
<li>尝试过使用熵来替代概率，效果不好</li>
<li>最后通过调节weight，然后跟踪整体准确率和召回率的变化，提前中断训练，使得others类的准确率能达到99%（因为在调大others的weight后，训练会优先照顾这个类别，因为一开始，others的召回特别低，但是准确率特别高，然后在整体F1值慢慢上升后，others的准确率会开始下降，这时需要终止训练，这里牺牲了整体1个百分点的F1值）</li>
</ol>
</li>
<li>曾经出现过一个句子分属多个类别的情况<ol>
<li>进行数据梳理后，发现两个类别可以分开。对这两个类别的数据重新进行梳理，准确率大幅提升</li>
</ol>
</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li>对于分属多个类别的语句，是否也可以尝试进行多标签分类</li>
<li>更可以改进的是DM的设置<ol>
<li>每个多轮都有一个others，比较累赘，说明每个多轮都需要找过others的准确率，而影响整体的准确率</li>
<li>是否可以通过记住当前多轮所在状态和所在的bot，然后将再此进入多轮的句子进行更精确的分类和进入判别，这样就不用每个多轮里面都加一个others类别了</li>
</ol>
</li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
<ol>
<li>对于分类，cnn和rnn有时候就很够用了，再不行上bert也差不多，然后就是数据的问题了，模型帮助不大</li>
<li>对于对话机器人更好的想法，如上5.2.2</li>
</ol>
</li>
</ol>
<h4 id="VIPKid"><a href="#VIPKid" class="headerlink" title="VIPKid"></a>VIPKid</h4><ol>
<li>数据如何</li>
<li>背景如何</li>
<li>模型如何</li>
<li>准确率如何</li>
<li>碰到哪些问题</li>
<li>怎么解决（trick或者模型）</li>
<li>为什么这么解决</li>
<li>效果怎么样</li>
<li>还有哪些可以改进的</li>
<li>现有哪些更好的方法吗（新的论文）</li>
</ol>
<h2 id="重难点"><a href="#重难点" class="headerlink" title="重难点"></a>重难点</h2><h3 id="Bert数据增强"><a href="#Bert数据增强" class="headerlink" title="Bert数据增强"></a>Bert数据增强</h3><p>分类碰到的问题：</p>
<ol>
<li>使用某个模型前，也可以先做一个学习曲线，判断使用的样本是否足够</li>
<li>进行错误样本分析</li>
<li>调节weight，优化不同类别的presicion</li>
<li>如果某一类别的F1值过低，可以看下是否是样本不足的原因</li>
<li>做数据增强</li>
<li>如果是两个类别经常混淆，可以先将其归为一类，之后在下一层，用规则或模型进行细分<ol>
<li>对下一层分类起，是模型加针对性的数据增强</li>
<li>这里数据增强有用，但是整体数据增强，不一定能增加准确率，比如“蓝屏”，在这里是“noshow”的意思，但是在别的地方有可能是指机器故障（看下数据，找个好点的例子）</li>
</ol>
</li>
<li>叠加分类器</li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>NER任务</title>
    <url>/2021/08/17/interview/NER%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何<br>共有15000条数据，12个类别（如「出发地」，「出发时间」，「类型」，「物品」，「体积」等）</li>
</ol>
<ol start="2">
<li><p>背景如何<br>比如其中一个多轮场景是「寄快递」</p>
</li>
<li><p>模型如何<br>先是上的baseline，然后针对问题进行改进的（最开始尝试intent和ner一起做）<br>baseline使用的是lstm+crf，因为一般ner的问题，都是编码获取的信息不够，所以后来上了bilstm+crf，准确率提升了1.5个百分点，也尝试使用bert，发现bert效果并不比bilstm好。之后</p>
</li>
<li><p>准确率如何<br>最后F1值83%</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>整体F1值问题<ol>
<li>F1值不是特别高，刚开始只能到达80%左右</li>
<li>ner一般都是embedding不够丰富，所以我们尝试了bilstm和bert，两个效果差不多，但是bilstm更轻便，能够提升1.5%的F1值</li>
<li>后来又跑了一下当时刚出的lattice lstm，效果确实还是不错的，相对bilstm能提升2个百分点，当然，后来在vipkid又尝试了FLAT，效果更好，相对lattice lstm，还能再提升2个百分点</li>
<li>不同解码方式<ol>
<li>CRF/指针网络/Biaffine</li>
<li>不同解码方式差异有限</li>
</ol>
</li>
</ol>
</li>
<li>有的类别预测不准，有的特别准<ol>
<li>样本不均衡<ol>
<li>本身标注的样本，就会有些不均衡，所以自己按规律造了5000数据</li>
<li>当然，测试集里是原始数据</li>
<li>有一定的效果提升</li>
</ol>
</li>
<li>同样量的样本，有的类别比较难预测（规律难找）<ol>
<li>有的类别虽然样本也不算少，但是本身规律比较难找，比如「明天」，「明天下午两点」，实体变化很大，所以效果不如意</li>
<li>这些特殊类别，我们主要是通过规则加上开源工具来兜底</li>
<li>比如地址，我们本身有很全的字典，如果模型预测概率小于某个阈值，我们就尝试用字典进行匹配</li>
<li>比如时间，我们用开源工具进行兜底</li>
</ol>
</li>
<li>召回不高<ol>
<li>丰富embedding</li>
<li>检查是否有漏标的数据（标注数据的噪音去除）</li>
</ol>
</li>
<li>嵌套实体/多种类别实体</li>
<li>实体span过长（规则，引入指针网络）</li>
</ol>
</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li>一个是可以从embedding方面进行改进</li>
<li>另一个是嵌套实体/多类别实体的问题解决</li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
</li>
</ol>
<h2 id="重难点"><a href="#重难点" class="headerlink" title="重难点"></a>重难点</h2><h3 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h3><ul>
<li>基于「词」的NER会受到分词的影响。比如需要预测的句子中，目标词汇被错误划分从而导致结果错误。所以目前NER主要还是基于「字」做NER，然后尽量加入「词」信息</li>
<li>LSTM接CRF：在decoder时，对每一步的输出h，进行vitterbi编码</li>
</ul>
<h3 id="Viterbi"><a href="#Viterbi" class="headerlink" title="Viterbi"></a>Viterbi</h3><p>维特比是一个动态规划算法。<br>我们需要找到最大路径，通常需要求出每一层的每一种路径的概率。<br>但是在维特比算法中，我们每次取当前层，节点个数（即可能的状态个数）的路径（每个节点取，到当前层、当前节点，概率最大的路径），因此可以减少很多运算</p>
<h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><ul>
<li>CRF位条件概率（判别模型）</li>
<li>HMM是联合概率（生成模型），且有独立假设</li>
</ul>
<h3 id="Lattice-LSTM"><a href="#Lattice-LSTM" class="headerlink" title="Lattice LSTM"></a>Lattice LSTM</h3><p>Lattice LSTM在编码时在相应的字信息处加入了词信息，词信息一样有input, cell state，但是没有hidden state(output)，将词信息的cell state和字信息的cell state进行加权求和，获取新的hidden state的计算数据，并未改变原来的decoder结构和维度，因此后面也可以直接接CRF。并且因为加入了词信息，因此准确率会有提升。<br>SF中，准确率提升2%，但是速度会下降一倍。</p>
<h3 id="FLAT"><a href="#FLAT" class="headerlink" title="FLAT"></a>FLAT</h3><ul>
<li>计算了相对位置的四种distance</li>
<li>使用XLNet中attention score的计算方式，来计算相对距离的attention score</li>
</ul>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>ner</tag>
        <tag>lattice</tag>
        <tag>FLAT</tag>
      </tags>
  </entry>
  <entry>
    <title>中文面试自我简介</title>
    <url>/2021/08/15/interview/%E4%B8%AD%E6%96%87%E9%9D%A2%E8%AF%95%E8%87%AA%E6%88%91%E7%AE%80%E4%BB%8B/</url>
    <content><![CDATA[<h2 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h2><p>高中毕业后通过武汉大学和拉罗舍尔大学的交流项目去到法国，后来也在法国洛林大学完成了硕士学业，学生期间一直就读数学和统计专业。</p>
<p>毕业后因为觉得直接回国会很可惜，所以尝试在法国本地就业，之后顺利拿到长居，进入了Sikim公司，也是在这家公司，通过给当地银行做个智能客服的项目，了解到并进入了NLP领域。</p>
<p>18年回国加入到了顺丰科技，因为有过智能客服的项目经验，所以在顺丰中负责智能客服的NLU模块，这段智能客服经验加深了我对算法和深度模型的理解和落地经验，因为我一直对生成模型比较感兴趣，所以我主动申请了负责闲聊的项目，对生成模型的运行和落地有了了解</p>
<p>20年加入到vipkid后，期间也继续过对话生成的项目，比如机器翻译，闲聊等等，但是主要负责智能客服的研发，虽然在不同公司都在做智能客服项目，但是幸运的是每次都会丰富和提升自己对对话机器人的了解。比如顺丰里主要负责的是算法的优化和准确率的提升，在vipkid，不仅需要解决算法的问题，也会专注于解决更业务层面的问题，比如DM的设计，模块之间的跳转，对话的设计（一句话多个意图，一段话分开说）等等</p>
<p>期间当然还做过别的项目，比如主题提取，纠错，或者单纯工程开发的统计系统，但是总体，还是闲聊上对算法的实验相对较多，对对话机器人的项目了解更全面</p>
<h4 id="自我介绍的目的"><a href="#自我介绍的目的" class="headerlink" title="自我介绍的目的"></a>自我介绍的目的</h4><ul>
<li>我对当前职位的兴趣</li>
<li>我的能力是否能够匹配当前职位</li>
</ul>
<h4 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h4><p>优点：好奇心强，责任心强，抗压能力强，协作沟通能力不错<br>缺点：拖沓，钻研度不够，会因为想要尝试，有时不太分主次，导致浪费很多时间</p>
<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何</li>
<li>背景如何</li>
<li>模型如何</li>
<li>准确率如何</li>
<li>碰到哪些问题</li>
<li>怎么解决（trick或者模型）</li>
<li>为什么这么解决</li>
<li>效果怎么样</li>
<li>还有哪些可以改进的</li>
<li>现有哪些更好的方法吗（新的论文）</li>
</ol>
<h4 id="记录"><a href="#记录" class="headerlink" title="记录"></a>记录</h4><p>我的优势：</p>
<ol>
<li>学历背景</li>
<li>数学背景</li>
<li>4年工作经验</li>
<li>8年海外生活经验（适应力，学习能力）</li>
<li>对智能客服了解比较全面</li>
<li>对生成模型也有了解</li>
<li>抗压，沟通，协作能力</li>
<li>好奇心强，勇于实践</li>
</ol>
<p>职业能力：</p>
<ol>
<li><p>对智能客服的深耕</p>
</li>
<li><p>对闲聊很感兴趣，觉得闲聊是以后的重点，所以在进入顺丰后，在坚固智能客服的同时，主动选择了并不是很热门的闲聊（闲聊一定要再读几篇论文，再深耕一下）</p>
</li>
<li><p>对智能客服的了解比较全面，先是提高准确率的技术，后来是业务方面处理的技术（如DM，如用户一次说多句话是如何处理的，如一句话有多个意图是如何处理的，等等）</p>
<p> <a href="https://www.infoq.cn/article/pryrhwq1pj6xkb59gbx6" target="_blank" rel="noopener">为客服构建更加智能的对话机器人：多轮应答时机触发模型</a><br> <a href="https://cloud.tencent.com/developer/news/468354" target="_blank" rel="noopener">人机对话关键技术及挑战</a><br> 文章提到了好几个闲聊重的优化技术，比如learning to start for sequence to sequence,Retrieval-Enhanced Adversarial Training For Neural Response Generation, etc.<br> <a href="http://www.woshipm.com/pd/4133505.html" target="_blank" rel="noopener">任务驱动型人机对话系统设计</a></p>
</li>
<li><p>闲聊主要就聊技术</p>
</li>
<li><p>可以描述一下每次的变动是为什么，比如为什么毕业后想要留在国外，为什么会从顺丰离职，所以为什么现在会非常希望找到一个比较实践长久的工作等</p>
</li>
<li><p>如何在对话设计时，设计的是多次对话拼接到一起然后求他的意图，那么</p>
<ol>
<li>在训练时，如何解决训练的样本问题（人工可以标注，但是样本不是很多，SF为5000-10000条）</li>
<li>训练时不会有不收敛的问题吗？因为同样一部分的语句在不同的类别中出现过（并且是肯定不能使用CNN这种对序列信息没有考虑的模型） </li>
</ol>
</li>
<li><p>抗压能力：LP工作情况统计的项目</p>
</li>
</ol>
<h2 id="明天"><a href="#明天" class="headerlink" title="明天"></a>明天</h2><ol start="5">
<li>写下客服/对话机器人的面试文章</li>
<li>或者整理下智能客服的面试文章<ol>
<li>Sbert</li>
<li>学习曲线</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>面试</category>
      </categories>
      <tags>
        <tag>面试</tag>
        <tag>自我介绍</tag>
      </tags>
  </entry>
  <entry>
    <title>Préparation pour l&#39;entretien français</title>
    <url>/2021/08/15/french/Preparation-pour-l-entretien-francais/</url>
    <content><![CDATA[<h2 id="Presentation"><a href="#Presentation" class="headerlink" title="Présentation"></a>Présentation</h2><blockquote>
<p>La partie de Présentation a duré totalement 7min30 en oral.</p>
</blockquote>
<p>Phrase de préambule : Je m’excuse si mon français est un peu rouillé. </p>
<a id="more"></a>

<h4 id="Education"><a href="#Education" class="headerlink" title="Education"></a>Education</h4><p>En 2007, j’ai passé en Chine mon bac, appréciant la logique des mathématiques et étant performante sur ce domaine j’ai commencé des études scientifiques à l’université de Wuhan et j’y ai également appris des bases de Français. J’ai réalisé à Wuhan un Bac+2 et j’ai ensuite choisi de poursuivre mes études en France par une licence de mathématiques en 3 ans (la France a une bonne réputation dans ce domaine et j’avais envie de découvrir un nouveau pays et d’apprendre une nouvelle langue). </p>
<p>Même si j’étais à l’aise en mathématiques, je trouvais le domaine trop théorique.  Je cherchais un domaine qui allie la logique des mathématiques et le coté très concret de l’informatique. J’ai appris qu’un master à Nancy combinait les deux et je suis donc partie là-bas. J’ai réalisé un stage chez Vize une start-up locale où j’ai découvert plus précisément les travaux liés à l’intelligence artificielle et de l’utilisation de la donnée (big data).</p>
<h4 id="Carriere"><a href="#Carriere" class="headerlink" title="Carriere"></a>Carriere</h4><p>À la sortie de mon master en 2016, je suis venu travailler en tant que Data à Reims à SIKIM une autre PME spécialisée sur le domaine. J’y ai travaillé un peu plus de deux ans. L’ambiance était très bonne au sein de la petite équipe, j’ai eu l’opportunité de travailler là-bas sur un projet de Chatbot pour une Caisse régionale du Crédit Agricole. Techniquement néanmoins, j’avais l’impression de stagner et les projets requérant une réelle technicité était rares tandis que je trouvais que je n’avais pas assez de travail ne serait ce que pour occuper pleinement mes journées. J’ai ensuite été contactée par une entreprise chinoise SF Express, c’était un grand groupe, le 4ème logisticien mondial, qui m’a proposé un poste à Shenzhen avec un meilleur salaire et davantage de responsabilité. Cela me permettait également de me rapprocher de ma famille. Au sein de l’entreprise j’ai pu développer davantage mes compétences techniques [Donner un ou deux exemples ?].<br>Avec projet Chabot de service client (qui serve à répondre des questions à nos clients), j’étais responsable de la module NLU (Compréhension du langage naturel), mise en l’œuvre des algorithmes de classification qui permettent au Chatbot de comprendre l’intention du client, des algorithmes de REN(reconnaissance d’entités nommés) qui permettent au Chatbot de reconnaissir les entités nommées, des algorithme, comme BERT, qui peut créer automatiquement plus de données pour que les algorithmes puissent avoir une meilleur performance.<br>Avec le Chit Chat Chatbot (une intelligence artificielle (IA) de type dialogueur pour apporter un certain soutien psychologique), j’ai pu mis en l’œuvre le modèle de NLG (Génération Automatique de Langage, il consiste à générer automatiquement du texte pour adresser des réponses personnalisées aux utilisateurs) en impliquant l’algorithme de génération de réponses, des algorithmes pour évaluer la sémantique du texte, des algorithme pour diversifier les réponses, etc.</p>
<p>En raison de changements dans la structure organisationnelle de l’entreprise, j’ai quitté SF Express et rejoint à VIPKID en 2020. En tant qu’un Data Scientist, la formation de mathématiques ne m’a pas offert un grand avantage de coder, mais grâce à mes expériences dans VIPKID, j’ai pu approfondit ma capacité de coder surtout en réalisant un system de statistique à temps réel. Il recueillit des données de travail des employés sous de nombreux aspects, après le traitement statistique, on rend compte aux dirigent de tous les niveaux de l’entreprise avec une certaine fréquence chaque jour. Mes expériences dans VIPKID m’ont aussi entrainé ma capacité de management, et m’a permis d’avoir une vue complète de Chatbot. Avec trois autres collègues, on a réalisé une plateforme de Chatbot de A à Z, y inclus FAQ, NLU, Task Management et Dialogue Management, qui s’est appliqué aux services clients de toutes les fonctions (totalement 7) dans l’entreprise. Le taux de traitement par cette plateforme atteint à 79.6 (environ 10 milles de session par jour en total).<br>J’ai aussi pu approfondi ma capacité en NLG en réalisant un modèle de traduction automatique adapté aux besoins de l’entreprise. Il économise 10 milles euros par mois pour l’entreprise.</p>
<h4 id="Situation-mnt"><a href="#Situation-mnt" class="headerlink" title="Situation mnt"></a>Situation mnt</h4><p>J’ai voulu revenir en France le moment que j’ai quitté SF Express, mais c’était pas la bonne occasion à cause du COVID 19. Et maintenant, en raison des politiques du pays vers des entreprise d’éducation et la diffusion de la vaccination (l’amélioration de COVID 19), je pense que c’est le bon moment de revenir en France où je planifie à résider une longue durée.</p>
<p>A ce jour, j’ai accumulé 5 ans d’expériences dans le domaine de NLP, en particulier dans dialogue system et NLG. J’aimerai bien pouvoir avoir l’opportunité de connaître et d’appliquer des connaissances des autres domaine relative tels que CV (vision par l’ordinateur), l’apprentissage multimoda, basé sur mes compétences de NLP.</p>
<p><font color="#989898" size=4> Les informations dans le monde réel se présentent généralement sous différentes modalités. Par exemple, les images sont généralement associées à des balises et à des explications textuelles ; les textes contiennent des images pour exprimer plus clairement l’idée principale de l’article. Différentes modalités sont caractérisées par des propriétés statistiques très différentes. Par exemple, les images sont généralement représentées sous forme d’intensités de pixels ou de sorties d’extracteurs de caractéristiques, tandis que les textes sont représentés sous forme de vecteurs de nombre de mots discrets. En raison des propriétés statistiques distinctes des différentes ressources d’information, il est très important de découvrir la relation entre les différentes modalités. L’apprentissage est un bon modèle pour représenter les représentations conjointes de différentes modalités. Le modèle d’apprentissage multimodal est également capable de combler la modalité manquante compte tenu de celles observées. Le modèle d’apprentissage multimodal combine deux machines de Boltzmann profondes correspondant chacune à une modalité. Une couche cachée supplémentaire est placé au-dessus des deux machines Boltzmann pour donner la représentation commune. </font></p>
<h2 id="others"><a href="#others" class="headerlink" title="others"></a>others</h2><h3 id="Chatbot"><a href="#Chatbot" class="headerlink" title="Chatbot"></a>Chatbot</h3><p>输入，输出，数据量，数据类型，如何做数据增广，使用的模型，碰到了哪些问题，如何解决的</p>
<h3 id="Chit-Chat"><a href="#Chit-Chat" class="headerlink" title="Chit Chat"></a>Chit Chat</h3><p>输入，输出，数据量，数据类型，如何做数据增广，使用的模型，碰到了哪些问题，如何解决的</p>
<h3 id="Chercher-top-20-questions-a-l’entretien"><a href="#Chercher-top-20-questions-a-l’entretien" class="headerlink" title="Chercher top 20 questions à l’entretien"></a>Chercher top 20 questions à l’entretien</h3>]]></content>
      <categories>
        <category>Français</category>
      </categories>
      <tags>
        <tag>préparation</tag>
        <tag>entretien</tag>
        <tag>français</tag>
      </tags>
  </entry>
  <entry>
    <title>二叉树</title>
    <url>/2020/03/04/leetcode/%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    <content><![CDATA[<h3 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h3><h4 id="二叉树-1"><a href="#二叉树-1" class="headerlink" title="二叉树"></a>二叉树</h4><p>在计算机科学中，二叉树是每个结点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）。二叉树常被用于实现二叉查找树和二叉堆。</p>
<h4 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h4><p>二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树）它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上<strong>所有结点</strong>的值<strong>均小于</strong>它的根结点的值； 若它的右子树不空，则右子树上<strong>所有结点</strong>的值<strong>均大于</strong>它的根结点的值； 它的左、右子树也分别为二叉查找树。</p>
<h4 id="二叉堆"><a href="#二叉堆" class="headerlink" title="二叉堆"></a>二叉堆</h4><h3 id="二叉树遍历"><a href="#二叉树遍历" class="headerlink" title="二叉树遍历"></a>二叉树遍历</h3><h4 id="遍历介绍"><a href="#遍历介绍" class="headerlink" title="遍历介绍"></a>遍历介绍</h4><p>二叉树的遍历分<strong>广度优先遍历</strong>和<strong>深度优先遍历</strong></p>
<p><img src="/images/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.jpg" alt="二叉树"></p>
<ol>
<li><p>广度优先遍历(层次遍历)</p>
<p>结果为：abcdfeg</p>
</li>
<li><p>深度优先遍历<br>深度优先遍历有三种方法：前序遍历，中序遍历和后序遍历</p>
<ul>
<li><p>前序遍历：abdefgc</p>
<p>根节点 -&gt; 左子树 -&gt; 右子树</p>
</li>
<li><p>中序遍历：debgfac</p>
<p>左子树 -&gt; 根节点 -&gt; 右子树</p>
</li>
<li><p>后序遍历：edgfbca</p>
<p>左子树 -&gt; 右子树 -&gt; 根节点</p>
</li>
</ul>
</li>
</ol>
<a id="more"></a>

<h4 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h4><h5 id="广度优先遍历"><a href="#广度优先遍历" class="headerlink" title="广度优先遍历"></a>广度优先遍历</h5><h5 id="深度优先遍历"><a href="#深度优先遍历" class="headerlink" title="深度优先遍历"></a>深度优先遍历</h5><p>二叉树深度优先遍历的三种方法大同小异，唯一需要改变的只是函数helper中子树和根的代码的运行顺序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preoderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        self.result = []</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        self.helper(root)</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            self.result.append(root.val)</span><br><span class="line">            self.helper(root.left)</span><br><span class="line">            self.helper(root.right)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        self.result = []</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        self.helper(root)</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            self.helper(root.left)</span><br><span class="line">            self.result.append(root.val)</span><br><span class="line">            self.helper(root.right)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        self.result = []</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        self.helper(root)</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            self.helper(root.left)</span><br><span class="line">            self.helper(root.right)</span><br><span class="line">            self.result.append(root.val)</span><br></pre></td></tr></table></figure>


<h3 id="二叉查找树查找"><a href="#二叉查找树查找" class="headerlink" title="二叉查找树查找"></a>二叉查找树查找</h3><ul>
<li>DP(Dynamic programming)：动态规划</li>
<li>DFS(Depth-First-Search)：深度优先搜索</li>
</ul>
<h4 id="给出一个n，求1…n能够得到的所有二叉树个数"><a href="#给出一个n，求1…n能够得到的所有二叉树个数" class="headerlink" title="给出一个n，求1…n能够得到的所有二叉树个数"></a>给出一个n，求1…n能够得到的所有二叉树个数</h4><p>(en: given n, how many structurally unique BST’s that store values 1…n?)</p>
<ul>
<li>n = 0，{}为空集，所以只有一种二叉树，即空树</li>
<li>n = 1，{1}也只有一种，就是1</li>
<li>n = 2，{1, 2}有两种，1 - 2， 2 - 1</li>
<li>n = 3，{1, 2, 3}<br><img src="../../public/images/%E4%BA%8C%E5%8F%89%E6%A0%911-3.png" alt="二叉树1-3"><br>综上我们可以发现：</li>
<li>n = 1时，如果根为1，有{}个二叉树</li>
<li>n = 2时，如果根为1，有{2}个二叉树，即n=1的二叉树个数；如果根为2，只有{1}个二叉树，即n=1的二叉树个数</li>
<li>n = 3时，如果根为1，有{2, 3}个二叉树，即n=2的二叉树个数；如果根为2，有{1}*{3}个二叉树个数；如果根为3，同根为1的情况</li>
</ul>
<p>当我们知道N时，它的二叉树个数即为每个{1,…,N}中数字为根时的二叉树个数的总和。</p>
<p>当其中某个元素i为根时，它的二叉树个数即 <strong>{1,…,i-1}二叉树个数</strong> x <strong>{i+1,…,N}二叉树个数</strong>，而 <strong>{1,…,i-1}二叉树个数</strong> 就是 <strong>n=i-1的二叉树个数</strong> ，<strong>{i+1,…,N}二叉树个数</strong> 就是 <strong>n=N-i的二叉树个数</strong></p>
<p>所以若先定义n为0，1，2时的二叉树个数<code>dp = [1, 1, 2]</code>，则:</p>
<ul>
<li>n = 3：<code>dp[3] = dp[0]*dp[2]+dp[1]*dp[1]+dp[2]*dp[0]</code></li>
<li>n = 4：<code>dp[4] = dp[0]*dp[3]+dp[1]*dp[2]+dp[2]*dp[1]+dp[3]*dp[0]</code></li>
<li>…</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dp = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> dp[n]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp += [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-2</span>)]  <span class="comment"># 后面创建多个</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i+<span class="number">1</span>):</span><br><span class="line">                    dp[i] += dp[j<span class="number">-1</span>] * dp[i-j]</span><br><span class="line">            <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>

<h4 id="给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树"><a href="#给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树" class="headerlink" title="给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树"></a>给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">分别以1...n中每个元素，如i，为根：</span><br><span class="line">    根i的左子树</span><br><span class="line">        求根i的左子树：求1...i-1的所有二叉查找树（同原始问题求1...n），所以以1...i-1中的每个元素，如j，为根：</span><br><span class="line">            根j的左子树</span><br><span class="line">                ...</span><br><span class="line">            根j的右子树</span><br><span class="line">                ...</span><br><span class="line">    根的右子树</span><br><span class="line">        求根i的右子树：求i+1...n的所有二叉查找树（同原始问题求1...n），所以以i+1...n中的每个元素，如k，为根：</span><br><span class="line">            根k的左子树</span><br><span class="line">                ...</span><br><span class="line">            根k的右子树</span><br><span class="line">                ...</span><br></pre></td></tr></table></figure>
<p>具体代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.val = x</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generateTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">return</span> self.helper(<span class="number">1</span>, n)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, start, end)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> start &gt; end:</span><br><span class="line">            <span class="keyword">return</span> [<span class="literal">None</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(start, end+<span class="number">1</span>):</span><br><span class="line">            leftTree = self.helper(start, i<span class="number">-1</span>)</span><br><span class="line">            rightTree = self.helper(i+<span class="number">1</span>, end)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> range(len(leftTree)):</span><br><span class="line">                <span class="keyword">for</span> r <span class="keyword">in</span> range(len(rightTree)):</span><br><span class="line">                    root = TreeNode(i)</span><br><span class="line">                    root.left = leftTree[l]</span><br><span class="line">                    root.right = rightTree[r]</span><br><span class="line">                    result.append(root)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h3 id="二叉树最大深度和最小深度"><a href="#二叉树最大深度和最小深度" class="headerlink" title="二叉树最大深度和最小深度"></a>二叉树最大深度和最小深度</h3><h4 id="二叉树最大深度"><a href="#二叉树最大深度" class="headerlink" title="二叉树最大深度"></a>二叉树最大深度</h4><p>二叉树最大深度可以用一下代码来求：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span> <span class="comment"># 60%</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        type: root:Tree</span></span><br><span class="line"><span class="string">        rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.maxDepth = <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(root, depth)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">                self.maxDepth = max(depth, self.maxDepth)</span><br><span class="line">            <span class="keyword">if</span> root:</span><br><span class="line">                helper(root.left, depth+<span class="number">1</span>)</span><br><span class="line">                helper(root.right, depth+<span class="number">1</span>)</span><br><span class="line">        helper(root, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self.maxDepth</span><br></pre></td></tr></table></figure>
<p>leetcode上没有问题，但是代码本身是有一个问题要注意的（也不完全算漏洞）</p>
<p>比如在下图中</p>
<p><img src="/images/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.jpg" alt="二叉树"></p>
<p>当遍历到<code>d</code>的时候，<code>d</code>没有左子树，<code>d.left==None</code>，所以会进入到代码<code>root==None</code>，那么此时的<code>depth=3</code>，这个<code>depth</code>其实只是到<code>d</code>的二叉树深度。</p>
<p>因为需要求的是<strong>最大深度</strong>，如果此节点<code>d</code>没有右子树，那么这条路径的深度就是<code>depth=3</code>；如果此节点有右子树，右子树的深度肯定比<code>depth=3</code>大，那么<code>maxDepth</code>就会被右子树的深度所取代，也就不会有 <strong>到节点<code>d</code>的路径深度<code>depth=3</code></strong> 出现了，这个问题得以被解决，但是在二叉树<strong>最小深度</strong>中，这个问题就暴露出来，不能直接套用上述代码了。</p>
<h4 id="二叉树最小深度"><a href="#二叉树最小深度" class="headerlink" title="二叉树最小深度"></a>二叉树最小深度</h4><p>因为上面描述的原因，所以要对代码进行改进：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minDepth</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        type: root:Tree</span></span><br><span class="line"><span class="string">        rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        self.minDepth = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(root, depth)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> root.left == <span class="literal">None</span> <span class="keyword">and</span> root.right == <span class="literal">None</span>:</span><br><span class="line">                self.minDepth.append(depth)</span><br><span class="line">            <span class="keyword">if</span> root.left:</span><br><span class="line">                helper(root.left, depth+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> root.right:</span><br><span class="line">                helper(root.right, depth+<span class="number">1</span>)</span><br><span class="line">        helper(root, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> min(self.minDepth)</span><br></pre></td></tr></table></figure>
<p>代码不算简洁，但是在leetcode上跑到了94。69%，效果很不错，开心开心。</p>
<h3 id="验证一个二叉树是否是二叉查找树"><a href="#验证一个二叉树是否是二叉查找树" class="headerlink" title="验证一个二叉树是否是二叉查找树"></a>验证一个二叉树是否是二叉查找树</h3><p>虽然二叉查找树具体到定义上比较复杂，但是可以发现，如果把二叉查找树<strong>中序遍历</strong>，则能得到一个<strong>有序数组</strong>，否则得到的数组并非完全有序的。</p>
<p>以后如果有子树、根之间的比较等等，不妨看看是否可以通过遍历方式入手来简化。</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>二叉树</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>MacBook</title>
    <url>/2020/02/19/MacBook/</url>
    <content><![CDATA[<h4 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h4><p>Xcode更新后，可能VScode不能用，显示<code>java</code>相关问题，这时可以选择</p>
<ol>
<li>打开<code>launch.json</code>文件</li>
<li>然后<code>add configuration</code></li>
<li>选择<code>python</code></li>
<li>删除<code>json</code>文件中关于<code>java</code>的语言设置，只保留<code>python</code>的</li>
</ol>
<p>这样就OK了</p>
]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown语法</title>
    <url>/2020/02/19/Markdown%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><ol>
<li>安装插件<code>Markdown All in One</code>，包含了最常用的<code>Markdown</code>优化。</li>
<li>安装插件<code>Markdown Preview Github Styling</code>，专门针对<code>github pages</code>的预览，功能有限，但是正是我需要的</li>
<li>如果不是针对<code>github</code>的预览，则可以安装<code>Markdown Preview Enhanced</code>，这个插件应用更普遍</li>
</ol>
<a id="more"></a>

<h2 id="Markdown语法"><a href="#Markdown语法" class="headerlink" title="Markdown语法"></a>Markdown语法</h2><ol>
<li><p><strong>标题</strong>：标题有共有六个等级，在前面加上一到六个 “#”</p>
</li>
<li><p><strong>正文</strong>：正文中想要换行，必须要多跳一行，如果在代码中只换一行，那么其实没有换行</p>
</li>
<li><p><strong>代码</strong>：正文中的代码块，在前后加上”`”，如果是一段代码段落，前后分别加上”```”</p>
<p> <strong>注：可以根据不同的语言配置不同的代码着色</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wenzi</span><span class="params">(self, n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>列表</strong></p>
<p> 有序列表：输入数字 + 一个点”.” + 一个空格</p>
<p> 无序列表：输入”-“/“*”/“+” + 一个空格</p>
<ul>
<li>无序列表<ul>
<li>与前面的表示符号无关<ul>
<li>只与缩紧行数相关</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>加粗、倾斜和删除</strong></p>
<p> <strong>加粗</strong></p>
<p> <em>倾斜</em></p>
<p> <del>删除线</del></p>
</li>
<li><p><strong>引用</strong></p>
<blockquote>
<p>此处是引用</p>
</blockquote>
<blockquote>
<p>此处是多层引用第一层</p>
<p>此处是多层引用第二层</p>
</blockquote>
<blockquote>
<p>多层嵌套第一层</p>
<blockquote>
<p>多层嵌套第二层</p>
<blockquote>
<p>多层嵌套第三层</p>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><p><strong>插入连接</strong></p>
<p> <a href="https://markdown-zh.readthedocs.io/en/latest/" target="_blank" rel="noopener">Markdown中文文档</a></p>
</li>
<li><p><strong>插入图片</strong></p>
<p> <img src="images/avatar.jpeg" alt="avatar"></p>
<ul>
<li>可以在图片上传到<code>github</code>后，用github上图片的地址链接，这样网页上可以正常显示</li>
<li>可以把包含图片的文件夹docs放到本地/hexo/public/images里，用相对路径/public/images/docs/images.jpg来调用</li>
<li>更多方式参考<a href="https://fuhailin.github.io/Hexo-images/" target="_blank" rel="noopener">这个文章</a></li>
</ul>
</li>
<li><p><strong>文字的个性设置</strong>：可以直接用html语法对正文进行编辑，达到想要的展示效果</p>
<ul>
<li><p>居中：</p>
  <center>这一行居中</center>
</li>
<li><p>换色 + 变化大小：</p>
<p>  接下来就是见证奇迹的时刻<br>  <font color="#989898"> 我爱用的注释颜色 </font><br>  <font color="#FF0000"> 我可以设置这一句的颜色哈哈 </font><br>  <font size=6> 我还可以设置这一句的大小嘻嘻 </font><br>  <font size=5 color="#FF0000"> 我甚至可以设置这一句的颜色和大小呵呵</font> </p>
</li>
<li><p>分段、分行<br>  分段：<code>&lt;p&gt;&lt;/p&gt;</code><br>  分行：<code>&lt;br&gt;</code></p>
</li>
<li><p>有序、无序列表</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">    &lt;li&gt;无序列表&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ul&gt;</span><br><span class="line"></span><br><span class="line">&lt;ol&gt;</span><br><span class="line">    &lt;li&gt;有序列表&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ol&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>数学公式</strong>：主题目录下<code>/hexo/theme/next/_config.yml</code>设置<code>mathjax</code>里的<code>enable: true</code><br>并且在需要使用数学公式的博客里打开公式开关：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>python的点滴</title>
    <url>/2020/02/19/python%E7%82%B9%E6%BB%B4/</url>
    <content><![CDATA[<h2 id="一些函数的使用"><a href="#一些函数的使用" class="headerlink" title="一些函数的使用"></a>一些函数的使用</h2><h3 id="找到当前index在数组位置中对称的位置"><a href="#找到当前index在数组位置中对称的位置" class="headerlink" title="找到当前index在数组位置中对称的位置"></a>找到当前index在数组位置中对称的位置</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">nums = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">index = len(nums)//<span class="number">2</span></span><br><span class="line">print(index) <span class="comment"># 3</span></span><br><span class="line">print(~index) <span class="comment"># -4</span></span><br><span class="line">median = (nums[index] + nums[~index])/<span class="number">2</span></span><br><span class="line">print(median) <span class="comment"># 3.5</span></span><br></pre></td></tr></table></figure>


<h3 id="基本数据类型的时间复制度"><a href="#基本数据类型的时间复制度" class="headerlink" title="基本数据类型的时间复制度"></a>基本数据类型的时间复制度</h3><p><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A61.png" alt="图片一"></p>
<a id="more"></a>

<p><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A62.png" alt="图片二"><br><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A63.png" alt="图片三"></p>
<h3 id="map-zip"><a href="#map-zip" class="headerlink" title="map, zip"></a>map, zip</h3><p>map和zip配合使用，实现矩阵转置</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">原矩阵</span><br><span class="line">matrix = [</span><br><span class="line">  [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],</span><br><span class="line">  [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],</span><br><span class="line">  [<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]</span><br><span class="line">]</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">rotate</span><span class="params">(self, matrix)</span>:</span>  </span><br><span class="line">        matrix[:] = map(list,zip(*matrix))</span><br><span class="line">        <span class="keyword">return</span> matrix</span><br><span class="line">print(Solution().rotate(matrix))</span><br><span class="line">[</span><br><span class="line">    [<span class="number">1</span>,<span class="number">4</span>,<span class="number">7</span>],</span><br><span class="line">    [<span class="number">2</span>,<span class="number">5</span>,<span class="number">8</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">6</span>,<span class="number">9</span>]</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>更多关于<code>map</code>和<code>zip</code>的解释，请看<a href="https://blog.csdn.net/shawpan/article/details/69663710" target="_blank" rel="noopener">一行代码搞定矩阵旋转——python</a>。</p>
<h3 id="defaultdict-function-factory"><a href="#defaultdict-function-factory" class="headerlink" title="defaultdict(function_factory)"></a>defaultdict(function_factory)</h3><p><strong>defaultdict</strong></p>
<p>dict subclass that calls a factory function to supply missing values</p>
<p><code>defaultdict</code>构建的是一个类似<code>dictionary</code>的对象，其中<code>keys</code>值自行确定赋值，但是<code>values</code>的类型是<code>fucntion_factory</code>的类实例，而且<strong>具有默认值</strong>。比如<code>defaultdict(list)</code>创建一个<code>dict</code>,对于任何一个还不存在的<code>dict[newkey]</code>都已经有一个默认<code>list</code>使得<code>dict[newkey].append</code>可以直接运行而不需要先运行<code>dict[newkey]=[]</code>。</p>
<p>例子：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">s = [(<span class="string">'yellow'</span>, <span class="number">1</span>), (<span class="string">'blue'</span>, <span class="number">2</span>), (<span class="string">'yellow'</span>, <span class="number">3</span>), (<span class="string">'blue'</span>, <span class="number">4</span>), (<span class="string">'red'</span>, <span class="number">1</span>)]</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line">d = collections.defaultdict(list)</span><br><span class="line"><span class="keyword">for</span> k, v <span class="keyword">in</span> s:</span><br><span class="line">    d[k].append(v)</span><br><span class="line">list(d.items())</span><br><span class="line">[(<span class="string">'blue'</span>, [<span class="number">2</span>, <span class="number">4</span>]), (<span class="string">'red'</span>, [<span class="number">1</span>]), (<span class="string">'yellow'</span>, [<span class="number">1</span>, <span class="number">3</span>])]</span><br></pre></td></tr></table></figure>

<h3 id="pandas聚合和分组运算之groupby"><a href="#pandas聚合和分组运算之groupby" class="headerlink" title="pandas聚合和分组运算之groupby"></a>pandas聚合和分组运算之groupby</h3><p>根据<code>key1</code>的值<code>a</code>,<code>b</code>来分组求<code>data1</code>的平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>df</span><br><span class="line">data1     data2 key1 key2</span><br><span class="line"><span class="number">0</span> <span class="number">-0.410673</span>  <span class="number">0.519378</span>    a  one</span><br><span class="line"><span class="number">1</span> <span class="number">-2.120793</span>  <span class="number">0.199074</span>    a  two</span><br><span class="line"><span class="number">2</span>  <span class="number">0.642216</span> <span class="number">-0.143671</span>    b  one</span><br><span class="line"><span class="number">3</span>  <span class="number">0.975133</span> <span class="number">-0.592994</span>    b  two</span><br><span class="line"><span class="number">4</span> <span class="number">-1.017495</span> <span class="number">-0.530459</span>    a  one</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grouped = df[<span class="string">'data1'</span>].groupby(df[<span class="string">'key1'</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grouped</span><br><span class="line">&lt;pandas.core.groupby.SeriesGroupBy object at <span class="number">0x04120D70</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>grouped.mean()</span><br><span class="line">key1</span><br><span class="line">a      <span class="number">-1.182987</span></span><br><span class="line">b       <span class="number">0.808674</span></span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>

<p>根据<code>cpucore-</code>的类别，求所有数值形式列的平均值</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>data_mac.groupby(<span class="string">'cpucore'</span>).mean()</span><br><span class="line">cpucore-	time_transform_bert	time_sess_run_bert	time_to_dict_bert	time_all_bert			</span><br><span class="line"><span class="number">1</span>	<span class="number">0.003124</span>	<span class="number">1.550403</span>	<span class="number">0.000032</span>	<span class="number">1.553559</span></span><br><span class="line"><span class="number">2</span>	<span class="number">0.001235</span>	<span class="number">0.730950</span>	<span class="number">0.000034</span>	<span class="number">0.732218</span></span><br><span class="line"><span class="number">3</span>	<span class="number">0.001565</span>	<span class="number">0.487936</span>	<span class="number">0.000037</span>	<span class="number">0.489538</span></span><br><span class="line"><span class="number">4</span>	<span class="number">0.001103</span>	<span class="number">0.323777</span>	<span class="number">0.000029</span>	<span class="number">0.324909</span></span><br><span class="line"><span class="number">5</span>	<span class="number">0.001185</span>	<span class="number">0.289874</span>	<span class="number">0.000033</span>	<span class="number">0.291093</span></span><br><span class="line"><span class="number">6</span>	<span class="number">0.001226</span>	<span class="number">0.284265</span>	<span class="number">0.000033</span>	<span class="number">0.285524</span></span><br><span class="line"><span class="number">0</span>	<span class="number">0.001225</span>	<span class="number">0.284462</span>	<span class="number">0.000032</span>	<span class="number">0.285719</span></span><br></pre></td></tr></table></figure>

<h3 id="is和-的区别"><a href="#is和-的区别" class="headerlink" title="is和==的区别"></a>is和==的区别</h3><p>在Python中一切都是对象。</p>
<p>Python中对象包含的三个基本要素，分别是：</p>
<ul>
<li>id(身份标识)</li>
<li>type(数据类型)</li>
<li>value(值)</li>
</ul>
<p>对象之间比较是否相等可以用<code>==</code>，也可以用<code>is</code>。</p>
<p><code>is</code> 和 <code>==</code> 都是对对象进行比较判断作用的，但对对象比较判断的内容并不相同。下面来看看具体区别在哪</p>
<ul>
<li><p><strong><code>is</code> 比较的是两个对象的 <code>id</code> 值是否相等，也就是比较两个对象是否为同一个实例对象，是否指向同一个内存地址</strong></p>
</li>
<li><p><strong><code>==</code> 比较的是两个对象的内容是否相等，默认会调用对象的 <code>__eq__()</code> 方法</strong></p>
</li>
</ul>
<p><strong>==比较操作符和is同一性运算符区别</strong></p>
<p>==是python标准操作符中的比较操作符，用来比较判断两个对象的value(值)是否相等。</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python3.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a </span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b == a</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a[:]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b == a</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<h3 id="是什么"><a href="#是什么" class="headerlink" title="^是什么"></a>^是什么</h3><p>这是Python中的异或运算符，表示对于操作符两侧数字的二进制数进行异或操作。</p>
<p>异或操作的含义为：同一位置上的数字相等则得0，同一位置上的数字相异则得1。（二进制表达中只有0,1两个数字）</p>
<p>例如：计算2^3首先要求得两数的二进制表达数，（十进制➡️二进制）：2➡️10，3➡️11</p>
<p>按位异或得（二进制➡️十进制）01➡️1</p>
<p>既2^3=1</p>
<h3 id="gt-gt-和-lt-lt-是什么"><a href="#gt-gt-和-lt-lt-是什么" class="headerlink" title="&gt;&gt;= 和 &lt;&lt;=是什么"></a>&gt;&gt;= 和 &lt;&lt;=是什么</h3><p>&gt;&gt; 和 &lt;&lt;都是位运算，对二进制数进bai行移位操作。</p>
<p>&lt;&lt; 是左移，du末位补zhi0，类比十进制数在末尾添0相当于原数乘dao以10，x&lt;&lt;1是将x的二进制表示左移一位，相当于原数x乘2。比如整数4在二进制下是100，4&lt;&lt;1左移1位变成1000(二进制)，结果是8。</p>
<p>&gt;&gt;是右移，右移1位相当于除以2。<br>而&gt;&gt;=和&lt;&lt;=，就是对变量进行位运算移位之后的结果再赋值给原来的变量，可以类比赋值运算符+=和-=可以理解。</p>
<p>比如x&gt;&gt;=2， 就是把变量x右移2位，再保留x操作后的值。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a &gt;&gt;= <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b &lt;&lt;= <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure>


<hr>
<h2 id="python中一些看似简单的运算需求"><a href="#python中一些看似简单的运算需求" class="headerlink" title="python中一些看似简单的运算需求"></a>python中一些看似简单的运算需求</h2><h3 id="求list的绝对值"><a href="#求list的绝对值" class="headerlink" title="求list的绝对值"></a>求list的绝对值</h3><p>求<code>list l = [-1,2,3,-4,-5,-6]</code>的绝对值的三种方法</p>
<ol>
<li><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_abs = list(map(abs, l))</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">list_abs = [abs(i) <span class="keyword">for</span> i <span class="keyword">in</span> l]</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">list_abs = list(np.abs(l))</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="判断某个key值是否在dict中"><a href="#判断某个key值是否在dict中" class="headerlink" title="判断某个key值是否在dict中"></a>判断某个key值是否在dict中</h3><p>有两种方法</p>
<p>一：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成一个字典</span></span><br><span class="line">d = &#123;<span class="string">'name'</span>:&#123;&#125;,<span class="string">'age'</span>:&#123;&#125;,<span class="string">'sex'</span>:&#123;&#125;&#125;</span><br><span class="line"><span class="comment">#打印返回值</span></span><br><span class="line"><span class="keyword">print</span> d.has_key(<span class="string">'name'</span>)</span><br><span class="line"><span class="comment">#结果返回True</span></span><br></pre></td></tr></table></figure>
<p>二：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#生成一个字典</span></span><br><span class="line">d = &#123;<span class="string">'name'</span>:&#123;&#125;,<span class="string">'age'</span>:&#123;&#125;,<span class="string">'sex'</span>:&#123;&#125;&#125;</span><br><span class="line"><span class="comment">#打印返回值，其中d.keys()是列出字典所有的key</span></span><br><span class="line"><span class="keyword">print</span> name <span class="keyword">in</span> d.keys()</span><br><span class="line"><span class="comment">#结果返回True</span></span><br></pre></td></tr></table></figure>
<p>第二中<code>in</code>的方法更好更快，但是还是不知道他的速度是不是O(1)???????</p>
<h3 id="合并大list中的小list"><a href="#合并大list中的小list" class="headerlink" title="合并大list中的小list"></a>合并大list中的小list</h3><p>性能最好的一种方法，来自<a href="https://blog.csdn.net/laiyibeijiweijiu/article/details/93891635" target="_blank" rel="noopener">这里</a>，更多方法对比看<a href="https://blog.csdn.net/weixin_40539892/article/details/79103290" target="_blank" rel="noopener">这里</a></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">a=[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b=[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">c=[<span class="number">4</span>,<span class="number">4</span>,<span class="number">4</span>]</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> chain</span><br><span class="line">d = list(chain(a, b, c))</span><br><span class="line">print(d)</span><br><span class="line"><span class="comment">#[1, 2, 3, 3, 4, 5, 4, 4, 4]</span></span><br></pre></td></tr></table></figure>
<p>上面的方法是a,b,c已经知道，但是在a,b,c还在大list中时，可以如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">newnums = []</span><br><span class="line"><span class="keyword">for</span> l <span class="keyword">in</span> nums:</span><br><span class="line">    newnums.extend(l)</span><br></pre></td></tr></table></figure>


<hr>
<h2 id="python中的加载"><a href="#python中的加载" class="headerlink" title="python中的加载"></a>python中的加载</h2><h3 id="获取当前路径的方式"><a href="#获取当前路径的方式" class="headerlink" title="获取当前路径的方式"></a>获取当前路径的方式</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">print(os.path.abspath(__file__))</span><br><span class="line">print(os.path.dirname(__file__))</span><br></pre></td></tr></table></figure>
<p>获取当前路径的方式有两种：</p>
<ul>
<li>前一种返回的是当前.py的绝对路径</li>
<li>第二种是返回当前.py所在文件夹的位置，可以在后面加入指定字段获取指定文件/路径，如<code>os.path.join(os.path.dirname(__file__),&#39;myData/mydata.csv&#39;)</code></li>
</ul>
<p>输出分别是</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">/Users/shijiaodeng/Documents/Travail/kaggle/<span class="number">4</span>-Quora/Python/data.py</span><br><span class="line">/Users/shijiaodeng/Documents/Travail/kaggle/<span class="number">4</span>-Quora/Python</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h2 id="Hexo-github"><a href="#Hexo-github" class="headerlink" title="Hexo + github"></a>Hexo + github</h2><h3 id="基本需求"><a href="#基本需求" class="headerlink" title="基本需求"></a>基本需求</h3><ol>
<li><p><code>github</code>上创建仓库，仓库名必须严格是<code>username.github.io</code></p>
</li>
<li><p>创建本地写博客的hexo文件夹MyHexo，然后进入文件夹，执行命令行<code>hexo init</code>，如果报错，会提示输入<code>npm install hexo --save</code></p>
</li>
<li><p>进入根目录下的<code>_config.yml</code>，增加</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:username&#x2F;username.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行命令<code>hexo g</code>，如果报错，一般是因为没有安装<code>git</code>，执行<code>npm install hexo-deployer-git --save</code>安装<code>hexo</code>下的<code>git</code>，然后重新<code>hexo g</code></p>
</li>
<li><p>执行<code>hexo d</code>，如果没有关联<code>git</code>和<code>hexo</code>，会自动提醒输入</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Username for &#39;https:&#x2F;&#x2F;github.com&#39;:</span><br><span class="line">Password for &#39;https:&#x2F;&#x2F;github.com&#39;:</span><br></pre></td></tr></table></figure>
<p> 这里我用了<code>ssh</code>钥匙。</p>
<ul>
<li>执行命令<code>ssh-keygen -t rsa -C github_de_email@gmail.com</code></li>
<li>进入<code>~/.ssh</code>复制文件<code>id_rsa.pub</code>里的公匙，进入到<code>github</code>里的<code>setting</code>，进入<code>SSH and GPG keys</code>，新建SSH钥匙<code>new SSH key</code>，粘贴公匙，<code>title</code>可以随意取或者不取，然后确定</li>
<li>运行<code>ssh -T git@github.com</code>，如果看到<code>Hi username! You&#39;ve successfully authenticated, but GitHub does not provide shell access.</code>证明SSH连接成功。</li>
</ul>
</li>
<li><p>自己博客的域名为 <a href="https://shijiaod.github.io/" target="_blank" rel="noopener">https://shijiaod.github.io/</a><br>有时<code>hexo d</code>后页面没有变化，可以尝试重启电脑…</p>
</li>
</ol>
<a id="more"></a>

<h4 id="踩过的坑之血与泪的教训"><a href="#踩过的坑之血与泪的教训" class="headerlink" title="踩过的坑之血与泪的教训"></a>踩过的坑之血与泪的教训</h4><ol>
<li><p><strong>⚠️ 注 ⚠️</strong>：硬是弄了一天才发现，是仓库名见错了，所以怎么<code>hexo d</code>都报错，说<code>ERROR: Repository not found.</code>，后来发现仓库名后面应该跟上<code>github.io</code>，即完整仓库名应该是<code>username.github.io</code>而不是<code>username</code>！</p>
</li>
<li><p>当时在<code>hexo d</code>一直报错要输入<code>username</code>和<code>password</code>时，尝试使用<code>git remote add origin</code>来连接<code>github</code>，但是更换到正确的仓库名后，发现自己删除掉以前建立的<code>origin</code>也不影响，可见这里其实不需要更多的步骤</p>
</li>
<li><p>执行命令<code>ssh -T git@github.com</code>后出现</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The authenticity of host &#39;github.com (13.229.188.59)&#39; can&#39;t be established.</span><br><span class="line">RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)?</span><br></pre></td></tr></table></figure>
<p> 是因为没有配置<code>ssh</code>，或者本地缺少一个文件夹（什么文件夹我也不清楚…）<br> 如果是没有配置<code>ssh</code>见上方如何配置，如果是后者，输入<code>yes</code>，而非回车或者<code>y</code></p>
</li>
<li><p>在升级npm，安装hexo下的git，重新安装hexo时，都会报错</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">No receipt for &#39;com.apple.pkg.CLTools_Executables&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">No receipt for &#39;com.apple.pkg.DeveloperToolsCLILeo&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">No receipt for &#39;com.apple.pkg.DeveloperToolsCLI&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">gyp: No Xcode or CLT version detected!</span><br><span class="line">gyp ERR! configure error</span><br><span class="line">gyp ERR! stack Error: &#96;gyp&#96; failed with exit code: 1</span><br><span class="line">gyp ERR! stack     at ChildProcess.onCpExit (&#x2F;usr&#x2F;local&#x2F;lib&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;node-gyp&#x2F;lib&#x2F;configure.js:351:16)</span><br><span class="line">gyp ERR! stack     at ChildProcess.emit (events.js:210:5)</span><br><span class="line">gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal&#x2F;child_process.js:272:12)</span><br><span class="line">gyp ERR! System Darwin 19.3.0</span><br><span class="line">gyp ERR! command &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;node&quot; &quot;&#x2F;usr&#x2F;local&#x2F;lib&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;node-gyp&#x2F;bin&#x2F;node-gyp.js&quot; &quot;rebuild&quot;</span><br><span class="line">gyp ERR! cwd &#x2F;Users&#x2F;shijiaodeng&#x2F;Documents&#x2F;MyBlog&#x2F;shijiaod&#x2F;node_modules&#x2F;nunjucks&#x2F;node_modules&#x2F;fsevents</span><br><span class="line">gyp ERR! node -v v12.13.1</span><br><span class="line">gyp ERR! node-gyp -v v5.0.5</span><br><span class="line">gyp ERR! not ok</span><br></pre></td></tr></table></figure>
<p> 是升级了mac的原因（到版本10.15.3），所以下载并安装Xcode并同意它的相关协议，问题就解决了。具体详见<a href="https://segmentfault.com/a/1190000021394623?utm_source=tag-newest" target="_blank" rel="noopener">这片文章</a></p>
</li>
</ol>
<h3 id="内容更丰富的博客"><a href="#内容更丰富的博客" class="headerlink" title="内容更丰富的博客"></a>内容更丰富的博客</h3><h4 id="博客主题"><a href="#博客主题" class="headerlink" title="博客主题"></a>博客主题</h4><ol>
<li><p><code>hexo</code>根目录下安装主题nexT<code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code></p>
</li>
<li><p><code>hexo</code>根目录下修改文件<code>_config.yml</code>中的主题为<code>nexT</code>：<code>theme: next</code></p>
</li>
<li><p>到next主题下更改配置文件<code>/hexo/theme/next/_config.yml</code>中的<code>scheme: pisce</code>，里面有四种主题可以选，<code>pisce</code>是经典的旁边有小栏框的格式</p>
</li>
<li><p>我喜欢把<code>sidebar</code>放在右边:<code>/hexo/theme/next/_config.yml</code>中改为</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line"># Sidebar Position.</span><br><span class="line"># position: left</span><br><span class="line">position: right</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入主题目录下的languages文件夹中，<code>cp zh-CN.yml zh-Hans.yml</code>，然后再进入<code>hexo</code>根目录下修改<strong>语言</strong>、名字等一些基本信息（我也不知道为什么要改成<code>zh-Hans</code>而不是直接用<code>zh-CN</code>…）</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Site</span><br><span class="line">title: 诗娇的博客</span><br><span class="line">subtitle: &#39;&#39;</span><br><span class="line">description: &#39;&#39;</span><br><span class="line">keywords:</span><br><span class="line">author: Shijiao DENG</span><br><span class="line">language: zh-Hans</span><br><span class="line">timezone: &#39;&#39;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在<code>/hexo/theme/next/_config.yml</code>中配置<code>avatar</code>设置图像</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: #&#x2F;images&#x2F;avatar.gif  #头像图片路径 图片放置在hexo&#x2F;public&#x2F;images</span><br><span class="line">  rounded: false  #是否显示圆形头像，true表示圆形，false默认</span><br><span class="line">  opacity: 0.7  #透明度0~1之间</span><br><span class="line">  rotated: false  #是否旋转 true表示旋转，false默认</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li><p>进入主题目录下的<code>_config.yml</code>修改主页目录，可以更改配置里的上下顺序来更改他们在主页各自排列的顺序，比如<code>about</code>本来在<code>home</code>下面，我们把它移到了最下面</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F; || home</span><br><span class="line">  #about: &#x2F;about&#x2F; || user</span><br><span class="line">  tags: &#x2F;tags&#x2F; || tags</span><br><span class="line">  categories: &#x2F;categories&#x2F; || th</span><br><span class="line">  archives: &#x2F;archives&#x2F; || archive</span><br><span class="line">  schedule: &#x2F;schedule&#x2F; || calendar</span><br><span class="line">  #sitemap: &#x2F;sitemap.xml || sitemap</span><br><span class="line">  #commonweal: &#x2F;404&#x2F; || heartbeat</span><br><span class="line">  about: &#x2F;about&#x2F; || user</span><br></pre></td></tr></table></figure>
<p> 也可以自己增加一个目录内容，不过这些配置都要与主题目录下的languages文件中对应的<code>yml</code>文档里配置相关联。比如你在站点根目录中的配置文件设置<code>language</code>为<code>zh-Hans</code>，那么就要进入到主题目录下的languages文件中修改<code>zh-Hans.yml</code>，这样才能显示出菜单项新增的中文内容（以something为例子）</p>
</li>
<li><p>前面通过修改next主题下的<code>_config.yml</code>文件中的<code>menu</code>选项，可以在主页面的菜单栏添加”标签”选项，但是此时点击”标签”，跳转的页面会显示”page not found”。此时我们要新建一个页面</p>
<p> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A201.png" alt="图一"><br> 在新建的<code>index.md</code>文件中添加<code>type: &quot;tags&quot;</code><br> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A202.png" alt="图二"><br> <code>categories</code>等同理</p>
</li>
<li><p>设置头像：</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: &#x2F;images&#x2F;avatar.gif  #头像图片路径 图片放置在next&#x2F;source&#x2F;images</span><br><span class="line">  rounded: false  #是否显示圆形头像，true表示圆形，false默认</span><br><span class="line">  opacity: 0.7  #透明度0~1之间</span><br><span class="line">  rotated: false  #是否旋转 true表示旋转，false默认</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>/hexo/theme/next/_config.yml</code>的<code>menu_settings</code>如果设置<code>icon: false</code>则无前面的图标，<code>badges: true</code>则标签都会显示数字</p>
</li>
<li><p>社交设置<code>/hexo/theme/next/_config.yml</code>里的<code>social</code></p>
</li>
<li><p>自动在文章中生成目录，在文章最前面加入<code>[toc]</code>，具体设置如下，但自己试了没有效果</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: false #是否启动侧边栏</span><br><span class="line">  number: true  #自动将列表编号添加到toc。</span><br><span class="line">  wrap: false #true时是当标题宽度很长时，自动换到下一行</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="添加站内搜索"><a href="#添加站内搜索" class="headerlink" title="添加站内搜索"></a>添加站内搜索</h4><ol>
<li>安装站内搜索插件 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></li>
<li>在根目录下的<code>_config.yml</code>添加 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></li>
<li>在<code>themes/next/_config.yml</code>文件中搜索<code>local_search</code>,进行设置 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true  # 设置为true</span><br><span class="line">  trigger: auto  # auto &#x2F;  manual，auto 自动搜索、manual：按回车[enter ]键手动搜索</span><br><span class="line">  top_n_per_article: 3 # 每篇博客显示搜索的结果数</span><br><span class="line">  unescape: true</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="不显示整篇文章"><a href="#不显示整篇文章" class="headerlink" title="不显示整篇文章"></a>不显示整篇文章</h4><p>在文章中想要显示的部分后面加上<code>&lt;!--more--&gt;</code>，从这里开始，后面文章会隐藏，只会显示”阅读全文”按钮</p>
<h4 id="更多惊喜"><a href="#更多惊喜" class="headerlink" title="更多惊喜"></a>更多惊喜</h4><p>还有很多细节，可参见以下两个网站：</p>
<p><a href="https://www.jianshu.com/p/3a05351a37dc" target="_blank" rel="noopener">网站一</a></p>
<p><a href="https://eirunye.github.io/2018/09/15/Hexo搭建GitHub博客—打造炫酷的NexT主题—高级—四/#more" target="_blank" rel="noopener">网站二</a></p>
<h2 id="如何把本地写好的-md文件推倒github网站上"><a href="#如何把本地写好的-md文件推倒github网站上" class="headerlink" title="如何把本地写好的.md文件推倒github网站上"></a>如何把本地写好的.md文件推倒github网站上</h2><h3 id="传统方式"><a href="#传统方式" class="headerlink" title="传统方式"></a>传统方式</h3><ul>
<li>进入到根目录（<code>/Documents/xxxx.github.io/</code>）下，输入命令行<code>hexo g</code> (<code>hexo generate</code>)生成静态页面</li>
<li>输入命令<code>hexo d</code> (<code>hexo deploy</code>)上传到<code>github</code>上<br>这时就可以在网站上看到刚刚写的博客了。<br>如果想要先本地预览效果，可以使用<code>hexo s</code> (<code>hexo server</code>)</li>
</ul>
<h3 id="针对我个人的本地编写文件并推到github上"><a href="#针对我个人的本地编写文件并推到github上" class="headerlink" title="针对我个人的本地编写文件并推到github上"></a>针对我个人的本地编写文件并推到github上</h3><ul>
<li>进入到（<code>/Documents/MyBlog/hexo</code>）下</li>
<li>执行 <code>hexo new &quot;name of file&quot;</code>，生成新的文件</li>
<li>对文件进行编辑</li>
<li>输入命令行 <code>hexo g</code></li>
<li>输入命令 <code>hexo d</code> </li>
</ul>
<h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h3><h3 id="添加评论等"><a href="#添加评论等" class="headerlink" title="添加评论等"></a>添加评论等</h3><h3 id="增加归档页面文章数目"><a href="#增加归档页面文章数目" class="headerlink" title="增加归档页面文章数目"></a>增加归档页面文章数目</h3>]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>数学公式</title>
    <url>/2020/02/19/%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="最优法"><a href="#最优法" class="headerlink" title="最优法"></a>最优法</h1><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>牛顿法迭代法解非线性方程，是把非线性方程$f(x)=0$线性化的一种近似方法。把$f(x)$在点$x_0$的某领域内展开成泰勒级数$f(x)=f(x_0)+f^{‘}(x_0)(x-x_0)+ \frac{f^{‘’}(x_0)(x-x_0)^2}{2!}+…+ \frac{f^{n}(x_0)(x-x_0)^n}{n!}+R_n(x)$，取其线性部分，即展开式的前两项$f(x_0)+f^{‘}(x_0)(x-x_0)$，令其等于0，以此作为非线性方程$f(x)=0$的近似方程，若$f^{‘}(x_0) \neq 0$，则解为$x_1=x_0- \frac {f(x_0)} {f^{‘}(x_0)}$，这样我们得到牛顿迭代法的一个迭代关系式</p>
<p>Python代码实例展示求解方程$(x-3)^3=0$的根</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-3</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fd</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newtonMethod</span><span class="params">(n, assum)</span>:</span></span><br><span class="line">    time = n</span><br><span class="line">    x = assum</span><br><span class="line">    Next = <span class="number">0</span></span><br><span class="line">    A = f(x)</span><br><span class="line">    B = fd(x)</span><br><span class="line">    print(<span class="string">'A = '</span> + str(A) + <span class="string">',B = '</span> + str(B) + <span class="string">',time = '</span> + str(time))</span><br><span class="line">    <span class="keyword">if</span> f(x) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> time, x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Next = x - A/B</span><br><span class="line">        print(<span class="string">'Next x = '</span> + str(Next))</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 1.</span></span><br><span class="line">    <span class="comment">## 这里设置的迭代条件不是当前值和最优值之间的距离，而是相比于上一个值的变化要小于某个值</span></span><br><span class="line">    <span class="comment">## 但我们要求的就是最优值，所以我们并不知道最优值的具体值，不能的到他们的距离</span></span><br><span class="line">    <span class="comment">## 所以要通过每次A的变化，来估计最优值</span></span><br><span class="line">    <span class="keyword">if</span> A - f(Next) &lt; <span class="number">1e-6</span>: </span><br><span class="line">        print(<span class="string">"Meet f(x) = 0, x = "</span> + str(Next))</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 2.</span></span><br><span class="line">    <span class="comment"># ## 所以这里其实是不是也可以通过x的变化，来估计最优值？</span></span><br><span class="line">    <span class="comment"># if A/B &lt; 1e-4:</span></span><br><span class="line">    <span class="comment">#     print("Meet f(x) = 0, x = " + str(Next))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## 3.</span></span><br><span class="line">    <span class="comment"># ## 这里当然也能用 if time &gt; m 来设置跳出循环的条件，但是相对 A - f(Next) 得到的值会更不精确一些</span></span><br><span class="line">    <span class="comment"># if time &gt; 10:</span></span><br><span class="line">    <span class="comment">#     print("Meet f(x) = 0, x = " + str(Next))</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> newtonMethod(n+<span class="number">1</span>, Next)</span><br><span class="line"></span><br><span class="line">newtonMethod(<span class="number">0</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h2 id="其它迭代算法"><a href="#其它迭代算法" class="headerlink" title="其它迭代算法"></a>其它迭代算法</h2><h3 id="欧几里得算法"><a href="#欧几里得算法" class="headerlink" title="欧几里得算法"></a>欧几里得算法</h3><p>最大公约数：Greatest Common Divisor(GCD)</p>
<h4 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h4><p>欧几里德算法是用来求两个正整数最大公约数的算法。是由古希腊数学家欧几里德在其著作《The Elements》中最早描述了这种算法,所以被命名为欧几里德算法。<br>扩展欧几里德算法可用于RSA加密等领域。</p>
<p>假如需要求 1997 和 615 两个正整数的最大公约数,用欧几里德算法，是这样进行的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1997 &#x2F; 615 &#x3D; 3 (余 152)</span><br><span class="line">615 &#x2F; 152 &#x3D; 4(余7)</span><br><span class="line">152 &#x2F; 7 &#x3D; 21(余5)</span><br><span class="line">7 &#x2F; 5 &#x3D; 1 (余2)</span><br><span class="line">5 &#x2F; 2 &#x3D; 2 (余1)</span><br><span class="line">2 &#x2F; 1 &#x3D; 2 (余0)</span><br></pre></td></tr></table></figure>
<p>至此，最大公约数为1<br>以除数和余数反复做除法运算，当余数为 0 时，取当前算式除数为最大公约数，所以就得出了 1997 和 615 的最大公约数 1。</p>
<h4 id="计算证明"><a href="#计算证明" class="headerlink" title="计算证明"></a>计算证明</h4><p>其计算原理依赖于下面的定理：</p>
<p>定理：两个整数的最大公约数等于其中较小的那个数和两数相除余数的最大公约数。最大公约数（Greatest Common Divisor）缩写为GCD。<br>$gcd(a,b) = gcd(b,a \mod b)$ (不妨设$a&gt;b$ 且$r=a \mod b$ ,$r$不为$0$)</p>
<h5 id="证法一"><a href="#证法一" class="headerlink" title="证法一"></a>证法一</h5><ul>
<li><p>正</p>
<ul>
<li>$a$可以表示成$a = kb + r$（$a$，$b$，$k$，$r$皆为正整数，且$r&lt;b$），则$r = a \mod b$</li>
<li>假设$d$是$a$,$b$的一个公约数，记作$d|a$,$d|b$，即$a$和$b$都可以被$d$整除。</li>
<li>而$r = a - kb$，两边同时除以$d$，$r/d=a/d-kb/d=m$，由等式右边可知$m$为整数，因此$d|r$</li>
<li>因此$d$也是$b$,$a \mod b$的公约数</li>
</ul>
</li>
<li><p>反</p>
<ul>
<li>假设$d$是$b$,$a \mod b$的公约数, 则$d|b$,$d|(a-k*b)$,$k$是一个整数。</li>
<li>进而$d|a$，因此$d$也是$a$,$b$的公约数</li>
<li>因此$(a,b)$和$(b,a \mod b)$的公约数是一样的，其最大公约数也必然相等</li>
</ul>
</li>
<li><p>得证</p>
</li>
</ul>
<h5 id="证法二"><a href="#证法二" class="headerlink" title="证法二"></a>证法二</h5><p>假设c = gcd(a,b),则存在m,n，使a = mc, b = nc;<br>令r = a mod b，即存在k，使r = a-kb = mc - knc = (m-kn)c;<br>故gcd(b,a mod b) = gcd(b,r) = gcd(nc,(m-kn)c) = gcd(n,m-kn)*c;<br>假设d = gcd(n,m-kn), 则存在x,y, 使n = xd, m-kn = yd; 故m = yd+kn = yd+kxd = (y+kx)d;<br>故有a = mc = (y+kx)dc, b = nc = xdc; 可得 gcd(a,b) = gcd((y+kx)dc,xdc) = dc;<br>由于gcd(a,b) = c, 故d = 1;<br>即gcd(n,m-kn) = 1, 故可得gcd(b,a mod b) = c;<br>故得证gcd(a,b) = gcd(b,a mod b).<br>注意:两种方法是有区别的。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>自己写的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    c = a%b</span><br><span class="line">    <span class="keyword">return</span> print(b) <span class="keyword">if</span> c==<span class="number">0</span> <span class="keyword">else</span> gcd(b, c)</span><br><span class="line"></span><br><span class="line">gcd(<span class="number">1997</span>, <span class="number">615</span>)</span><br></pre></td></tr></table></figure>

<p>百度百科上的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> a != <span class="number">0</span>:</span><br><span class="line">        a, b = b % a, a</span><br><span class="line">    <span class="keyword">return</span> print(b)</span><br><span class="line"></span><br><span class="line">gcd(<span class="number">1997</span>, <span class="number">615</span>)</span><br></pre></td></tr></table></figure>

<h4 id="扩展欧几里得算法"><a href="#扩展欧几里得算法" class="headerlink" title="扩展欧几里得算法"></a>扩展欧几里得算法</h4><p>扩展欧几里得算法（英语：Extended Euclidean algorithm）是欧几里得算法（又叫辗转相除法）的扩展。已知整数$a$、$b$，扩展欧几里得算法可以在求得$a$、$b$的最大公约数的同时，能找到整数$x$、$y$（其中一个很可能是负数），使它们满足贝祖等式</p>
<p>$ax+by=gcd(a,b)$</p>
<p>如果a是负数，可以把问题转化成$|a|(-x)+by=gcd(|a|,b)$，然后令$x^{‘}=(-x)$。</p>
<p>扩展欧几里得算法可以用来计算模反元素(也叫模逆元)，而模反元素在RSA加密算法中有举足轻重的地位。</p>
<h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ext_euclid</span><span class="params">(a, b)</span>:</span>     </span><br><span class="line">    <span class="keyword">if</span> b == <span class="number">0</span>:         </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>, <span class="number">0</span>, a     </span><br><span class="line">    <span class="keyword">else</span>:         </span><br><span class="line">        x, y, q = ext_euclid(b, a % b) </span><br><span class="line">        <span class="comment"># q = gcd(a, b) = gcd(b, a%b)   </span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 这一行怎么求的，有时间可以论证一下      </span></span><br><span class="line">        x, y = y, (x - (a // b) * y)     </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, y, q</span><br></pre></td></tr></table></figure>

<h3 id="斐波那契数列"><a href="#斐波那契数列" class="headerlink" title="斐波那契数列"></a>斐波那契数列</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>斐波那契数列（Fibonacci sequence），又称黄金分割数列、因数学家列昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：1、1、2、3、5、8、13、21、34、……在数学上，斐波那契数列以如下被以递推的方法定义：F(1)=1，F(2)=1, F(n)=F(n - 1)+F(n - 2)（n ≥ 3，n ∈ N*）在现代物理、准晶体结构、化学等领域，斐波纳契数列都有直接的应用，为此，美国数学会从 1963 年起出版了以《斐波纳契数列季刊》为名的一份数学杂志，用于专门刊载这方面的研究成果。</p>
<h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#################### 输出第n个斐波那契数 ####################</span></span><br><span class="line"><span class="meta">@fn_timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci1</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="comment">## 这种方法，每次都要嵌套循环，不能记录上一次的结果之间用，所以时间复杂度非常高！！</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> fib(n<span class="number">-1</span>) + fib(n<span class="number">-2</span>)</span><br><span class="line">    <span class="keyword">return</span> fib(n)</span><br><span class="line"></span><br><span class="line"><span class="meta">@fn_timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci2</span><span class="params">(n)</span>:</span></span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    i = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= n:</span><br><span class="line">        l.append(l[i<span class="number">-1</span>]+l[i<span class="number">-2</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> l[i<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(fibonacci1(<span class="number">20</span>))</span><br><span class="line">print(fibonacci2(<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#################### 输出前n个斐波那契数 ####################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(n)</span>:</span></span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    i = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; n:</span><br><span class="line">        l.append(l[i<span class="number">-1</span>]+l[i<span class="number">-2</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line">print(fibonacci(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

<p>可以尝试用代码画出斐波那契函数的曲线 斐波那契弧线<br><img src="/public/images/image/fibonacci.jpg" alt="fibonacci"></p>
]]></content>
      <categories>
        <category>数学公式</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>牛顿法</tag>
        <tag>最优法</tag>
        <tag>迭代算法</tag>
      </tags>
  </entry>
  <entry>
    <title>算法刷题总结</title>
    <url>/2020/02/19/leetcode/%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<ol>
<li>如果有搜索类循环，可以用dict来储存，这样搜索时间为O(1)，如果不行，可以考虑二叉树搜索，二叉树搜索时间小于O(n)<ol>
<li>如找重复的数值</li>
<li>小于/大于某个值的数值等</li>
</ol>
</li>
</ol>
<h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><h3 id="字符串反转"><a href="#字符串反转" class="headerlink" title="字符串反转"></a>字符串反转</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">'abc' =&gt; 'cba'</span><br></pre></td></tr></table></figure>
<p>方法一：</p>
<p>用两个指针一起从两头往中间前进，每次都对调两个指针对应的数值，不需要区分数组单双数的情况。</p>
<h3 id="最长不重复字串"><a href="#最长不重复字串" class="headerlink" title="最长不重复字串"></a>最长不重复字串</h3><p>LeetCode 3. Longest Substring Without Repeating Characters</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Given a string, find the length of the longest substring without repeating characters.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: &quot;abcabcbb&quot;</span><br><span class="line">Output: 3 </span><br><span class="line">Explanation: The answer is &quot;abc&quot;, with the length of 3. </span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: &quot;bbbbb&quot;</span><br><span class="line">Output: 1</span><br><span class="line">Explanation: The answer is &quot;b&quot;, with the length of 1.</span><br><span class="line"></span><br><span class="line">Example 3:</span><br><span class="line"></span><br><span class="line">Input: &quot;pwwkew&quot;</span><br><span class="line">Output: 3</span><br><span class="line">Explanation: The answer is &quot;wke&quot;, with the length of 3. </span><br><span class="line">Note that the answer must be a substring, &quot;pwke&quot; is a subsequence and not a substring.</span><br></pre></td></tr></table></figure>

<p>方法一：</p>
<ol>
<li>做一次for循环</li>
<li>每一步更新指针start的位置<ol>
<li>当start后面有重复的字符，start跳到重复的字符当前位置</li>
<li>当start后面有重复其前面的字符，但是在start后面没有重复，dict会在后面更新，但是start指针不调整</li>
</ol>
</li>
<li>核心代码如下<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> j, string <span class="keyword">in</span> enumerate(s):</span><br><span class="line">  <span class="keyword">if</span> string <span class="keyword">in</span> dictOfsubstring:</span><br><span class="line">    start = max(start, dictOfsubstring[string]+<span class="number">1</span>)</span><br><span class="line">  maxlen = max(maxlen, j-start+<span class="number">1</span>)</span><br><span class="line">  dictOfsubstring.update(&#123;string:j&#125;)</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>方法二：</p>
<h3 id="最长对称字符串"><a href="#最长对称字符串" class="headerlink" title="最长对称字符串"></a>最长对称字符串</h3><p>LeetCode 5. Longest Palindromic Substring</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Given a string s, find the longest palindromic substring in s. You may assume that the maximum length of s is 1000.</span><br><span class="line"></span><br><span class="line">Example 1:</span><br><span class="line"></span><br><span class="line">Input: &quot;babad&quot;</span><br><span class="line">Output: &quot;bab&quot;</span><br><span class="line">Note: &quot;aba&quot; is also a valid answer.</span><br><span class="line"></span><br><span class="line">Example 2:</span><br><span class="line"></span><br><span class="line">Input: &quot;cbbd&quot;</span><br><span class="line">Output: &quot;bb&quot;</span><br></pre></td></tr></table></figure>

<p>方法一：</p>
<p>暴力解法，两次for循环，然后找出最大的对称字符串</p>
<p>方法二：</p>
<ol>
<li><p>一个for循环</p>
</li>
<li><p>加上对称判断。对称只有两种可能性：</p>
<ol>
<li>有一个中心字符，如’aba’</li>
<li>没有中心字符，如’abba’</li>
</ol>
<p>每当找到某种的中心点时，找出其可以向两边扩展的最远距离（例如情况ii时，代码如下），然后通过记录中心点和扩展的范围，得到对称字符串本身或者长度</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(len(s)):</span><br><span class="line"> ...</span><br><span class="line"> <span class="keyword">if</span> i+<span class="number">1</span>&lt;len(s) <span class="keyword">and</span> s[i] == s[i+<span class="number">1</span>]:</span><br><span class="line">     mid = <span class="number">1</span></span><br><span class="line">     count1 = <span class="number">2</span></span><br><span class="line">     <span class="keyword">while</span> i-mid&gt;=<span class="number">0</span> <span class="keyword">and</span> i+<span class="number">1</span>+mid&lt;len(s) <span class="keyword">and</span> s[i-mid] == s[i+<span class="number">1</span>+mid]:</span><br><span class="line">         count1 += <span class="number">2</span></span><br><span class="line">         mid += <span class="number">1</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>字符串相关问题，好像都会上指针。最重要的点，一是上一个还是两个指针，二是指针的更新方法。</p>
<h2 id="字符列"><a href="#字符列" class="headerlink" title="字符列"></a>字符列</h2><h3 id="最长上升子序列"><a href="#最长上升子序列" class="headerlink" title="最长上升子序列"></a>最长上升子序列</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Given an unsorted array of integers, find the length of longest increasing subsequence.</span><br><span class="line"></span><br><span class="line">Example:</span><br><span class="line"></span><br><span class="line">Input: [10,9,2,5,3,7,101,18]</span><br><span class="line">Output: 4 </span><br><span class="line">Explanation: The longest increasing subsequence is [2,3,7,101], therefore the length is 4. </span><br><span class="line"></span><br><span class="line">Note:</span><br><span class="line">There may be more than one LIS combination, it is only necessary for you to return the length.</span><br><span class="line">Your algorithm should run in O(n2) complexity.</span><br><span class="line">Follow up: Could you improve it to O(n log n) time complexity?</span><br></pre></td></tr></table></figure>

<p>方法一：</p>
<ol>
<li>用dp记录到当前数值位置，能有的最长上升子序列，初始化为1，即最长上升子序列为当前数值本身</li>
<li>两次for循环（即两个指针），一次记录到当前位置最长上升子序列的，一次是在当前位置前，前面位置上有的最大上升子序列</li>
<li>注意⚠️：如果第一个指针（当前位置）对应的数值num大于第二个指针（当前位置前面的指针）的数值value，那么当前位置的dp[i]更新为max(dp[i], dp[j]+1)，j为第二个指针的位置</li>
<li>代码如下：</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">IncreasingSubsets</span><span class="params">(nums)</span>:</span></span><br><span class="line">  <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">      <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"></span><br><span class="line">  dp = [<span class="number">1</span>]*len(nums)</span><br><span class="line">  <span class="keyword">for</span> i <span class="keyword">in</span> range(len(nums)):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> range(i):</span><br><span class="line">          <span class="keyword">if</span> nums[j] &lt; nums[i]:</span><br><span class="line">              dp[i] = max(dp[j]+<span class="number">1</span>, dp[i])</span><br><span class="line">  <span class="keyword">return</span> max(dp)</span><br></pre></td></tr></table></figure>

<p>方法二：</p>
<ol>
<li>用dp记录最长子序列的值</li>
<li>for循环一遍nums，把每次的num二分查找插入到dp中</li>
</ol>
<h2 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h2>]]></content>
      <categories>
        <category>数学公式</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>牛顿法</tag>
        <tag>最优法</tag>
        <tag>迭代算法</tag>
      </tags>
  </entry>
  <entry>
    <title>AM-softmax</title>
    <url>/2020/02/19/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/AM-softmax/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>基于已有的L-softmax, A-softmax，作者提出了AM-softmax，一个适用于分类模型的loss计算方法。主要用于增加类间距离，减小类内距离</p>
<h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><h2 id="反向传播求导简单"><a href="#反向传播求导简单" class="headerlink" title="反向传播求导简单"></a>反向传播求导简单</h2><p>反向传播对x求导结果为1</p>
<h2 id="公式更直观"><a href="#公式更直观" class="headerlink" title="公式更直观"></a>公式更直观</h2><p>相对于L-softmax，AM-softmax的参数简单直观</p>
<h2 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h2><p>使用加法cos(/delta)-m，不同于L-softmax中使用乘法cos(m*/delta)，超参数直接作用于cos(/delta)更好，因为：cos(/delta)和delta虽然是一一对应，但是变化趋势不同，cos(/delta)在pai处更密集。因此在乘法中，得到wf后需要进行反余弦计算，因此会消耗更多的算力</p>
<h2 id="两个超惨数"><a href="#两个超惨数" class="headerlink" title="两个超惨数"></a>两个超惨数</h2><p>e的右上角公式为s(cos(/delta)-m)</p>
<ul>
<li>其中m是用来增加类间距离，强行使两个类的边缘样本有一定的距离而更不容易混淆</li>
<li>反向传播是，x的导数为1，前面增加一个s则反向求导后gradient会变为s，使用一个s是为了让好样本的梯度更大，来减小类内样本的方差，也就是更快的减小类内距离</li>
</ul>
<h2 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h2><p>对于质量较好的样本，不需要进行feature normalization。<br>但是对于质量较差的样本，使用feature normalization后，效果会有提升。<br>因为质量越差，特征归一化的值越小，因此反向传播中受到的关于越大（1/alpha），所以特征归一化更适合图像质量差的任务</p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/LogisticRegression/</url>
    <content><![CDATA[<h1 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h1>]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>LR</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Bert/</url>
    <content><![CDATA[<h1 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h1><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>Bert中使用的是Layer Normalization，源于对batch normalization的改进。<br>batch normalization的优点：</p>
<ul>
<li>加快神经网络训练时间</li>
<li>对学习率有更好的容忍</li>
<li>重新拉正数据分布，避免进入激活函数的饱和区域，缓解梯度消失<br>batch normalization的缺点：</li>
<li>对batch size的大小比较敏感</li>
<li>在RNN和LSTM等序列模型中效果不太好<br>针对以上两个BN的缺点，提出了LN，较好的解决了上面两个问题</li>
</ul>
<h3 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h3><p>同transformer里的position embedding不同，这里的position embedding需要训练获取。<br>transformer里的position embedding用的的正弦版本。两者各有优劣<br>正弦：</p>
<ul>
<li>一定位置间的embedding可以被彼此线性表示</li>
<li>相比训练好的position embedding，正弦版本能允许更长的输入序列<br>训练的position embedding</li>
<li>相比正弦的常量表示，学习来的embedding表示能力更强。加上bert后续需要做ner对位置上的字符进行状态预测，因此一个可以学习的参数来表达位置，更合理</li>
</ul>
<h3 id="gelu"><a href="#gelu" class="headerlink" title="gelu"></a>gelu</h3><p>Bert Transfromer结构中使用了这个激活函数—gelu（Gaussian error linear units，高斯误差线性单元），Gelu在论文中已经被验证，是一种高性能的神经网络激活函数，因为GELU的非线性变化是一种符合预期的随机正则变换方式（这句话，说实话，我翻译自原论文，具体怎么理解呢？我自己是如下理解的）。</p>
<p>激活函数的作用：给网络模型加入非线性因子，这个非线性因子的实际操作就是在wx+b这样的线下变化后面加入一个非线性变化的函数fun。<br>Gelu的操作方式：Gelu怎么完成非线性变换的呢？引入这样的变化函数：<br>        公式中x是自己，P(X&lt;=x)决定x中有多少信息保留 ，并且由于P是服从高斯分布的，也就满足了非线性的特征，并且更加符合数据的分布预期。</p>
<h3 id="warm-up"><a href="#warm-up" class="headerlink" title="warm-up"></a>warm-up</h3><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><h2 id="获取向量"><a href="#获取向量" class="headerlink" title="获取向量"></a>获取向量</h2><p>bert中有很多种获取向量的方法，具体如下：</p>
<p><strong>init</strong>()：重头戏，模型的构建在此完成，三步走完成。<br>主要分为三个模块：embeddings、encoder 和 pooler。<br>首先构建输入，包括 input_ids、input_mask 等。其次进入 embeddings 模块，进行一系列 embedding 操作，涉及 embedding_lookup() 和 embedding_postprocessor() 两个函数。<br>然后进入 encoder 模块，就是 transformer 模型和 attention 发挥作用的地方了，主要涉及 transformer_model() 函数，得到 encoder 各层输出。<br>最后进入 pooler 模块，只取 encoder 最后一层的输出的第一个 token 的信息，送入到一个大小为 hidden_size 的全连接层，得到 pooled_output，这就是最终输出了。</p>
<ul>
<li>get_pooled_output(self)：获取 pooler 的输出。</li>
<li>get_sequence_output(self)：获取 encoder 最后的隐层输出，输出大小为 [batch_size, seq_length, hidden_size]。</li>
<li>get_all_encoder_layers(self)：获取 encoder 中所有层。返回大小应该是 [num_hidden_layers, batch_size, seq_length, hidden_size]。</li>
<li>get_embedding_output(self)：获取对 input_ids 的 embedding 结果，大小为 [batch_size, seq_length, hidden_size]，这是 word embedding、positional embedding、token type embedding（论文中的 segment embedding）和 layer normalization 一系列操作的结果，也是 transformer 的输入。</li>
<li>get_embedding_table(self)：获取 embedding table，大小为 [vocab_size, embedding_size]，即词汇表中的词对应的 embedding。</li>
</ul>
<p>详情见<a href="https://zhuanlan.zhihu.com/p/140718739" target="_blank" rel="noopener">Bert是如何构建模型的</a></p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/SBert/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>SBert针对句子匹配问题，提出了一种新的想法。没有很复杂的公式，可以理解为，是一些常见的计算进行各种融合的实战经验总结。<br>主要思想也是对bert进行fine tuning，来获取语义上有意义的空间向量<br>    语义上有意义：论文中有解释，是指句子意思相近的句子在空间向量上也相近</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><h3 id="速度快"><a href="#速度快" class="headerlink" title="速度快"></a>速度快</h3><p>论文中提出了，相对于通常使用的NSP，即concat sent1和sent2来判断两个句子是否相似，我们将两个句子独立分开输入到同一bert模型（两个句子使用的参数相同）中，再将得到的两个向量进行计算，要快上很多倍，主要原因如下：</p>
<ol>
<li>假设有10000个样本<ol>
<li>如果用NSP句子对计算，那么可以相互匹配出10000*10000个句子对，去除重复的也有一半，所以样本数上计算量很大</li>
<li>如果将句子分开过bert模型，那么10000个样本只用过10000次bert模型，计算量会少很多</li>
</ol>
</li>
<li>每个样本过bert模型的计算量也会不同<ol>
<li>如果用NSP句子对，假设两个句子的长度分别为m和n，那么concate之后进行self attention，就是O（（m+n）^2）的时间复杂度</li>
<li>如果两个句子独立进行self attention，两个句子的时间复杂度也是O（m^2） while m &gt;= n</li>
</ol>
</li>
</ol>
<h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><p>在bert的embedding输出层，接了一个pooling。<br>作者比较了三种pooling方法，一种直接使用CLS的结果，一种使用mean，一种使用max。使用mean的效果最好<br>作者并没有在论文中详细阐述这个mean的pooling是作用在哪个维度上的，我个人推测可能是对seq_length做的mean，这样才能和cls层出来的embedding维度相同<br>找到一个解释是：</p>
<ul>
<li>MEAN策略，计算各个token输出向量的平均值代表句子向量。各个token的向量维度一样，即把所有token的embeding相加，然后处以token数即得到平均，也就是这个句子的句向量</li>
<li>MAX策略，取所有输出向量各个维度的最大值代表句子向量。各个token有很多个维度，每个维度上比较多个token当前维度的值，并取最大，然后获得句向量<br>所以以上都是对seq_length这一行进行了pooling，将其变为了1</li>
</ul>
<p>具体cnn里filter和pooling后的维度见<a href="https://mp.weixin.qq.com/s/vq4-4M46okms5_sxw9PBQA" target="_blank" rel="noopener">实战TextCNN 文本分类</a></p>
<h3 id="针对不同的数据提出了三种loss"><a href="#针对不同的数据提出了三种loss" class="headerlink" title="针对不同的数据提出了三种loss"></a>针对不同的数据提出了三种loss</h3><h4 id="classification-objective-function"><a href="#classification-objective-function" class="headerlink" title="classification objective function"></a>classification objective function</h4><p>在将两个句子的embedding，u和v，concate时，作者实验了很多种不同的方法<br>比如：（u,v），（|u-v|），（u,v,|u-v|）等等，可以到时候放个结果图过来，论文中的table6。<br>作者最后发现（u,v,|u-v|）这种concate方式效果最好</p>
<h4 id="regression-objective-function"><a href="#regression-objective-function" class="headerlink" title="regression objective function"></a>regression objective function</h4><p>计算两个句子的余弦相似性，然后使用均方差loss</p>
<h4 id="triplet-objective-function"><a href="#triplet-objective-function" class="headerlink" title="triplet objective function"></a>triplet objective function</h4><p>如果是有正负样本的数据，可以使用triplet loss<br>max(dist(sa-sp)-dist(sa-sn)+margin, 0)<br>这里dist()计算的两者欧式距离，并且把margin设为1</p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/SVM/</url>
    <content><![CDATA[<h1 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h1><p>核函数的本质</p>
<p>上面说了这么一大堆，读者可能还是没明白核函数到底是个什么东西？我再简要概括下，即以下三点：</p>
<ol>
<li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(如上文2.2节最开始的那幅图所示，映射到高维空间后，相关特征便被分开了，也就达到了分类的目的)；</li>
<li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(如上文中19维乃至无穷维的例子)。那咋办呢？</li>
<li>此时，核函数就隆重登场了，核函数的价值在于它虽然也是讲特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。</li>
</ol>
<p><a href="https://blog.csdn.net/u010199356/article/details/88836026" target="_blank" rel="noopener">核函数解释博客</a><br>很有意思，有空可以自己试试用matlab画图看看</p>
<p><a href="https://zhuanlan.zhihu.com/p/77750026" target="_blank" rel="noopener">SVM推导过程</a><br>也可以看看李航的统计书里的推导</p>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>Triplet-Loss</title>
    <url>/2020/02/19/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/Triplet-Loss/</url>
    <content><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Triplet-Loss最先由图像里提出来。为了解决负样本与标准样本过于相似的问题，来增加类间距离，减小类内距离<br>Triplet-Loss通过最小化dist(a,p)-dist(a,n)+margin，来训练出可以更好表征样本的embedding<br>dist()一般使用欧式距离，当然也可以用余弦等其它距离</p>
<h1 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h1><h2 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h2><p>使用triplet-loss通常需要将样本组合成三元组，&lt;a, p, n&gt;<br>如何组合有很多中方法，最直接的一种，将所有正样本和所有可能的负样本与标准样本一一组成三元组，这样可组合的可能性过多，这种方法并不适用，所以作者提出了另外两种方法offline和online</p>
<h2 id="offline"><a href="#offline" class="headerlink" title="offline"></a>offline</h2><p>每n steps后，使用当前模型，计算出离每个标准问最远的正样本和最近的负样本，然后重新接着训练</p>
<h2 id="online"><a href="#online" class="headerlink" title="online"></a>online</h2><p>从batch-size里选取正样本和负样本（我看很多文章里也写不清楚到底是怎么取，以下是个人理解）<br>每个batch-size里每个类别要取40个样本，然后随机选取一些负样本加入进来。batch-size大小最好为1800<br>我的思考：</p>
<ul>
<li>每个batch-size不需要包含所有类别，可以每次包含k=5个类别，每个类别选取40个样本放入，共有200个样本，然后再选取1600个负样本放入</li>
<li>接着对于batch-size中的每一个类别，我们选取40次正样本，然后从剩下的1800-40个负样本中随机选取负样本</li>
<li>每个类别我们总共组成40个三元组，这个batch-size我们会组成200个三元组</li>
<li>之所以要每个类别要选取40个正样本，是为了保证此类别中正样本可选取的数量不要太少，因为在组成三元组样本时，经常会碰到负样本过多的问题，这里强行规定的一定量的正样本是的两种样本比在batch-size中不至于太失衡</li>
</ul>
<h2 id="semi-hard"><a href="#semi-hard" class="headerlink" title="semi-hard"></a>semi-hard</h2><p>在论文中，作者解释如果每次都取hardest sample，即离标准样本最近的负样本，容易导致模型过早限于局部最小值（如果learning rate没足够大到能跳出局部最小区域，就会造成loss不下降的问题），因此我们尝试使用semi-hard sample</p>
<ul>
<li>hardest sample为距离标准样本最近的负样本（有可能距离小于正样本到标准样本的距离）</li>
<li>semi sample为，比正样本距离标准样本的距离很近但稍远一点点的负样本。这类样本虽然不是最难的负样本，但也是很难分的样本，因为他们距离标准样本的距离与正样本相似，因此容易混淆</li>
</ul>
<h2 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h2><ul>
<li>为了让SGD更快下降，batch-size越小越好</li>
<li>为了让triplet loss效果更好，batch-size越大越好</li>
<li>作者选择了batch-size=1800</li>
<li>作者这里margin设为0.2</li>
</ul>
]]></content>
      <categories>
        <category>基础知识</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>FAQ</title>
    <url>/2019/12/08/interview/FAQ/</url>
    <content><![CDATA[<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了降本增效，我们规划了八大任务型场景，并以FAQ为辅，构建智能在线客服。总场景覆盖率是87.6%，其中FAQ覆盖率为16.7%</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>｜顶层分类器｜ =&gt; ｜FAQ｜+｜任务型场景｜+｜闲聊｜</p>
<h2 id="FAQ-1"><a href="#FAQ-1" class="headerlink" title="FAQ"></a>FAQ</h2><p>出去任务型囊括的几大场景外，还有一些并不足以构建场景，但是用户会有疑问的一些问题，我们通过FAQ进行回答。</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>总共大概9W数据，3W原始数据，5.6W扩充数据，总共800多个类别。扩充数据大部分是通过人工术语扩充，即针对一个标准问题，我们要扩充为不同的句式问法。<br>测试集在扩充数据以前，按分布取的8000样本。</p>
<p>（吴恩达的那个测试集数量决定的方法，在这里不太实用，因为数据不够，所以直接取25%也是不错的选择，5%是照顾一下样本少的类别）</p>
<h3 id="清洗"><a href="#清洗" class="headerlink" title="清洗"></a>清洗</h3><p>连续的数字统一清洗为连续的0，单位也要统一</p>
<p>长短句：</p>
<ul>
<li>有不少长句是因为地址过长，所以我们会提取很长的地址，来做数据增广，让模型更多的学会句式，而忽略长地址的影响</li>
<li>有不少长句是因为用户不满，抱怨的句子，这时需要通过顶层分类器，或者任务型场景里的分类器，分到“抱怨”这个类别，然后转人工</li>
<li>还有一些长句，我们会人工进行压缩，只取有用的部分</li>
</ul>
<p>问题：</p>
<ul>
<li>很多非常长的句子，哪怕不是抱怨，我们也会转人工</li>
<li>几次说一个问题，或者对上一个问题追问，我们在FAQ中目前无法解决。比如“我要寄快递”这句话我们回复完了后，如果用户再说一句“加急”，如果是在FAQ中，我们会不能够回答“加急”，会让用户再次组织语言，或者转人工</li>
</ul>
<h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p>扩充数据大部分是通过人工术语扩充，即针对一个标准问题，我们要扩充为不同的句式问法。</p>
<ul>
<li>句式/问法扩充</li>
<li>句子顺序交换</li>
<li>过采样</li>
</ul>
<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>使用的是jieba分词（jieba分词的逻辑是什么样的？效果为什么这么好？）</p>
<p>经过比较后，我们选用公司内部领域内的词向量，效果要比word2vec要好，OOV会少一些，而且不允许词向量微调效果要更好。词向量的效果也比字向量的好。</p>
<p>什么模型中比较的结果？每个模型都是这个比较结果吗？</p>
<ul>
<li>初步设置了一个baseline，用的孪生网络 LSTM + cosine similarty</li>
<li>在这个模型中比较的结果，然后应用到后面的模型中</li>
</ul>
<p>word2vec向量如何得到的</p>
<ul>
<li>有 skip-gram 和 CBOW 两种</li>
</ul>
<p>训练加速方法的逻辑是什么</p>
<ul>
<li>负采样<ul>
<li>取一个正样本，5个负样本，计算得到正样本和不得到负样本的概率</li>
<li>负样本是通过词频大小来取的</li>
<li>num_word/total_num_word 来得到每个词的词频占比，然后在[0, 1]中随机选择数字</li>
</ul>
</li>
<li>分层 softmax<ul>
<li>哈夫曼树</li>
<li>最小路径权重（词频）</li>
</ul>
</li>
</ul>
<p>用的什么训练</p>
<ul>
<li>gensim.word2vec</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>800多个类是怎么分的？</p>
<ul>
<li>人工分的</li>
<li>很大程度上依赖于答案，所以业务同事直接给的类别和答案</li>
</ul>
<p>数据是怎么样的？</p>
<ul>
<li>ID，标准样本，答案，类别</li>
<li>样本最后都按ID以向量的形式储存，选定匹配的序号后，可以直接从取出向量计算</li>
<li>只需要把用户的输入重新过一遍网络来获取他的向量</li>
</ul>
<p>用了哪些模型</p>
<ul>
<li>SVM</li>
<li>LSTM + attention，[a, a_chapeau, a-a_chapeau, a*a_chapeau] + LSTM + CNN =&gt; “分类” and “fc =&gt; similary”</li>
<li>am-softmax + LSTM</li>
</ul>
<p>分别效果如何</p>
<ul>
<li>am-softmax + LSTM 的效果最好</li>
<li>SVM次之</li>
<li>LSTM + attention 最差</li>
</ul>
<p>为什么</p>
<ul>
<li>SVM本身分类效果就很好，配合if-idf + word2vec的特征提取，泛化效果特别好，但是需要大量内存和时间</li>
<li>为了加强句子的匹配，我们尝试了更复杂的模型</li>
<li>LSTM + attention模型过拟合很严重，而且泛化能力较弱，分类效果没有SVM好，最后匹配结果的准确度也下降很多</li>
<li>最后我们尝试了 word2vec + am-softmax + LSTM，既可以分类，也可以匹配，还可以进行多标签分类，结果也达到了期望的效果<ul>
<li>我们真正要做的是特征提取模型</li>
<li>用分类作为训练方案</li>
<li>最后对模型提取的特征进行对比排序</li>
<li>使用 triplet-loss 增加的类间距，能更好的找到阈，很好的增加了直答的比例，但是对于一些多标签的分类，</li>
</ul>
</li>
</ul>
<p>有没有别的模型的想法？还有哪些可以改进的地方？</p>
<p>重难点是什么？</p>
<ul>
<li>SVM虽然分类效果很好，但是可以用来做匹配的信息过少（只有 word embedding），或者需要重新设计一个模型计算相似度，比较复杂</li>
<li>第二种方法更适合文本的直接匹配，不适合先分类+匹配的做法，且计算量很大</li>
<li>am-softmax 主要是在正负样本的选择上</li>
</ul>
<p>如何进行的错误样本分析？</p>
<p>是否引入了熵</p>
<ul>
<li>二分类器，也可以引入熵</li>
<li>二分类器，还可以是多标签的</li>
</ul>
<p>如何处理多意图问题/引入了多标签吗？sigmoid</p>
<ul>
<li>本身是一个二分类问题</li>
<li>在计算的过程中，就是看当前样本是正样本还是负样本</li>
<li>所以相当于有800多个二分类器？？？</li>
</ul>
<p>整体准确率要达到多少？</p>
<p>是否使用了学习曲线</p>
<p>有没有过拟合</p>
<ul>
<li>有</li>
</ul>
<p>loss如何计算的</p>
<ul>
<li>用d_an,d_ap,y，计算出am-softmax</li>
</ul>
<p>是否用了shuffle</p>
<ul>
<li>用了</li>
</ul>
<p>dropout多少</p>
<ul>
<li>0.4（原文0.5）</li>
</ul>
<p>为什么加入高斯噪音？</p>
<ul>
<li>model文件里Model的self.noise</li>
</ul>
<p>triplet-loss里的距离为什么用欧式距离？</p>
<ul>
<li>计算两个句子的距离用的是欧式距离</li>
<li>triplet文件里TripletLoss的dist_mat=euclidean_dist</li>
</ul>
<p>如何计算相似度的？</p>
<ul>
<li>先用欧式距离计算d_an,d_ap，加入y计算他们的am-softmax</li>
<li>相似度就是用的欧式距离</li>
</ul>
<p>还可以怎样计算相似度</p>
<ul>
<li>内积</li>
<li>余弦</li>
<li>曼哈顿</li>
<li>多层感知器网络（MLP）</li>
</ul>
<p>为什么相似度用的欧式距离？有没有尝试过别的方法？</p>
<ul>
<li>最先尝试的内积和余弦，但是效果没有欧式距离好</li>
<li>后来也尝试过曼哈顿</li>
</ul>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p>覆盖率，成功率，减少客服80%咨询时间，平均降低人工咨询市场50s</p>
<h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p>样本不均衡</p>
<ul>
<li>数据增广</li>
<li>采样<ul>
<li>过采样：复制 + SMOTE过采样 + 加入噪音等生成数据</li>
<li>欠采样：删除数据 + 将多样本类的样本分为N分，只取每份的中心数据作为样本</li>
</ul>
</li>
<li>loss权重调节</li>
<li>组合集成方法，组合多个分类器，每个分类器每个类的数据是随机分成的<strong>少数类数据的量</strong></li>
<li>特征选择（CNN其实就是做了特征选择）</li>
</ul>
<p>准确率如何提升</p>
<ul>
<li>错误样本分析<ul>
<li>数据清洗，特征提取，比如“？”</li>
</ul>
</li>
<li>改善模型</li>
<li>改善loss使用am-softmax</li>
<li>引入熵</li>
<li>多标签分类</li>
</ul>
<p>多意图分类+错分类，如何解决？</p>
<ul>
<li>多标签分类</li>
<li>使用sigmoidcrossentropywithlogits</li>
<li>引入熵</li>
<li>引入am-softmax</li>
</ul>
<p>过拟合</p>
<ul>
<li>dropout</li>
<li>early stopping</li>
<li>shuffle</li>
</ul>
<p>上线后的问题？</p>
<ul>
<li>新的术语，</li>
<li>不存在FAQ里的问题，</li>
<li>闲聊，</li>
<li>抱怨，</li>
<li>多轮（信息分几次发送）</li>
<li>未登录词</li>
</ul>
<h3 id="还有哪些可以改进的点"><a href="#还有哪些可以改进的点" class="headerlink" title="还有哪些可以改进的点"></a>还有哪些可以改进的点</h3><ul>
<li>多轮，这是个大问题（在FAQ也做信息提取，保存信息）</li>
<li>更好的做长句的扩充和数据收集，让模型对长句的泛化能力更好，比如对长句的判别（加attention+cnn）</li>
<li>阈值问题，并不一定会游泳，但是会想尝试使用交叉法来看一下结果</li>
</ul>
<h3 id="自己觉得骄傲的点有哪些"><a href="#自己觉得骄傲的点有哪些" class="headerlink" title="自己觉得骄傲的点有哪些"></a>自己觉得骄傲的点有哪些</h3>]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>智能客服项目</title>
    <url>/2019/12/08/interview/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<h2 id="智能客服项目"><a href="#智能客服项目" class="headerlink" title="智能客服项目"></a>智能客服项目</h2><p>智能客服项目主要有三个模块FAQ、任务型对话和闲聊，用户输入一个句子后是进入FAQ、任务型对话还是闲聊，是由一个大的顶层分类器来决定。</p>
<p>FAQ和闲聊都是单轮对话，所以用户后面的输入都会重新进入到顶层分类器再次进行分类。</p>
<p>任务型对话则是多轮的，如果一个句子被顶层分类器划分到任务型对话中，那么在这个任务结束以前，用户后面的输入都会不经过顶层分类器而直接进入到任务型对话中。<br>判断任务型对话是否结束有两种方式：</p>
<ul>
<li>一是这个任务已经完成</li>
<li>二是虽然任务还未完成，但是模型判断需要跳出任务，此时直接结束任务返回通过顶层分类器进行模块判别的进程。</li>
</ul>
<p>在所有的任务中，我们都需要先为这个任务建立一个baseline，通常为相关任务常用的相对简单的模型。</p>
<a id="more"></a>


<hr>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>在NLP中，词嵌入是一个非常重要和基本的点。不同于图片领域，可以直接将图片色彩值、像素值等直接拿来进行学习，在NLP领域中，字、词是需要转换成数值，然后才能用一些算法来进行学习，而这个将词/字转换成数值的方法，就叫做词嵌入 word embedding。</p>
<p>最简单的词嵌入就是ont-hot方法了，但是在都拥有大数据集的情况下，one-hot不仅使得词维度过大，而且并不能够表示文档、字词之间的表意关系。</p>
<p>词嵌入如此重要，我们在词嵌入方向也做了很多研究，并且现在已有很多种效果很好的词嵌入的方法。</p>
<ol>
<li><p>最开始自己在为法国农业银行做智能客服时使用的tf-idf就是一个比较经典的词嵌入方法，但是它仍然有维度过大和不能表示词表意之间的关系</p>
</li>
<li><p>我们这里主要要讲的就是我们经常用到的word2vec词向量方法。word2vec词向量主要有两种计算方法，一种是skip-gram，另一种就是CBOW</p>
<ol>
<li><p>skip-gram</p>
<p>skip-gram是通过在已知当前词的情况下，预测前n个词和后n个词</p>
<p>当前词作为中心词时，因此要预测前n个词和后n个词，所以是从2*n个词的地方来进行参数优化，即进行学习，所以得到的词向量更准确一些，因此对于一些低频词较多的情况下，可以酌情使用这种方法，但是计算量会更大一些</p>
</li>
<li><p>CBOW</p>
<p>CBOW是在已知前面n个词和后面n个词的情况下，对当前词进行预测</p>
<p>通过前n个词和后n个词来预测当前词，学习效率快，但是相对而言，优化的次数要少，因此得到的词向量会相对没有那么精确，但是因为计算量相对较小，而且这种方法得到的词向量已经满足常规使用需求了，所以这种方法的词向量被使用的更频发</p>
</li>
<li><p>两者都是通过对目标函数的优化，来优化隐藏层的参数W，而这个W就是我们的词向量</p>
</li>
<li><p>这两种计算方法其实计算量都特别大，所以有两种优化方法，一种是负采样，一种是softmax方法，我并没有细看</p>
</li>
</ol>
</li>
</ol>
<p>其实并没有规避one-hot维度大、没有词表意之间练习的问题，我们只是把这个问题提前解决了之后，再拿来使用，所以这应该也算是迁移学习吧</p>
<hr>
<h3 id="顶层分类器"><a href="#顶层分类器" class="headerlink" title="顶层分类器"></a>顶层分类器</h3><hr>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><p>FAQ经历的漫长的演变。</p>
<ol>
<li><p>第一版</p>
<ul>
<li>最开始做的FAQ就是tf-idf加上余弦相似度</li>
<li>缺点：<ul>
<li>词包模型，没有句子中词汇出现的顺序信息</li>
<li>有的词汇出现频繁，但并不是很重要的词汇；相反有的词汇出现频率相对较小，但是确实非常重要的判别词汇，所以tf-idf给予字词的权重有待优化</li>
</ul>
</li>
<li>导致：<ul>
<li>有些句子可能在距离上更接近A句子，其实是相似于B句子，比如句式相似，只有一两个重点字不同</li>
</ul>
</li>
<li>为了优化这个方法对词汇权重计算的偏差问题（重要词汇的权重不够大），又找出了所有的重要词汇，并且计算每个句子中重要词汇的杰卡德系数</li>
<li>最后结合余弦相似度和杰卡德系数来判断两个句子的相似性</li>
<li>此时因为数据量小，句子相对简单，top1值并不高60%，但是top3值可以达到95%，剩下的95%是因为数据库中不存在相关语句</li>
</ul>
</li>
<li><p>第二版</p>
<ul>
<li>后来用的是深度学习加上余弦相似</li>
<li>深度学习主要是用word2vec和LSTM得到句子的embedding</li>
<li>有目标句子Anchor，同类（答案相同的）问题中抽取一个正例Positive，非同类问题中抽取一个负例Negative</li>
<li>d(a,p) &lt;= d(a,n)，所以d(a,p)-d(a,n) &lt;= 0，所以d(a,p)-d(a,n)+margin &lt;= 0</li>
<li>对于新输入的句子，训练好的模型唯一的用途就是embedding，用模型embedding出来的数值计算余弦相似度进行排序</li>
<li>缺点：<ul>
<li>因为负采样的原因不稳定，每一对&lt;a,p,n&gt;中，正采样可以用的样本过少，负采样样本较多，且大概率负样本很容易满足d(a,p)&lt;=d(a,n)的限制，因为一般情况下，负样本都比正样本离目标样本远的多</li>
</ul>
</li>
</ul>
</li>
<li><p>第三版</p>
<ul>
<li><p>loss中使用am-softmax。以前的<strong>loss</strong>是简单的取交叉熵，现在是</p>
<ul>
<li>对两个vector都做了归一化，即求内积变为求余弦相似度</li>
<li>增加类间距离，缩小类内距离：每个样本所属类的距离，必须<strong>远</strong>小于它跟其它类的距离。这个<strong>远</strong>可以是<strong>到其它类距离的1/2</strong>，可以是<strong>到其它类的距离减去某个值: dist()-m</strong>，等。这里用的是第二种，即余弦相似度减去某个值m</li>
<li>为了让概率P更均匀的分布在0-1之间，对余弦相似度进行了s倍的缩放</li>
</ul>
</li>
<li><p>用了triplet loss中online采样的变体：</p>
<ul>
<li>假设包含B个图片的banch有P个不同的人组成，每人有K个图片，即B=PK。两种在线采样策略分别是：<ul>
<li>batch all: 取目标样本下所有的正样本和所有的负样本（PK<em>(K-1)</em>(P-1)K个triplet，PK为所有样本数，K-1为正样本树，(P-1)K为负样本树）</li>
<li>batch hard: 取目标样本下距离最小的负样本和距离最大的正样本（PK个triplet）</li>
</ul>
</li>
<li>这两个采样策略，一个太复杂，计算量太大，一个太极端，不稳定。所以我们取了中和的方法，即<strong>取目标样本下每个类别中距离最小的m个负样本和距离最大的m个正样本，然后一一对应来计算（PK * m * (P-1)m个triplet with m &lt;&lt; K）</strong></li>
<li>offline采样：这个方法不够高效，因为最初要把所有的训练数据喂给神经网络，而且每过1个或几个epoch，可能还要重新对negative examples进行分类</li>
</ul>
</li>
<li><p>triplet loss中的分类是以问答相似/相同为标准的小类大约3500个，还有一个大类，即我们把FAQ问题整体分了个类，约840个类，FAQ整体数据大概10W。我们在得到一个新的输入句子时，会从840个类中分别抽取3个query，来计算相似度，我们取平均值最高的5个类，然后在只取这5个类中所有的问题来和新输入的句子进行相似度匹配，来得到需要输出的答案。</p>
<ul>
<li>第一次类的寻找，是为了节约时间，不让输入的query和10W个句子暴力匹配</li>
<li>第二次在5个类中找到需要的结果，是对具体回答的答案的寻找</li>
<li>是否直接输出找到的最匹配的答案，还有一个阈值来决定</li>
<li>如果我们对第一选项不确定，即没有超过相关阈值，那么我们可以推荐前三个相似度最高的问题，让用户选择</li>
<li>用户的选择，能够为我们带来新的数据，帮助优化模型</li>
<li>如果所有问题都没有超过推荐阈值，那么需要用户重新输入</li>
</ul>
</li>
<li><p>第三版使用了triplet loss改进后，top1值提高到了87%，top3值提高到了94%</p>
</li>
</ul>
</li>
</ol>
<h4 id="相似度计算方法"><a href="#相似度计算方法" class="headerlink" title="相似度计算方法"></a>相似度计算方法</h4><p>最长公共子序列、最长公共子串和编辑距离，都用的是动态规划，代码中需要注意：</p>
<ol>
<li>矩阵的建立</li>
<li>循环时，A，B指针和矩阵下标的对应</li>
</ol>
<h5 id="levenshtein"><a href="#levenshtein" class="headerlink" title="levenshtein"></a>levenshtein</h5><p>当i=0 or j=0,res[i][j]=0<br>res[i][j]=min{res[i-1][j]+1,res[i][j-1]+1,res[i-1][j-1]+flag}, flag=0 if A[i]=B[j], flag=1 if A[i]!=B[j]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">levenshtein</span><span class="params">(seq1, seq2)</span>:</span></span><br><span class="line">    size_x = len(seq1) + <span class="number">1</span></span><br><span class="line">    size_y = len(seq2) + <span class="number">1</span></span><br><span class="line">    matrix = np.zeros ((size_x, size_y))</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(size_x):</span><br><span class="line">        matrix [x, <span class="number">0</span>] = x</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> xrange(size_y):</span><br><span class="line">        matrix [<span class="number">0</span>, y] = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">1</span>, size_x):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> xrange(<span class="number">1</span>, size_y):</span><br><span class="line">            <span class="keyword">if</span> seq1[x<span class="number">-1</span>] == seq2[y<span class="number">-1</span>]:</span><br><span class="line">                matrix [x,y] = min(</span><br><span class="line">                    matrix[x<span class="number">-1</span>, y] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x<span class="number">-1</span>, y<span class="number">-1</span>],</span><br><span class="line">                    matrix[x, y<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                matrix [x,y] = min(</span><br><span class="line">                    matrix[x<span class="number">-1</span>,y] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x<span class="number">-1</span>,y<span class="number">-1</span>] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x,y<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">    <span class="keyword">print</span> (matrix)</span><br><span class="line">    <span class="keyword">return</span> (matrix[size_x - <span class="number">1</span>, size_y - <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h5 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h5><p>当i=0 or j=0,res[i][j]=0<br>当A[i]=B[j],res[i][j]=res[i-1][j-1]+1<br>当A[i]!=B[j],res[i][j]=0</p>
<h5 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h5><p>当i=0 or j=0,res[i][j]=0<br>当A[i]=B[j],res[i][j]=res[i-1][j-1]+1<br>当A[i]!=B[j],res[i][j]=max{res[i-1][j],res[i][j-1]}</p>
<hr>
<h3 id="任务型对话"><a href="#任务型对话" class="headerlink" title="任务型对话"></a>任务型对话</h3><p>任务型对话主要有时效运费、查单、转寄退回等几个场景，这里主要用时效运费的场景来说明项目过程。</p>
<p>在时效运费场景中，NLU主要有两个模块，意图识别和NER。</p>
<h4 id="意图识别"><a href="#意图识别" class="headerlink" title="意图识别"></a>意图识别</h4><p>意图识别总共有8类，总数据量是1万5，留了1500个数据做测试集，3000个数据做验证集。</p>
<p>最开始使用的是stacking，后来随着数据量的增加，改用深度学习。<br>深度学习测试了cnn+rnn和cnn，发现cnn的效果比cnn+rnn好，应该是：</p>
<ul>
<li><p>数据本身都是比较短的句子，且序列性不强，所以rnn的优势没有体现出来</p>
</li>
<li><p>只有cnn更好的保存了提取的特征，优化了分类效果</p>
</li>
</ul>
<h5 id="如何优化分类问题"><a href="#如何优化分类问题" class="headerlink" title="如何优化分类问题"></a>如何优化分类问题</h5><h6 id="类别F1值分析"><a href="#类别F1值分析" class="headerlink" title="类别F1值分析"></a>类别F1值分析</h6><p>分析整体F1值，看主要影响效果的是哪一个/哪几个类，然后针对性分析，提高优化效果。<br>针对F1值过低的类别，仔细分析错误分类的数据，找出原因</p>
<h6 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h6><p>根据学习曲线，可以找出当前模型训练出最佳结果所需要的最少的数据数，可以避免资源的浪费，也可以排除结果不佳可能是数据集不够的困惑。</p>
<h5 id="说一下stacking以及它在这个项目中的应用"><a href="#说一下stacking以及它在这个项目中的应用" class="headerlink" title="说一下stacking以及它在这个项目中的应用"></a>说一下stacking以及它在这个项目中的应用</h5><p>stacking中我们用了两层分类器，第一层有三个基分类器RF，NB，LR，第二层分类器是线性核函数的SVM，folder=3，</p>
<ol>
<li><p>先用tf-idf提取特征</p>
</li>
<li><p>第二层的输入为第一层的输出，不包括原始数据</p>
</li>
<li><p>第二层分类器如果是更简单的分类器，效果更好，比如我们用的是线性核函数SVM。</p>
<ul>
<li><p>可能因为上一层的几个基分类器多维度提取特征已经比较复杂，所以第二层的分类器过于复杂会造成过拟合。</p>
</li>
<li><p>输出是概率，更符合分类的要求</p>
</li>
</ul>
</li>
<li><p>较小数据集上stacking的表现并不如意，有时甚至比单独的分类器的效果要差</p>
</li>
<li><p>基分类器中如果有效果特别差的，可以将其移除，可能优化最后的结果</p>
</li>
<li><p>stacking的预测需要时间相对较长</p>
</li>
</ol>
<h6 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h6><p>逻辑回归是一个判别模型<br>通过对激活函数sigmoid的引入，使得线性回归中的$W^T · x$映射到(0,1)之间，使逻辑回归成为一个概率预测问题，参数就是$W^T$<br>为了使逻辑回归公式$P(y|x) = p_1^y·p_0^{1-y}$最大，我们引入最大似然估计</p>
<p>逻辑回归假设数据符合伯努利分布，所以是一个参数模型。<br>通过引入sigmoid函数，将输出映射到[0,1]之间，</p>
<p>逻辑回归本来是二元分类，但是也可以用作多元分类。<br>如果逻辑回归中引入正则化，我们需要对特征进行标准化，在这标准化也可以加快训练。</p>
<p>最大似然(max最大化问题) 可以导出 loss funcition(min Cross Entropy最小化问题)</p>
<p>方法：</p>
<ul>
<li><p>one VS all，选择计算结果最高的那个类。</p>
</li>
<li><p>引入softmax</p>
</li>
</ul>
<p>缺点：样本不均衡，因为1 VS all</p>
<ol>
<li>损失函数</li>
</ol>
<p>欠拟合：增加特性，增加数据<br>过拟合：正则化，dropout，提前停止训练，减少模型复杂度</p>
<p><strong>在统计学中对变量进行线行回归分析，采用最小二乘法进行参数估计时，R平方为回归平方和与总离差平方和的比值，表示总离差平方和中可以由回归平方和解释的比例，这一比例越大知越好，模型越精确，回归效果越显著。R平方介于0~1之间，越接近1，回归拟合效果越好，一道般认为超过0.8的模型拟合优度比较高。</strong><br><img src="/public/images/image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-R2.png" alt="逻辑回归-R2"></p>
<h6 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h6><p>今天终于感觉自己终于懂了SVM的一点皮毛。</p>
<ol>
<li><p><strong>SVM：Support Vector Machine支持向量机</strong></p>
<p> SVM一般用于二元分类，但是因为one-vs-all的存在，我们也可以用它来进行多元分类。</p>
<p> SVM的效果一般特别好，特点是计算量比较大，但是他对数据的分布没有要求，它通过最大化到超平面最近点的距离，来进行分类。</p>
</li>
<li><p><strong>Python &gt;&gt; sklearn中的SVM：</strong></p>
<p> SVC: Support Vector Classification 支持向量用于分类<br> SVR: Support Vector Regression 支持向量用于回归<br> LinearSVC: Linear SVC 核函数为<strong>线性核函数</strong>的支持向量</p>
</li>
<li><p><strong>SVM几种常用核函数：</strong></p>
<p><img src="../../public/images/image/SVM%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0.svg" alt="核函数"></p>
</li>
<li><p><strong>核函数的选择</strong>（吴恩达教授说）：</p>
<ul>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel (“rbf”)</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ul>
</li>
<li><p>参考视频：<br><a href="https://www.bilibili.com/video/av75892058/" target="_blank" rel="noopener">SVM核函数–表示这个视频真的很好</a></p>
</li>
<li><p>自己对核函数的总结：</p>
<ul>
<li>核函数最重要的作用就是映射，不同的核函数提供不同的映射方法</li>
<li>核函数和normalization有点相似，都是对点的映射，只是目标和方式不同，normalization是为了让数据分布更均匀，而SVM中的核函数是为了让数据分布更开散、更易分割</li>
<li>核函数的加入，是的SVM的计算量增大，尤其是使用高斯核函数和多项核函数的时候</li>
<li>在文本分类项目中的stacking方法中，我们使用的是LinearSVC</li>
<li>一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果</li>
<li>自己只是了解皮毛，对于核函数公式的推导，如何知道一个核函数是否是有效核函数等，都还不是太清楚</li>
</ul>
</li>
</ol>
<h5 id="介绍一下CNN以及在这个项目中的应用"><a href="#介绍一下CNN以及在这个项目中的应用" class="headerlink" title="介绍一下CNN以及在这个项目中的应用"></a>介绍一下CNN以及在这个项目中的应用</h5><p>在CNN中，<strong>卷积</strong>的作用是<strong>特征提取</strong>（比如图像中的边缘检测），<strong>池化</strong>的作用是压缩特征数，对<strong>特征进行降维</strong>。</p>
<p>在CNN分类模型中，我们简单的使用了三个卷积核(大小为filter_size*embedding_size)，加入了L2正则化。</p>
<p><strong>如何让CNN也能解决序列问题</strong></p>
<ul>
<li>CNN在卷基层后，还是保留了句子的顺序关系的，但是在经过max pooling对特征降维后，就打乱了这种顺序，所以我们其实可以去掉max pooling，只让CNN做卷积</li>
<li>或者也可以在CNN中加入位置信息</li>
</ul>
<h5 id="碰到的问题"><a href="#碰到的问题" class="headerlink" title="碰到的问题"></a>碰到的问题</h5><ol>
<li><p>样本不均衡</p>
<ul>
<li><p>过采样：不易太过，否则容易过拟合，但是预测效果很差</p>
</li>
<li><p>欠采样：当数据样本足够多时才能使用</p>
<p>除了样本不均衡本身带来的训练问题，在测试集中，因为样本严重不均衡，且需要尽量保证测试集和训练集的分布一样，所以可能存在测试集中某些类只有几个数据，导致结果随机性较大不可靠，这时应该尽量把真实数据留在测试集中</p>
</li>
</ul>
</li>
<li><p>阈值难以确定<br>每一次的预测，有所有类别的概率[0.1, 0.2, 0.3, 0.4, 0.5]，然后取概率最大的那个类别当作预测结果。<br>但是因为这个结果在每个类别中的边界线并不稳定，所以我们尝试对预测概率[0.1, 0.2, 0.3, 0.4, 0.5]的熵的分析来进一步判别。</p>
</li>
<li><p>任务结束前如何判断用户是否更改意图，跳出当前场景</p>
<p> 我们增加了一个类others，包含所有肯定不属于当前场景的对话数据，比如：“我要投诉”，“今天天气适合登山”等等。</p>
<p> 对于需要跳出当前场景的对话，如果我们没有跳出，比不需要跳出场景对话但是却跳出了的结果要更严重，所以我们主要要考虑others类的准确率，其次是F1值和召回率。</p>
<p> 这里为了使得others相对于召回率有更大的准确率，我们增加了这个类的权重（求完各个类的交叉熵后，乘以权重系数，即增加这个类对loss的敏感度）。如果训练时间足够长，是否增加权重最后的结果都是相同的，但是增加权重后，模型会优先保证others这一类的准确率，所以我们可以让训练停在others有较大准确率、且总体结果比较符合我们要求的时候。即牺牲别的类的准确率来优先保证others的准确率。</p>
</li>
<li><p>过拟合</p>
<p> 表现为训练集的F1值不断上升但是验证集的变化不大。原因是数据量很多，但是类别很少，且句子都很简短。</p>
<p> 解决方法：</p>
<ul>
<li>加入L2正则化：加入L2正则化初期效果很好，后面慢慢还是会过拟合</li>
<li>停止训练：在发现训练集不断上升但是验证集结果变化不大时，及时停止训练。</li>
</ul>
</li>
<li><p>在计算损失函数时，不要允许概率直接等于0，而是加上一个极小的正数，这样会减少信息的丢失，整体的概率和与1有一点偏差并不影响</p>
</li>
</ol>
<h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><p>一共有15000个数据，12个类别（连上不属于这12个类别的数据，如“的”、“你们”等，共13个类别），如”出发地”，”出发时间”，”类型”，”物品”，”体积”,”重量”等。</p>
<p>训练集：14000，验证集：，测试集：1000。</p>
<p>baseline用的是LSTM+CRF，后来改为BiLSTM+CRF后，效果有一定提升。optimizer选用的是Adam。</p>
<p>准确率达97.4%，但F1值只有87.4%<br>主要是对时间的获取准确率太低，DUR和TIM分别只有43%和50%</p>
<h4 id="BiLSTM-CRF"><a href="#BiLSTM-CRF" class="headerlink" title="BiLSTM+CRF"></a>BiLSTM+CRF</h4><h5 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h5><ol>
<li>条件：判别式模型；随机场：无向图模型</li>
<li>判别模型P(Y|X)</li>
<li>打破了观测独立假设（朴素贝叶斯假设就是观测独立假设）</li>
<li>全局归一化（MEMM最大熵马尔可夫模型是局部归一化）</li>
<li>一般的深度学习模型只是学习了文本/特征上下文的关系(BiLSTM)，但是CRF的加入可以让它还学到label上下之间的关系，加入了一个相邻序列间转换的概率</li>
<li><img src="/public/images/image/BiLSTM+CRF&#32;loss.png" alt="BiLSTM+CRF的loss求解"></li>
</ol>
<h6 id="CRF和HMM、MEMM相比的优势"><a href="#CRF和HMM、MEMM相比的优势" class="headerlink" title="CRF和HMM、MEMM相比的优势"></a>CRF和HMM、MEMM相比的优势</h6><ul>
<li>CRF没有HMM严格的独立假设条件，所以可以容纳更多的上下文信息（HMM独立假设：输出仅如当前状态相关）</li>
<li>CRF相比MEMM，统计了全局的概率，考虑的数据在全局的分布，并归一化，克服了MEMM模型标记偏置（局部归一化的原因）的缺点</li>
<li>CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，计算下一个状态的概率分布</li>
</ul>
<p><strong>RF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。</strong></p>
<p><strong>HMM和CRF对比： 其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。</strong></p>
<h6 id="自编码-VS-自回归"><a href="#自编码-VS-自回归" class="headerlink" title="自编码 VS 自回归"></a>自编码 VS 自回归</h6><p>自回归模型：ELMo，GPT，GPT2，XLNet<br>自编码模型：Bert</p>
<h6 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h6><p>双向自回归语言模型，使用的是从左到右、从右到左的两个LSTM的拼接，特征提取能力不如Transformer。只提供拼接后的词向量</p>
<h6 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h6><p>单向自回归语言模型，使用的是transformer，但是是left-to-right的单向transformer：transformer decoder，用到的self-attention是masked-multi-self-attention。可以fine-turning</p>
<h6 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h6><p>双向自编码语言模型，其中的mask有加噪音的效果，且有增强上下文信息获取的效果。可以fine-turning<br>优点：</p>
<ul>
<li>双向transformer，所以可以获取上下文信息</li>
<li>用到transformer，可以并行运算</li>
</ul>
<p>缺点：</p>
<ul>
<li>mask在预测中没有，影响模型的泛化能力</li>
<li>缺乏生成能力</li>
<li>没有考虑预测的mask间的相关性</li>
</ul>
<h6 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h6><p>优点：</p>
<ul>
<li>用随机采样句子分解的顺序，来解决自回归模型不能联系上下文的问题</li>
<li>解决了预测词间没有依赖的问题</li>
<li>attention mask是的模型可以看到自己那个词attent到它前面分解的词，但是不attent到它看不到的后面分解的词</li>
<li>原自回归模型中永远都是预测下一个词，但是这里因为打乱了句子的分解顺序，所以不知道下一个预测的词是哪个位置的词，或者说预测哪里都一样，所以就变成了bow词包性质的预测，那么建模就没有那么强的序列性。所以我们决定，不用知道下一个词在哪里，我们就预测当前位置的词，那么我们会天然把当前输入encoder到信息中。但是这回带来另一个问题，就是可能要预测的当前词就是输入，那就没有任何意义。但是当我们到另外一个位置时，当前位置的信息需要encoder到里面，所以发生了一个矛盾点，即当前位置信息，某些情况需要encoder到信息中，某些情况又不能encoder到信息中。所以我们就提出了two stream attention，把feature拆成两组，一组encoder当前输入，一组不用，context stream encoder当前输入，用作fine-turning，query stream 不 encoder当前输入，用作预测，并且两个stream参数共享</li>
</ul>
<h6 id="ALBert"><a href="#ALBert" class="headerlink" title="ALBert"></a>ALBert</h6><p>改进：</p>
<ul>
<li>词向量降维</li>
<li>权重共享</li>
<li>更轻，效果更好</li>
<li>NSP 改为了 SOP，即负样本为反转句子</li>
<li>增加了数据量和训练时间</li>
</ul>
<p>缺点：</p>
<ul>
<li>减少了内存占用，但是并没有减少计算量</li>
</ul>
<h6 id="RoBerta"><a href="#RoBerta" class="headerlink" title="RoBerta"></a>RoBerta</h6><p>改进：</p>
<ul>
<li>静态mask变为动态mask:而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式</li>
<li>with NSP vs without NSP:roberta用的是full-sentence</li>
<li>更大的mini-batch：256 -&gt; 2K</li>
<li>更多的数据和更长的训练时间</li>
</ul>
<h6 id="为什么用Transformer？Transformer相比于RNN和CNN有什么异同？"><a href="#为什么用Transformer？Transformer相比于RNN和CNN有什么异同？" class="headerlink" title="为什么用Transformer？Transformer相比于RNN和CNN有什么异同？"></a>为什么用Transformer？Transformer相比于RNN和CNN有什么异同？</h6><ul>
<li>Transformer和CNN都不存在序列依赖问题，可以并行计算（计算能力不是问题？？），但是Transformer相对于CNN保留了序列关系，他有RNN的优点，却没有RNN计算能力相对差的缺点</li>
<li>CNN中卷基层无法捕捉远距离特征（依赖于卷积层的设定），但是Transformer中，self-attention可以获取前后文所有词与当前词的相关性</li>
<li>RNN和Transformer都可以解决句子中的依赖问题（有点同上面提到的序列问题相似），而是Transformer做的更好，他可以得到句子中每个单词间的相关性，并且一步到位</li>
<li>Transformer的缺点是，当输入文本特别长的时候，他的计算量会飞速增长。</li>
<li>Transformer中的self-attention不存在梯度消失的问题，所以可以更好的看到更远距离的信息</li>
</ul>
<h6 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h6><h5 id="Lattice-LSTM"><a href="#Lattice-LSTM" class="headerlink" title="Lattice+LSTM"></a>Lattice+LSTM</h5><p>NER中一般采用BIEO标注的方法，这种方法一般需要把字一个个的分开标注，所以用的是字向量。<br>字向量：</p>
<ul>
<li>优点<ul>
<li>嵌入字向量，不会有OOV</li>
<li>语料多少，不会影响向量整体大小所占内存</li>
</ul>
</li>
<li>缺点<ul>
<li>缺少字词之间的语义关系</li>
</ul>
</li>
</ul>
<p>词向量</p>
<ul>
<li>优点<ul>
<li>完整的嵌入词语信息</li>
</ul>
</li>
<li>缺点<ul>
<li>性能依赖分词精度</li>
<li>开放领域，跨领域分词是一个难题</li>
<li>会有OOV影响模型性能</li>
<li>语料大时，词典容量大，占用内存资源大</li>
</ul>
</li>
</ul>
<p>可见NER中用字向量有很大优势，可以在用词向量的基础上，也加入词向量来嵌入词语的信息吗？</p>
<p>如果使用分词，对词向量进行标注<br>Lattice是除了使用词向量外，还把字向量作为特征也加入进去了，所以整个计算复杂度升高了很多。</p>
<h5 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h5><p>在测试完Lattice LSTM后，发现整体F1值虽然增加了2%，但是速度却下降了特别多（将近5倍），所以最后还是选用的BiLSTM+CRF</p>
<hr>
<h3 id="Bert-1"><a href="#Bert-1" class="headerlink" title="Bert"></a>Bert</h3><h4 id="Bert的两大亮点"><a href="#Bert的两大亮点" class="headerlink" title="Bert的两大亮点"></a>Bert的两大亮点</h4><ul>
<li>MASK机制：随机mask一些token来预测</li>
<li>next sentence predict：判断句子B是否是句子A的下一句话</li>
</ul>
<p>有三个输入：字向量，位置向量，句子向量<br>15%的词mask，这15%中有80%是真的用MASK来代替，10%是原有单词，10%是随机选择的单词<br>优点：</p>
<ul>
<li>只有80%的字为MASK是因为微调时，输入数据中不会有MASK</li>
<li>10%错误是因为这样模型不能100%确定当前字一定是正确的，所以迫使模型更多的依赖上下文</li>
</ul>
<h4 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h4><p><a href="https://www.bilibili.com/video/av56239558/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">超厉害的transformer讲解视频，看完就明朗！</a></p>
<ol>
<li><p>transformer最重要的组成部分就是self-attention</p>
</li>
<li><p>attention的计算中，$alpha_{1,i} = q^1 · k^i / \sqrt{d}$，这里除以$\sqrt{d}$是因为作者认为在前面求内积时，如果$q^1$和$k^i$的dimension越大，自然$q^1 · k^i$内积也会<strong>相对</strong>越大，所以要除以$q^1$和$k^i$的dimension $d$来平衡。</p>
</li>
<li><p>self-attention输出的seq中，输出$y_i$是同CNN中一样，可以并行计算的</p>
</li>
<li><p><img src="/public/images/image/self-attention.png" alt="self-attention"></p>
</li>
<li><p>表示位置position的向量$e^i$不是学出来的，而是开始的时候就设置了的，通过$e^i$的数值，可以直接判断出当前输入在句子中是在什么位置</p>
</li>
<li><p>为什么直接把位置信息$e^i$加到$a^i$上，而不是concat呢？这样会不会造成信息混乱呢？</p>
<p>那么我们试着不把$e^i$直接加入到$a^i$，而是在输入处，让位置信息$e$和输入信息$x$直接concat，得到比如$X$，他们乘以把$W_e$和$W_x$ concat在一起的矩阵$W$，我们会发现$W·X = W_e · e + W_x · x = W_e · e + a$，所以最后得到的结果还是相加的，具体情况如下图<br><img src="/public/images/image/self-attention-2.png" alt="positioninfo"></p>
</li>
<li><p>layer norm VS batch norm</p>
</li>
<li><p>为什么layer norm一般会搭配RNN使用？</p>
</li>
<li><p>在transformer的decoder中，中间的multi-head attention的前两个输入来自于encoder的输出，最后一个的输入来自于decoder，前两个是$Q$和$K$，他们是用来计算attention的，后面一个是$V$，计算出来的attention矩阵和$V$进行运算来看attention和$V$的关联度。</p>
</li>
<li><p>multi-head attention里之所以用几个attention是因为不同的attention专注点不同，如果句子里有很多需要关联的关系来学习的化，multi-head attention就会大大提升效果</p>
</li>
</ol>
<h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><p>常见的两种attentio机制</p>
<h4 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h4><ul>
<li>数据增广中，我们不需要用到segment embedding，因为我们都是同一个句子，但是意图分类中我们可以加入intent embedding</li>
<li>随机mask 15%的token，其中10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]</li>
<li>当作一个seq2seq模型来使用，让模型直接输出句子，即mask会被替换成别的单词/词语</li>
<li><ol>
<li>用词向量比字向量好，用字向量会出现错误的词组</li>
<li>词向量需要自己重新训练，中文的Bert是字向量，所以失去了迁移模型的优势</li>
</ol>
</li>
</ul>
<hr>
<h3 id="纠错"><a href="#纠错" class="headerlink" title="纠错"></a>纠错</h3><h4 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h4><h4 id="levenshtein-1"><a href="#levenshtein-1" class="headerlink" title="levenshtein"></a>levenshtein</h4><h4 id="最长公共子串-1"><a href="#最长公共子串-1" class="headerlink" title="最长公共子串"></a>最长公共子串</h4><hr>
<h3 id="概述历程"><a href="#概述历程" class="headerlink" title="概述历程"></a>概述历程</h3><h4 id="法国"><a href="#法国" class="headerlink" title="法国"></a>法国</h4><h4 id="顺丰"><a href="#顺丰" class="headerlink" title="顺丰"></a>顺丰</h4><h3 id="几大模块"><a href="#几大模块" class="headerlink" title="几大模块"></a>几大模块</h3><h2 id="待写文章"><a href="#待写文章" class="headerlink" title="待写文章"></a>待写文章</h2><ol>
<li><p>过拟合欠拟合</p>
</li>
<li><p>样本不均衡</p>
</li>
<li><p>batch norm详述及优缺点</p>
</li>
<li><p>batch norm和layer norm的不同</p>
</li>
</ol>
<h3 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h3><p>自己的优势是对整个产品流程的理解，和对产品如何更好面向客户的技术上的着重点的理解</p>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>面试</title>
    <url>/2019/12/08/interview/%E9%9D%A2%E8%AF%95/</url>
    <content><![CDATA[<p>n个无序数组，求top K</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">TopK</span><span class="params">(listArray, K)</span>:</span></span><br><span class="line">    k = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(listArray)):</span><br><span class="line">        <span class="keyword">if</span> k + len(listArray[i]) &gt; K:</span><br><span class="line">            listK += listArray[i][:K-k]</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>面试感触</title>
    <url>/2019/12/08/interview/%E9%9D%A2%E8%AF%95%E6%84%9F%E8%A7%A6/</url>
    <content><![CDATA[<h2 id="恒昌第三轮面试"><a href="#恒昌第三轮面试" class="headerlink" title="恒昌第三轮面试"></a>恒昌第三轮面试</h2><p>今年下午两点和恒昌第三轮面试完。</p>
<p>面试官整体给我的感觉是一种艺术家气息，举例用的是文艺复兴时期绘画的例子，他说那时候有钱人才能用蓝色，特殊人，比如天使才可以用蓝色还说那时候画家并不是创作画家，而是甲方要什么，画家就要一点不差的按照他们的要求来画，和我们现在的算法工程师一样，都是工人，并不是工程师。工程师是可以从现实中抽象建模的人。但这并不是一个人题，因为凡事都是需要经历这样一个过程的，从一点一点的模仿，到熟练，再到创新。我们工程目前还处于一个比较笨的状态，还是工人角色，远没到创新的时候，而且现在人工智能的浪潮也再一次落下，我们已经在尾声，如果以后我出去说自己是做深度学习的，可能就找不到工作了。</p>
<p>听了它说的，我越来越觉得，真的不同的人有不同的看法，他会问我是否学过scala，但是爱奇艺的面试官会说他不会因为我会Hadoop或者不会Hadoop而决定是否录用我。</p>
<p>虽然两者并不矛盾，但是凸显了他们对这个应聘者、甚至这个领域工作者的一个不同的要求和需求。</p>
]]></content>
      <categories>
        <category>La Vie</category>
      </categories>
  </entry>
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <url>/2019/12/07/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
    <content><![CDATA[<h2 id="什么是梯度消失-梯度爆炸"><a href="#什么是梯度消失-梯度爆炸" class="headerlink" title="什么是梯度消失/梯度爆炸"></a>什么是梯度消失/梯度爆炸</h2><p>梯度消失或梯度爆炸是指在神经网络权重更新的反向传播中，$\frac{\partial Loss}{\partial W}$过小或过大的现象</p>
<h2 id="梯度消失-梯度爆炸会产生什么问题"><a href="#梯度消失-梯度爆炸会产生什么问题" class="headerlink" title="梯度消失/梯度爆炸会产生什么问题"></a>梯度消失/梯度爆炸会产生什么问题</h2><p>梯度爆炸会导致权重的更新过大（表现为LOSS出现Nan）<br>梯度消失会导致权重的更新过慢或者几乎不更新</p>
<a id="more"></a>

<h2 id="为什么会有梯度消失和梯度爆炸"><a href="#为什么会有梯度消失和梯度爆炸" class="headerlink" title="为什么会有梯度消失和梯度爆炸"></a>为什么会有梯度消失和梯度爆炸</h2><p>一种是任务梯度消失/梯度爆炸的原因是反向传播过程中的叠成导致的<br>另一种认为反向传播不该背这个锅，反向传播求导只是一种求最优解的方式，最根本的原因还是网络结构的问题（线性和非线性公式连成组合）</p>
<blockquote>
<p>看到<a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">有个文章</a>说我们不应该纠结于梯度消失和梯度爆炸，应该从不同的角度看LSTM，比如选择性、信息不变性等等，觉得这是个很好的想法。当然，他对于RNN和DNN中梯度消失和梯度爆炸的看法也相对详细正确，下面会细说。</p>
</blockquote>
<p>无论是因为网络结构还是反向传播，不可否认的是梯度消失和梯度传播确实来源于对权重求导后很多小于1的数值的连乘，下面我门主要看一下为什么会产生这些小于1的数值。</p>
<h3 id="几个激活函数"><a href="#几个激活函数" class="headerlink" title="几个激活函数"></a>几个激活函数</h3><p>在进入反向传播求导前，先认识一下神经网络中我们经常用到的激活函数</p>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>sigmoid 常见于CNN中，在RNN和LSTM中一般不会用到sigmoid<br>sigmoid的公式为$S(x)=\frac{1}{1+e^{-x}}$<br>其函数和导数图如下</p>
<p><img src="./images/image/sigmoid.png" alt="sigmoid"></p>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>LSTM和RNN中会用到tanh，详情可以看上一篇文章<br>tanh的公式为$tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{x}}$<br>其函数和导数图如下</p>
<p><img src="/images/image/tanh.png" alt="tanh"></p>
<h4 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h4><p>Relu主要是为了解决梯度消失和梯度爆炸的问题<br>其公式为：<br>$<br>relu(x)=\begin{cases}<br>0, x&lt;0 \\<br>x, x&gt;0<br>\end{cases}<br>$</p>
<p>其函数和导数图如下</p>
<p><img src="/images/image/relu.jpg" alt="relu"></p>
<p>relu的优点：</p>
<ul>
<li>解决了梯度消失/梯度爆炸的问题</li>
<li>计算速度快</li>
<li>加速了网络训练（这点我有点怀疑，是指计算速度快呢还是指效果更好，更容易收敛找到最优解呢？如果是第二点，如何证明的？）<br>relu缺点：</li>
<li>负数部分恒为0，导致一些神经元无法激活（但是有很多relu的改进版已经解决了这个问题，比如leakrelu）</li>
<li>输出不是以0为中心</li>
</ul>
<blockquote>
<p><strong>问题：如果大于0的函数都返回其本身，不是失去了使用非线性函数的意义吗？</strong></p>
</blockquote>
<h3 id="为什么会产生梯度消失和梯度爆炸"><a href="#为什么会产生梯度消失和梯度爆炸" class="headerlink" title="为什么会产生梯度消失和梯度爆炸"></a>为什么会产生梯度消失和梯度爆炸</h3><p>DNN和RNN中的梯度消失和梯度爆炸是不同的</p>
<h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><p>写比较麻烦，下次有时间再看看专门推一遍公式写一遍吧，这里直接贴吧：</p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播w导数解释"></p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADb%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播b导数解释"></p>
<p>没有手动推导，所以曲折的理解过程如下：</p>
<ol>
<li><p>我当时看到两个博客里的公式不一样很困惑，后来发现是自己粗心，因为第一个是对W求导，第二个是对b求导。但是看到很多博客里其实都写的是第二种情况。<br>其实更应该考虑的是第一种情况，因为在神经网络中，bias是为了防止wx等于零起的辅助作用，真正有意义的是讨论W。</p>
</li>
<li><p>虽然真正有意义的是讨论W，但是图二中对b求导的公式其实和真正对w求导的公式差不多，因为他们前面的所有结果都是相同的，唯一不同的是最后一步对b求导结果是1，而对w求导结果是$\delta’(z)$，也就是图一中的$f_1$。所以图二中的公式可以看作是图一公式的详细版。所以在反向传播中，决定导数数值的不仅是激活函数的函数，也有W本身。</p>
</li>
</ol>
<p>如上所示，我们可以看到$loss$对$W_1$求导，其实是$\delta’(z) \cdot w_i$的连乘，在上面我们已经了解到，sigmoid和tanh的导数，都小于零，w一般都会初始化为均值为0，方差为1的数值，我们可以有$\vert \delta’(z) \cdot w_i \vert&lt;1$($\vert \delta’(z) \cdot w_i \vert&lt;\frac{1}{4}$当激活函数为sigmoid时)。因此会有梯度消失。</p>
<p>当然，当w特别大是，我们会有$\vert \delta’(z) \cdot w_i \vert&gt;1$，就会导致梯度爆炸。</p>
<p>从上我们可以看到DNN中的梯度爆炸或者梯度消失，本质是大于1或者小于1的数的<strong>连乘</strong>（注意这里不是幂次方，RNN中才是幂次方，下面会细说）导致的。</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>RNN中很重要的一点是权重共享。一样贴图</p>
<p><img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="rnn反向传播w导数解释"></p>
<p>这里有几个点需要解释：</p>
<ul>
<li><p>为什么DNN中就连乘，但是RNN的导数中有加法呢？<br>DNN中写出的公式是<strong>每一层</strong>$w_i$前的求导公式<br>RNN这里写出的是<strong>所有层数</strong>的$w_x$前的求导公式的和，因为RNN是权重共享，所以对$W_x$的更新就是每一层需要对$W_x$更新的和。<br>因此第三层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%89%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第三层导数">)，第二层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第二层导数">)，第一层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%80%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第一层导数"></p>
</li>
<li><p>公式中$\frac{\partial S_i}{\partial S_i-1}$和DNN中的$\frac{\partial f_i}{\partial f_i-1}$是一样的性质，唯一不同的是这里$\frac{\partial S_i}{\partial S_i-1}=\frac{\partial S_i-1}{\partial S_i-2}=tanh’\cdot W_s$，所以这里其实是一个小于1的数的<strong>幂次方</strong></p>
</li>
</ul>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p>关于DNN和RNN的梯度消失和梯度爆炸，<a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">这里</a>解释的非常详细。</p>
<p>我也自己来总结以下加深印象</p>
<ul>
<li><p>DNN中每层都有不同的参数，所以每一层都各是各的梯度。RNN中因为权重共享，所以梯度为所有层数梯度的总和（上面提到过）</p>
</li>
<li><p><strong>每一层中</strong>：DNN是小于1的梯度的<strong>连乘</strong>，而RNN中因为$\frac{\partial S_i}{\partial S_i-1}=\frac{\partial S_i-1}{\partial S_i-2}$的关系，所以是<strong>幂次方</strong></p>
</li>
<li><p>RNN中的梯度不会真正的消失，它是每一层梯度的和，RNN中的梯度消失主要是指：离的越远，梯度的传递越弱，所以梯度被邻近的梯度主导，导致模型难以学到远距离信息的长期依赖性问题</p>
</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>附加一点LSTM的</p>
<ul>
<li><p>在LSTM中，有很多条路径，cell state这条路径扮演的角色有点像ResNet残差连接，使前面的数据可以不受中间数据的影响传输到后面。</p>
</li>
<li><p>如果去除cell state这条路径，LSTM中该消失还是消失，该爆炸也爆炸，但是有了cell state，<em>梯度=消失梯度+cell state的梯度</em>，我们看到如果cell state的梯度不消失，就可以一定程度上缓解梯度消失，在第一版没有遗忘门的LSTM中，这里的cell state的梯度=1，后来加入了一个遗忘门，但是因为tanh的取值在-1和1之间，并且多数情况趋于1或者-1，所以加入遗忘门后，LSTM也可以一定程度上缓解梯度消失问题。</p>
</li>
</ul>
<h5 id="为什么说是缓解呢"><a href="#为什么说是缓解呢" class="headerlink" title="为什么说是缓解呢"></a>为什么说是缓解呢</h5><ul>
<li><p>我们不能保证每一层都有前面传过来的残差连接，即我们不能保证哪几层cell state的门就一定是0（怎么解释有待再详细思考）</p>
</li>
<li><p>cell state趋于1或者-1，且有不趋于1和-1的时候，这个时候梯度也是小于1的数</p>
</li>
<li><p><strong>那如果每一层的门的数值不一样，是不是也会导致$\frac{\partial S_i}{\partial S_i-1} \neq \frac{\partial S_i-1}{\partial S_i-2}$？所以这里其实也不是幂次方的关系？</strong></p>
</li>
</ul>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><ol>
<li><p>梯度爆炸一般的解决方法是使用gradient clipping(梯度裁剪)，即通过设置梯度的值域范围来限制梯度过大。</p>
</li>
<li><p>L2正则化：$Loss=(y-W^tx)^2+\alpha \cdot \Vert W \Vert^2$，如果权重过大，可以通过Loss的减小来一定程度控制W过大。</p>
</li>
</ol>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>在神经网络中，梯度消失出现的更多一些，也更难解决。</p>
<ol>
<li><p>用tanh取代sigmoid激活函数。因为tanh导数的范围是-1到1，相对sigmoid的导数0到$\frac{1}{4}$，tanh可以相对sigmoid在一定程度缓解梯度消失</p>
</li>
<li><p>使用relu激活函数，当x&gt;0时，relu导数是1。但是觉得他会引起一些其他问题，并且在x&gt;0时为线性函数，所以有点失去了使用激活函数的意义，上面已经提到过了</p>
</li>
<li><p>batchnorm(简称BN)非常复杂，以后专门弄一篇研究一下。在反向传播中$\delta’(z) \cdot w_i$有W的存在，我们通过强行拉回偏离的W的分布到均值为0方差为1的正太分布上，消除了W带来的放大和缩小的问题，来一定程度缓解梯度消失（梯度爆炸）的问题</p>
<p> 有文章说这里是通过BN改变激活函数的输入，使其落在激活函数比较敏感的区域，让输入的微小变化造成Loss较大的变化来缓解梯度消失，这一点还要详细思考一下。</p>
<p> a. 我们用BN对数据进行缩放，强行改变数据输入，是否会对模型有影响？</p>
<p> b. BN中，如果batch size过小，会导致数据不准确，过大会对内存有要求</p>
<p> c. 这里norm的是输入x还是权重w？：两个都不是，而是激活函数的输入$Wx+b$</p>
</li>
<li><p>残差结构：LSTM中cell state的应用，就类似于残差结构，可以一定程度上让在序列前面的信息不受影响的（较完整的）传输到后面的序列中</p>
</li>
<li><p>谨慎选择随机初始化权重，如xavier初始化</p>
</li>
</ol>
<h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><p>非常感谢<a href="https://zhuanlan.zhihu.com/p/76772734" target="_blank" rel="noopener">这篇文章</a>，很全面的解释了很多我的疑惑。</p>
<h3 id="hexo-markdown"><a href="#hexo-markdown" class="headerlink" title="hexo-markdown"></a>hexo-markdown</h3><ol>
<li><p>又一个新发现，在markdown中，如果使用<code>$</code>，公式会和字在同一层展示，如果使用<code>$$</code>，公式会自己单独用一行展示。如下：</p>
<ul>
<li><code>$</code>：$y=f(x)$</li>
<li><code>$$</code>：$$y=f(x)$$</li>
</ul>
</li>
<li><p>尝试了很多插入图片的方法。只有复制github上图片连接最好用，这样在markdown，首页和文章中都可以正确显示图片。如果只是在<code>_config.yml</code>中修改<code>post_asset_folder:true</code>，再使用相对路径来显示图片，那么图片只能在文章首页正确显示，但是不能在文章中正确显示，并且在markdown中的相对路径和在网页上的，不一定是一样的。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>RNN</tag>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
        <tag>DNN</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2019/12/06/LSTM/</url>
    <content><![CDATA[<h2 id="LSTM来源"><a href="#LSTM来源" class="headerlink" title="LSTM来源"></a>LSTM来源</h2><p>LSTM由RNN变化而来，所以在介绍LSTM前，我们先了解一下RNN。</p>
<p>RNN是一类用来处理序列数据的神经网络，当前状态不仅受到当前输入$x_t$的影响，也受前面状态$h_{t-1}$的影响</p>
<p>$h_t=\delta (W^{h_{t-1}} \cdot h_{t-1} + W^{x_t} \cdot x_t)$<br>$y=\delta (W^y \cdot h_t)$</p>
<pre><code>ignore bais here</code></pre><p>且神经元是共享权重W的，即:<br>$W^{h_{t-1}}=W^{h_{t}}$<br>$W^{x_{t-1}}=W^{x_t}$<br>$W^{y-1}=W^y$</p>
<a id="more"></a>

<h2 id="RNN问题"><a href="#RNN问题" class="headerlink" title="RNN问题"></a>RNN问题</h2><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>RNN在实际应用中又一个很大的问题，就是长期依赖性问题，即一个输出$y_t$主要受到其前面几个输入和状态的影响，对较长时间前的输入和状态则非常不敏感，这是由于反向传播中梯度消失引起的。<br>为了解决这一问题，我们引入了LSTM（门机制）。</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>我们在计算RNN反向传播的梯度时会发现，因为RNN是一个共享权重的神经网络，所以当权重W可以特征分解为$W=Q \cdot \Lambda \cdot Q^T$且Q为正交矩阵时，则梯度包含特征值矩阵的幂次方，因此当特征值&gt;1时，则会梯度爆炸，反之则会梯度消失。<br>梯度爆炸比较容易察觉，一般表现为loss变为Nan<br>梯度消失则相对比较难以察觉，一般表现为loss几乎不变，但是loss几乎不变不代表就是产生了梯度消失</p>
<h2 id="LSTM基本描述"><a href="#LSTM基本描述" class="headerlink" title="LSTM基本描述"></a>LSTM基本描述</h2><p>LSTM一定程度上解决了RNN梯度消失和梯度爆炸的问题，得益于三个门的引入</p>
<h3 id="LSTM的三个门"><a href="#LSTM的三个门" class="headerlink" title="LSTM的三个门"></a>LSTM的三个门</h3><p>在LSTM中引入了三个门：输入门，输出门和遗忘门</p>
<p>遗忘门：控制了上一个cell的状态$C_{t-1}$中有多少信息进入到当前状态$C_t$中<br>输入门：控制了有多少信息会被保存到cell状态$C_t$中<br>输出门：控制了当前cell的状态$C_t$(cell state)有多少信息会输出到下一个神经元（即隐藏单元输出hidden state）</p>
<p><img src="/images/image/LSTM3-chain.png" alt="LSTM"></p>
<h3 id="LSTM特点"><a href="#LSTM特点" class="headerlink" title="LSTM特点"></a>LSTM特点</h3><p>LSTM一定程度上将RNN中反向传播中梯度中的叠成关系转换成叠加关系，所以缓解了梯度消失和梯度爆炸的问题。</p>
<h3 id="LSTM和GRU的比较"><a href="#LSTM和GRU的比较" class="headerlink" title="LSTM和GRU的比较"></a>LSTM和GRU的比较</h3><p>LSTM有三个门，GRU只有两个（更新门update和相关门revelance），所以LSTM的运算更为复杂。<br>GRU是在LSTM后出来的，为了缓解LSTM的计算问题，但是在一些实际应用中，GRU的效果并不比LSTM差，所以应用越来越广泛。</p>
<p><img src="/images/image/GRU.png" alt="GRU"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>关于RNN的长期依赖性问题，看了很多文章，一直不能确到底是因</p>
<ul>
<li><p>$h_t=Q^T \cdot \Lambda^t \cdot Q \cdot h_{(0)}$导致的在<strong>前向传播</strong>中$h_{(0)}$对$h_t$的影响因为幂次方的关系而非常小</p>
</li>
<li><p>还是因为梯度消失导致的长期依赖关系.</p>
<p>直到看到Andrew Ng在<a href="https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns" target="_blank" rel="noopener">视频</a>中明确指出：RNN的长期依赖性问题来源于<strong>梯度消失</strong>，并且在<a href="http://pelhans.com/2019/04/24/deepdive_tensorflow-note9/#长期依赖问题" target="_blank" rel="noopener">文章</a>中看到公式$\Delta_{h^{(0)}}$的求解，才理解：RNN的长期依赖性问题来源于反向传播中因共享权重W产生的$\Delta$的幂次方，造成了梯度消失，而使得当前状态对前面序列的依赖过小。</p>
</li>
</ul>
</li>
<li><p>在LSTM中有几种激活函数：</p>
<ul>
<li>sigmoid主要用在门的计算中，因为sigmoid很容易趋近于0或者1，所以使得门可以控制保存（等于1时）或不保存（等于0时）信息</li>
<li>softmax一般用于输出y之前</li>
<li>tanh用于求$\tilde{C}^t$和$h_{t}$的时候</li>
</ul>
</li>
<li><p>hexo中并不支持LaTex格式写公式的问题见<a href="http://stevenshi.me/2017/06/26/hexo-insert-formula/" target="_blank" rel="noopener">此文章</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>长期依赖性</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Chitchat</title>
    <url>/2019/01/16/chitchat/</url>
    <content><![CDATA[<h2 id="Chitchat"><a href="#Chitchat" class="headerlink" title="Chitchat"></a>Chitchat</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>chitchat持续的时间很长了。每次进步不大，但是至少都在前进。因为持续时间长，每次想法和改动的地方比较多，也是学到了不少东西，想要把这些都记录下来，所以开了这个文档。</p>
<p>chitchat 目前已经自己改进到v1.5了。虽然每一次改动的内容并不大，但是每一次都是一个尝试。</p>
<a id="more"></a>

<h4 id="到目前为止，每个version的改进简介"><a href="#到目前为止，每个version的改进简介" class="headerlink" title="到目前为止，每个version的改进简介"></a>到目前为止，每个version的改进简介</h4><ul>
<li><p>v1.1 是所有以前的版本，但是因为隔的太久，当时又没有记录，所以都不太记得了。</p>
</li>
<li><p>v1.2 是自己重构代码，除此之外，和第一个版本的差别不大。</p>
</li>
<li><p>v1.3 是在<code>seq2seq</code>的基础上加入了<code>bert</code>的<code>get_pooled_output</code>。</p>
</li>
<li><p>v1.4</p>
<ul>
<li>v1.4 这个版本在v1.2版本上改变了loss。因为在同时训练了v1.3版本和v1.2版本后进行比较，发现相同时间内训练出来的结果（因为版本v1.3中加入了<code>bert</code>，所以相同时间内训练出来的<code>epoch</code>数相差很大），v1.2版本的更好。因为考虑到时间成本，所以v1.4版本是在v1.2版本的基础上进行改进的。具体内容可以看v1.4的<code>README.md</code>。</li>
<li>v1.4里也加入了一些“问候”，“再见”等的规则。</li>
</ul>
</li>
<li><p>v1.5 <code>seq2seq</code>中在<code>train</code>时，一般用上一步正确的输出作为下一步的输入，但是这里改成用上一步真实的输出作为下一步的输入。具体内容可见这个文件夹下的《变形seq2seq，即在training时，上一步的输出成为下一步的输入》</p>
</li>
<li><p>又把<code>learning_rate</code>调小了，在训练了100个epoch，但是结果还是很糟糕。</p>
</li>
<li><p>v1.6 尝试在v1.4的基础上把<code>layer</code>从3改成6</p>
</li>
</ul>
<h3 id="闲聊"><a href="#闲聊" class="headerlink" title="闲聊"></a>闲聊</h3><p>闲聊原本数据是120万左右，后来经过数据清洗后，剩下80万左右的数据。<br>数据清洗：</p>
<ol>
<li>删除重复对话</li>
<li>将多轮对话剪切成单轮对话</li>
<li>删除回复过短的回答，或者不需要的回答/问题（如 字符，笑脸符等）</li>
<li>删除过长对话（30字以内）</li>
<li>统一回复（如 哈哈哈哈哈哈 =&gt; 哈哈哈哈哈，如 我不知道啊，哈哈哈 =&gt; 我不知道啊）</li>
</ol>
<h4 id="流程设计"><a href="#流程设计" class="headerlink" title="流程设计"></a>流程设计</h4><ol>
<li>“问候”和“再见”等，是用规则写的</li>
<li>如果没有匹配到“问候”或者“再见”，就会进入到模型生成回复</li>
<li>模型是seq2seq+attention</li>
<li>attention是哪个attention，对结果影响并不大</li>
<li>把数据分为几块，比如5-10个字的句子一起，10-20个字的句子一起，等等，然后让句子从短到长进行输入，一是让训练更稳定，二是能一定程度上加快速度</li>
</ol>
<h4 id="模型问题"><a href="#模型问题" class="headerlink" title="模型问题"></a>模型问题</h4><ol>
<li>无意义回答过多 =&gt; 加入anti-LM</li>
<li>相似回答过多 =&gt; 加入余弦相似到loss中，有一定的效果，但是很难统计，因为可能对某个测试集，效果明显，对某些测试集，效果就不是那么明显</li>
<li>简单问题正确回答的百分比 =&gt; “问候”等对话进行了规则处理</li>
<li>过于简短回答的百分比 =&gt; 数据集上进行了处理</li>
</ol>
<h5 id="anti-LM"><a href="#anti-LM" class="headerlink" title="anti-LM"></a>anti-LM</h5><p>通常情况下，我们的损失函数loss为${\hat T}=\underset{T}{argmax} {logp(T|S)}$。从损失函数我们可以看出，在给定输入$S$的情况下，模型会选择输出概率最大的句子，这回倾向输出出现频率大的句子，通常这些句子为无意义句子。<br>所以我们引入互信息作为新的目标函数，以两个句子的相关性作为度量标准。互信息公式如下：<br>$log \frac{p(S|T)}{p(S)p(T)}$<br>所以表达式可以写成</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\log p(T)-\log p(S)}$</p>
<p>但因为$S$为输入，$\log p(S)$是固定的，所以表达式也为</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\log p(T)}$</p>
<p>对惩罚项$-\log (T)$前加入一个参数，即为</p>
<p>${\hat T}=\underset{T}{argmax} {logp(T|S)-\lambda logp(T)}$</p>
<p>我们发现虽然目标函数改为互信息MMI，但是损失函数最大的改变就是加入了$-log(T)$，而$p(T)$表示一个句子出现的概率，所以这个损失函数其实是加了一个对出现概率大的句子的惩罚，$\lambda$就是那个惩罚系数。</p>
<p>惩罚项$-logp(T)$中的</p>
<p>$p(T)=\prod_{k=1}^{N_{t}}p(t_k|t_1,t_2,…,t_{k-1})$</p>
<p>在句子$T$的生成中，第一个字$t_1$的生成一定程度上决定了后面字$t_k$的生成，所以$t_1$相对于$t_k$而言，对句子$T$的影响更大。又因为如果我们直接让</p>
<p>$p(T)=\prod_{k=1}^{N_{t}}p(t_k|t_1,t_2,…,t_{k-1})$</p>
<p>实际生成的句子可能会更容易有语法问题。所以我们取了一个折中的方法，让</p>
<p>$U(T)=\prod_{k=1}^{N_{t}}p(t_k|t_1,t_2,…,t_{k-1}) \cdot g(k)$ </p>
<p>with<br>$$ g(k)=<br>  \begin{cases}<br>  1 &amp; {if k \leq \gamma} \<br>  0 &amp; {if k \geq \gamma}<br>  \end{cases}<br>  $$<br>取代$p(T)$，使得损失函数为</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\lambda \log U(T)}$</p>
<p>另外实际应用中，还加入了响应句子长度的因素$\gamma N_t$，作为模型相应的依据，因此将目标函数修正如下</p>
<p>$Score(T) = p(T|S)-\lambda U(T) + \gamma N_t$</p>
<p>所以损失函数应该是</p>
<p>$\underset{T}{argmin} {-\log p(T|S)+\lambda \log p(T)-\log (\gamma N_t)}$</p>
<h5 id="anti-bidi"><a href="#anti-bidi" class="headerlink" title="anti-bidi"></a>anti-bidi</h5><p>由上面的推导，我们从MMI得到</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\lambda \log p(T)}$</p>
<p>我们又知道</p>
<p>$\log p(T)=\log p(T|S) + \log p(S) - \log p(S|T)$</p>
<p>带入得到</p>
<p>${\hat T}=\underset{T}{argmax} {(1-\lambda)\log p(T|S)+\lambda \log p(S|T)}$</p>
<p>因为$p(S|T)$需要通过生成的$T$来再次通过训练好的seq2seq模型再计算一遍，计算量特别大，所以我们通过beam search只取前$N$个$T$，比如200个，再计算$S$。</p>
<p>但是在使用beam search的时候，我们知道，beam search有很多输出只是一两个字或者只是符号发生了变化，减少了取$N$个不同输出$T$的意义，因为本身输出句子的多样性不能满足这里的要求，所以我们对beam search进行了更改。</p>
<p>一般beam search在到下一节点时，我们是通过混合所有父节点的字节点，统一进行比较，然后取整体概率最大的$N$个选择，这里我们更改为，针对每一个父节点，我们先对其对应的字节点按先后顺序将其概率从上往下减去一个越来越大的数，使得我们能更好的保存更多不同的父节点，自然句子的多样性也就增加了。具体例子见下图<img src="/public/images/image/beam_search_anti_bidi.png" alt="beamsearch_antibidi"><br>另外，也在anti-bidi中，做了同anti-LM相同的处理，增加了回复长度的影响。</p>
<p>闲聊用的是seq2seq，loss中加入了anti-lm来减少无意义回复，加入了问句与回答的余弦相似性来减少重复问题的回答</p>
<h5 id="anti-bidi-VS-anti-LM"><a href="#anti-bidi-VS-anti-LM" class="headerlink" title="anti-bidi VS anti-LM"></a>anti-bidi VS anti-LM</h5><ol>
<li>两者虽然都是使用的MMI，但是使用的公式不一样，背后的逻辑原理也不一样</li>
<li>anti-bidi比anti-LM要更耗时，因为他要经过两次模型，且非常依赖于beam search生成的句子的多样性</li>
</ol>
<h5 id="anti-LM的具体实现"><a href="#anti-LM的具体实现" class="headerlink" title="anti-LM的具体实现"></a>anti-LM的具体实现</h5><p>如何计算P(T)<br>首先应该意识到P(T)应该是一个语言模型学习出来的结果，而不简单的是联合概率的乘积。那么如何得到这个模型学习出来的结果呢？在实现时需要在decoder的输入端输入一个全零的初始化状态，具体做法可以输入一句话长度的pad标志符，让decoder生成结果，得到P(T)</p>
<p>详见<code>just_another_seq2seq_master</code>里<code>train-anti.py</code>中第86～88行和第98～101行。</p>
<p>下面代码见前三行和中间的<code>add_loss</code>：</p>
<ol>
<li>前三行代码：输入为一定维度的PAD</li>
<li><code>add_loss</code>计算的是输入为PAD时(即初始状态下)的输出，即$P(T)$</li>
<li><code>add_loss *= -0.5</code>表示每次计算完$P(T)$后乘以$-0.5$，即$\lambda=0.5$</li>
<li>我们没有用$U(T)$，但是并没有明显输出语句不通顺的情况</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dummy_encoder_inputs = np.array([</span><br><span class="line">    np.array([WordSequence.PAD]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(batch_size)])</span><br><span class="line">dummy_encoder_inputs_lengths = np.array([<span class="number">1</span>] * batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epoch + <span class="number">1</span>):</span><br><span class="line">    costs = []</span><br><span class="line">    bar = tqdm(range(steps), total=steps,</span><br><span class="line">                desc=<span class="string">'epoch &#123;&#125;, loss=0.000000'</span>.format(epoch))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> bar:</span><br><span class="line">        x, xl, y, yl = next(flow)</span><br><span class="line">        x = np.flip(x, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        add_loss = model.train(sess,</span><br><span class="line">                                dummy_encoder_inputs,</span><br><span class="line">                                dummy_encoder_inputs_lengths,</span><br><span class="line">                                y, yl, loss_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        add_loss *= <span class="number">-0.5</span></span><br><span class="line">        <span class="comment"># print(x, y)</span></span><br><span class="line">        cost, lr = model.train(sess, x, xl, y, yl,</span><br><span class="line">                                return_lr=<span class="literal">True</span>, add_loss=add_loss)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        bar.set_description(<span class="string">'epoch &#123;&#125; loss=&#123;:.6f&#125; lr=&#123;:.6f&#125;'</span>.format(</span><br><span class="line">            epoch,</span><br><span class="line">            np.mean(costs),</span><br><span class="line">            lr</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">    model.save(sess, save_path)</span><br></pre></td></tr></table></figure>

<h4 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h4><h4 id="anti-lm"><a href="#anti-lm" class="headerlink" title="anti-lm"></a>anti-lm</h4><h5 id="最大互信息"><a href="#最大互信息" class="headerlink" title="最大互信息"></a>最大互信息</h5><h4 id="attention-LSTM"><a href="#attention-LSTM" class="headerlink" title="attention+LSTM"></a>attention+LSTM</h4><h5 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h5><h5 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h5><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>闲聊的评估是带有非常大的主观性的，而且场景不同，用户不同，对相同结果的满意度也会不一样，所以我们对模型的评估希望从更客观、更基础的地方来进行评估和改进：</p>
<ol>
<li>无意义回答的频率/百分比</li>
<li>相似回答的频率/百分比</li>
<li>正确回答的百分比</li>
<li>简单问题正确回答的百分比</li>
<li>相同问题，是否能给出相同答案</li>
<li>过于简短回答的百分比</li>
<li>对话内容大多无意义，无法深入交谈具体事情</li>
</ol>
<h4 id="努力方向"><a href="#努力方向" class="headerlink" title="努力方向"></a>努力方向</h4><ol>
<li>模型优化</li>
<li>数据集质量优化+数据集扩充</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>chitchat</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>protobuf</title>
    <url>/2019/01/16/protobuf/</url>
    <content><![CDATA[<h4 id="protobuf"><a href="#protobuf" class="headerlink" title="protobuf"></a>protobuf</h4><ul>
<li><code>protobuf</code>：把消息序列化的工具</li>
<li><code>rpc</code>： 远程方法调用</li>
<li><code>redis</code>：缓存</li>
<li><code>mysql</code>：持久储存</li>
</ul>
<h4 id="搭建微服务实例"><a href="#搭建微服务实例" class="headerlink" title="搭建微服务实例"></a>搭建微服务实例</h4><p><a href="https://blog.goodaudience.com/ml-client-server-using-grpc-in-python-3cba7693d1f5" target="_blank" rel="noopener">ML Client/Server Using gRPC in Python – Good Audience</a></p>
<h4 id="安装Protocol-Buffers："><a href="#安装Protocol-Buffers：" class="headerlink" title="安装Protocol Buffers："></a>安装Protocol Buffers：</h4><ol>
<li>ruby -e “$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;" target="_blank" rel="noopener">https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</a></li>
<li>brew install protobuf</li>
<li>安装gRPC：pip install grpcio</li>
<li>安装gRPC：pip install grpcio-tools</li>
</ol>
<a id="more"></a>

<h4 id="编译proto"><a href="#编译proto" class="headerlink" title="编译proto"></a>编译proto</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m grpc_tools.protoc -I .&#x2F;protos --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;intent.proto</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-I</code><br>指定生成的<code>pb2.py</code>和<code>pb2_grpc.py</code>文件的储存路径</li>
<li><code>./protos/intent.proto</code><br>需要编译的<code>.proto</code>文件的位置</li>
</ul>
<p>如果文件原来的位置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I .&#x2F;protos&#x2F; --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br><span class="line">- intent_pb2.py</span><br><span class="line">- intent_pb2_grpc.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I . --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- intent_pb2.py</span><br><span class="line">- intent_pb2_grpc.py</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br></pre></td></tr></table></figure>

<p>没弄明白为什么，不过为了方便函数之间的调用，一般使用第一种方法。</p>
<h4 id="http-VS-grpc"><a href="#http-VS-grpc" class="headerlink" title="http VS grpc"></a>http VS grpc</h4><ol>
<li><code>http</code>和<code>grpc</code>都是通过<code>tcp</code>运行。</li>
<li>不同的是<code>grpc</code>会将信息转换成二进制格式，因此传输更快，所需内存更小，效率更高。<br>但是因为<code>grpc</code>中，双方都需要编写相同的<code>.proto</code>，彼此传输的格式要固定，因此要求相对更严格一些。而http则没有这种要求，双方并没有格式的约定，所以<code>http</code>也更大一些，速度相对更慢一些。</li>
<li><code>http</code>通过<code>&quot;\\r\\n&quot;</code>来分隔信息。<code>grpc</code>则要通过<code>.proto</code>文件里的<code>message</code>严格规范信息的数目、类型等。</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title>docker</title>
    <url>/2019/01/16/docker/</url>
    <content><![CDATA[<h2 id="docker-挂载"><a href="#docker-挂载" class="headerlink" title="docker 挂载"></a>docker 挂载</h2><p>前面是本地文件，后面是镜像文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus&#x3D;3 -v &#x2F;Users&#x2F;shijiao&#x2F;Documents&#x2F;Bert&#x2F;:&#x2F;shijiao&#x2F;mkl conda-origin bash</span><br></pre></td></tr></table></figure>

<p><code>--cpus=8</code>设置的是cpu利用百分比，而不是个数，因为报错中，<code>Range of CPUs</code>是小数，而不是整数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus&#x3D;8 -v &#x2F;Users&#x2F;shijiao&#x2F;Documents&#x2F;Bert&#x2F;:&#x2F;shijiao&#x2F;mkl conda-origin bash</span><br><span class="line">Error response from daemon: Range of CPUs is from 0.01 to 6.00, as there are only 6 CPUs available.</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<hr>
<h2 id="SF-配置并使用-Docker"><a href="#SF-配置并使用-Docker" class="headerlink" title="SF 配置并使用 Docker"></a>SF 配置并使用 Docker</h2><h3 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h3><ol>
<li><p>Homebrew 安装命令：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">brew cask install docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 Docker：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 --name webserver nginx</span><br></pre></td></tr></table></figure>

<p> 打开 <code>http://localhost</code>，看到 <code>Welcome to nginx!</code> 说明 Docker 安装成功！</p>
<ul>
<li>-d: 让容器在后台运行</li>
<li>-p: 将容器内部使用的网络端口映射到我们使用的主机上</li>
</ul>
</li>
<li><p>停止 Nginx 服务器并删除：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker stop webserver</span><br><span class="line">docker rm webserver</span><br></pre></td></tr></table></figure>
</li>
<li><p>国内镜像加速</p>
<p> Docker -&gt; Preferences -&gt; Daemon -&gt; Registry mirrors 添加：<code>https://registry.docker-cn.com</code></p>
</li>
<li><p>测试加速器</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker info</span><br></pre></td></tr></table></figure>

<p> 如有以下配置内容，说明加速器配置成功：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">Registry Mirrors:</span><br><span class="line">    https://registry.docker-cn.com/</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h3><blockquote>
<p>个人偏好：仅配置一个镜像 (我以<strong><em>湖人总冠军</em></strong>命名 <code>lakers_champion</code>，不包含任何代码数据)，仅仅把需要安装的package封装好<br>这样就避免不用每个项目都给它单独弄一个镜像，10G+占空间啊！</p>
</blockquote>
<ol>
<li><p>本地build镜像</p>
<p> 新建 Dockerfile，e.g.,</p>
 <figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> okwrtdsh/anaconda3:pytorch-<span class="number">10.0</span>-cudnn7</span><br><span class="line"><span class="keyword">MAINTAINER</span> kun.xie@sfmail.sf-express.com</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install --upgrade pip \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install tqdm \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install jieba \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install gensim \\</span></span><br><span class="line"><span class="bash">&amp;&amp; apt-get install -y vim</span></span><br><span class="line">&amp;&amp; apt-get install -y git</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com tensorflow tensorboardX</span></span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple cupy pynvrtc --trusted-host mirrors.aliyun.com</span><br><span class="line">&amp;&amp; pip install git+https://github.com/salesforce/pytorch-qrnn</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com torchtext</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com jieba gensim</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com opencc-python-reimplemented</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span></span><br><span class="line">&amp;&amp; conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> conda install jupyter -y --quiet</span></span><br><span class="line">&amp;&amp; conda install pytorch torchvision -c pytorch</span><br><span class="line">&amp;&amp; conda install scikit-learn -y --quiet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD . /kxie</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir kxie</span></span><br><span class="line"><span class="comment">### 其实以上安装的package可以简单点写到文件requirements.txt中，以下一行命令就能完成安装操作 -- 因为懒没做！ ###</span></span><br><span class="line"><span class="comment"># RUN pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com -r ./requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /kxie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [ <span class="string">"/bin/bash"</span> ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>加了 命令 <code>CMD [&quot;/bin/bash&quot;]</code>后，我们直接<code>docker run -it name_of_mirror</code>后就能直接进入<code>/bin/bash/</code>了。</p>
</li>
<li><p>我们也可以在运行时指定别的命令，如<code>docker run -it name_of_mirror cat /etc/os-release</code>，就是用<code>cat /etc/os-release</code>替换了<code>/bin/bash</code>命令。</p>
<p>随后在当前目录下 <code>build</code>：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">docker build -t 10.202.107.19/sfai/lakers_champion .</span><br></pre></td></tr></table></figure>
<p>可以查看本地已经build好的镜像：<code>docker images</code></p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br><span class="line">REPOSITORY                                            TAG                 IMAGE ID               CREATED                   SIZE</span><br><span class="line">10.202.107.19/sfai/lakers_champion   latest               25473e876cd6        About an hour ago   11.1GB</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>把本地build好的镜像push到Harbor</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">docker login 10.202.107.19</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
</li>
<li><p>在server上把镜像从Harbor中pull下来</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ssh 01366808@10.202.90.201</span></span><br><span class="line">sudo docker pull 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>

<p> <code>docker images</code> 同样可以查看server上目前已有的镜像</p>
</li>
<li><p>server上运行</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">sudo nvidia-docker run -i -t --rm -p &lt;随便设置_1&gt;:8888 -p &lt;随便设置_2&gt;:6006 -v /HDATA/1/01366808/shijiao/test_map:/shijiao 10.202.107.19/sfai/tf1.12-cuda10-cudnn7-speech:latest bash</span><br></pre></td></tr></table></figure>
 <p><font color="#989898">
 <ul>
 <li>`<随便设置_1>` & `<随便设置_2>` 按个人喜好自行更改，避免冲突</li>
 <li>我把所有的代码以及数据放在了server上个人文件夹`~/kxie/`里面</li>
 <li>之后打开Jupyter Notebook页面的时候由于映射关系，能看到所有数据以及代码！很方便！！</li>
 </ul>
 </font></p>

<p> 运行 <code>ternsorboard</code> &amp; <code>Jupyter Notebook</code>：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash -</span></span><br><span class="line">nohup tensorboard --host=0.0.0.0 --logdir=/data/<span class="built_in">log</span>/kxie &amp;</span><br><span class="line">/opt/conda/bin/jupyter notebook  --ip=<span class="string">'*'</span> --no-browser --allow-root --NotebookApp.token= --notebook-dir=<span class="string">'/shijiao'</span></span><br></pre></td></tr></table></figure>
<p> 这样就可以打开网页 <code>http://10.202.90.201:&lt;随便设置_1&gt;</code> 跑代码了！Jupyter的可视化还是很方便的，从本地copy paste也容易！</p>
<p> <strong>具体命令分析，可以看新工作笔记第二页</strong></p>
</li>
</ol>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h3 id="想修改镜像内容怎么办？"><a href="#想修改镜像内容怎么办？" class="headerlink" title="想修改镜像内容怎么办？"></a>想修改镜像内容怎么办？</h3><p>进入docker镜像里面改：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion /bin/bash </span><br><span class="line">修改内容 <span class="comment"># 然后退出并记住 ***Container ID***</span></span><br><span class="line">docker commit &lt;ID&gt; 10.202.107.19/sfai/lakers_champion</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">server上重新pull: sudo docker pull 10.202.107.19/sfai/lakers_champion</span><br></pre></td></tr></table></figure>

<p>e.g., 修改本地镜像，使得能够使用<code>fzf</code>:</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion</span><br><span class="line"><span class="comment"># after enter into this image:</span></span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/junegunn/fzf.git ~/.fzf</span><br><span class="line">~/.fzf/install</span><br><span class="line">vi .vimrc</span><br><span class="line"><span class="built_in">set</span> rtp+=~/.fzf</span><br><span class="line"><span class="built_in">exit</span> <span class="comment"># and remember container id</span></span><br><span class="line">docker commit &lt;container id&gt; 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/junegunn/fzf" target="_blank" rel="noopener">GitHub - junegunn/fzf: A command-line fuzzy finder</a></p>
<p>将本地文件添加到镜像中：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker cp &lt;<span class="built_in">local</span> <span class="built_in">source</span>&gt; &lt;Container ID&gt;:&lt;dest&gt;</span><br><span class="line"><span class="comment"># 然后同上commit</span></span><br></pre></td></tr></table></figure>

<p>Terminal过段时间就会中断 <code>timed out: Connection to 10.202.90.201 closed.</code> 如何重新进入正在后台运行的Docker？<br>(当然也有办法永不中断，自行Google)</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ssh 01366808@10.202.90.201</span></span><br><span class="line">sudo docker <span class="built_in">exec</span> -it &lt;ID&gt; bash</span><br></pre></td></tr></table></figure>

<p>可以用 <code>docker ps</code> 查看Docker进程以及ID</p>
<p>删除镜像：<code>sudo docker rmi &lt;ID&gt;</code></p>
<p>结束运行的容器：<code>sudo docker stop &lt;ID&gt;</code></p>
<p>修改镜像名字或者tag：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker tag &lt;old_name:old_tag&gt; &lt;new_name:new_tag&gt;</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">docker tag &lt;ID&gt; &lt;new_name:new_tag&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Sync"><a href="#Sync" class="headerlink" title="Sync"></a>Sync</h3><p><font color="#989898">好像server上不连网的，没法Git同步，影响效率！</font></p>

<ol>
<li><p>从本地scp到服务器上:</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">scp -r &lt;<span class="built_in">source</span>&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>rsync</code>：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">rsync -av -e ssh --exclude=<span class="string">'excluded dir'</span> &lt;<span class="built_in">source</span>&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>

<p> <code>--exclude=&#39;excluded dir&#39;</code>：本地<source>中有些目录的内容是你不想传到server上的</p>
 <p><font color="#989898">
 <ul>
 <li>一般情况：本地修改完代码，使用上面rysnc命令同步到server上 (或者简单点通过Jupyter copy paste)，然后跑吧！</li>
 <li>交换 source dest 位置实现反向同步</li>
 </ul>
 </font></p>
</li>
</ol>
]]></content>
      <categories>
        <category>实战问题</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow实战应用</title>
    <url>/2019/01/16/tensorflow/</url>
    <content><![CDATA[<h2 id="tensorflow-重新加载并继续训练模型"><a href="#tensorflow-重新加载并继续训练模型" class="headerlink" title="tensorflow 重新加载并继续训练模型"></a>tensorflow 重新加载并继续训练模型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ckpt_file &#x3D; tf.train.latest_checkpoint(path_save_model)</span><br><span class="line">if ckpt_file:</span><br><span class="line">    saver.restore(sess, ckpt_file)</span><br></pre></td></tr></table></figure>

<p>这里<code>ckpt_file</code>是文件<code>checkpoint</code>里<code>model_checkpoint_path</code>的值。<br>只要<code>model_checkpoint_path</code>指定的<code>checkpoint</code>在路径下存在，就会直接加载这个<code>checkpoint</code>继续训练。否则就会重新从<code>0</code>开始。</p>
<a id="more"></a>

<h3 id="tensorboard"><a href="#tensorboard" class="headerlink" title="tensorboard"></a>tensorboard</h3><p><code>log/</code>中会因为重新加载，有几个分开的文件，但是对于同一个变量（如<code>loss</code>）会在同一个图内显示所有其随着<code>steps</code>变化的曲线。<br><img src="/images/image/tensorboard.png" alt="tensorboard_tf.train.latest_checkpoint.png"></p>
<hr>
<h2 id="用tensorflow实现seq2seq的细节问题"><a href="#用tensorflow实现seq2seq的细节问题" class="headerlink" title="用tensorflow实现seq2seq的细节问题"></a>用tensorflow实现seq2seq的细节问题</h2><h3 id="1-maximum-iteration"><a href="#1-maximum-iteration" class="headerlink" title="1. maximum_iteration"></a>1. maximum_iteration</h3><p>在<code>dynamic_decode</code>中有参数<code>maximum_iteration</code>。</p>
<p>这个参数在<code>training</code>的时候并没有什么作用，因为在<code>training</code>时，<code>TrainingHelper</code>中有参数<code>sequence_length</code>，在解码到<code>sequence_length</code>的长度后，<code>TrainingHelper</code>会通过参数<code>finished</code>来告诉模型，解码已经结束。</p>
<p>在<code>inference</code>的时候，则会因为没有<code>sequence_length</code>的限制，而会不知道该在哪里停止，或者解码的句子过长。所以<code>maximum_iteration</code>在<code>inference</code>的时候，最好是有设定的。</p>
<h3 id="2-具体的seq2seq模型的输入和输出实例"><a href="#2-具体的seq2seq模型的输入和输出实例" class="headerlink" title="2. 具体的seq2seq模型的输入和输出实例"></a>2. 具体的seq2seq模型的输入和输出实例</h3><p><code>encoder_inputs</code>：length = [3, 4]</p>
<pre><code>&lt;sos&gt;, a, b, c, &lt;eos&gt;, &lt;pad&gt;
&lt;sos&gt;, c, b, a, d, &lt;eos&gt;</code></pre><p><code>decoder_inputs</code>: length = [2, 3]</p>
<pre><code>&lt;sos&gt;, e, f, &lt;eos&gt;
&lt;sos&gt;, e, f, g</code></pre><p><code>decoder_outputs</code>: length = [2, 3]</p>
<pre><code>e, f, &lt;eos&gt;, &lt;eos&gt;
e, f, g, &lt;eos&gt;</code></pre><p>可以看出，<code>&lt;sos&gt;</code>是不会出现在<code>decoder_outputs</code>里面的，无论是在<code>training</code>还是<code>inference</code>里，<code>decoder</code>的第一个输入都是<code>&lt;sos&gt;</code>，<code>decoder</code>的第一个输出就直接是句子的第一个字/词。</p>
<hr>
<h2 id="变形seq2seq，即在training时，上一步的输出成为下一步的输入。"><a href="#变形seq2seq，即在training时，上一步的输出成为下一步的输入。" class="headerlink" title="变形seq2seq，即在training时，上一步的输出成为下一步的输入。"></a>变形seq2seq，即在training时，上一步的输出成为下一步的输入。</h2><p><strong>用<code>GreedyEmbeddingHelper</code>替换<code>TrainingHelper</code></strong></p>
<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>在seq2seq的training中，我们一般用<code>target</code>，即上一步正确的输出，作为下一步的输入。但是因为这样，同样一句话在train时的输出，后infer时的输出是完全不一样的（同事说是因为模型训练的还不够好，这点有待考证）。为了使自己在infer时的输出更可控，我改变了train时的模型，下一步的输入不再是<code>target</code>（上一步的<strong>正确输出</strong>），而是上一步模型的<strong>真实输出</strong>。</p>
<h3 id="2-困难"><a href="#2-困难" class="headerlink" title="2. 困难"></a>2. 困难</h3><ul>
<li>这样最大的困难就是在<code>sequence_loss</code>的计算时会因为shape不同而报错。<br>因为少了y_label的约束，真实输出的句子长度是完全不可控的，但是在计算<code>sequence_loss</code>时计算的是两者<code>label</code>的熵（<code>nn_ops.sparse_softmax_cross_entropy_with_logits</code>），所以我们需要统一两者的<code>shape</code>。</li>
</ul>
<ul>
<li>真实输出<code>self.decoder_logits_train=outputs.rnn_outputs</code>。<code>self.decoder_logits_train</code>的<code>shape</code>是<code>[batch_size, sequence_len, vocab_size]</code>。这里的<code>sequence_len</code>实际上是这个<code>batch_size</code>中，真实输出的<strong>最长句子的长度</strong>。<br>同样，这个<code>batch_size</code>的<code>targets</code>，也会有个最长句子的长度<code>self.max_decode_length</code>。我们只需要让每个<code>batch_size</code>中的<code>sequence_len=self.max_decode_length</code>就可以了。<br>如果<code>sequence_len</code>太短，就加到<code>self.max_decode_length</code>的长度，如果太长，就用<code>tf.slice</code>给切断。<br>但是问题又来了：<ul>
<li>因为<code>self.decoder_logits_train.shape[1]</code> <code>type</code>的问题，又因为它的值是<code>None</code>，想要比较它和<code>self.max_decode_length</code>非常困难</li>
<li>就算比较了两者的值，想要把<code>self.decoder_logits_train</code>的第二维<code>tf.concat</code>到<code>self.max_decode_length</code>也要进行<code>tf.subtract(self.max_decode_length, self.decoder_logits_train.shape[1])</code>的运算。也会增加困难</li>
</ul>
</li>
</ul>
<h3 id="3-解决方案"><a href="#3-解决方案" class="headerlink" title="3. 解决方案"></a>3. 解决方案</h3><p>好在我们在数据整理的时候去除了过长的句子，使得<code>targets</code>的最大长度不会超过50。所以<code>self.max_decode_length</code>不会大于50。</p>
<p>我们可以直接<code>tf.concat</code> <code>self.decoder_logits_train</code>和一个<code>shape</code>为<code>[batch_size, 50, vocab_size]</code>的矩阵，然后用<code>tf.slice</code>把第二维按<code>self.max_decoce_length</code>来切断。这样无论<code>self.decoder_logits_train</code>是过长还是过短，我们都能得到最终想要的结果。</p>
<h3 id="4-遗留问题"><a href="#4-遗留问题" class="headerlink" title="4.遗留问题"></a>4.遗留问题</h3><ul>
<li>因为这里我们知道<code>self.max_decode_length</code>的最大长度，所以可以取巧，如果不知道的话，可能更麻烦一些。</li>
<li>当句子过短时，会因为后面都是用<code>&lt;pad&gt;</code>补上，因此可以计算出正确的<code>loss</code>。<br>但是当句子过长时，因为我们把<code>self.max_decode_length</code>后面部分全部截断，因此这里<code>loss</code>只计算了前面一部分的句子，后面部分的句子并不可控（可能后面部分非常长，但是仍然只有截断时的那个字符的差别产生<code>loss</code>——即<strong>真实输出</strong>文字，<strong>正确输出</strong>是”\<eos>“）。<br>所以<strong>可能</strong>会导致模型生成特别特别长的句子，但是只有前半部分是我们想要的。</li>
</ul>
<hr>
<h2 id="tf分布式-–-DistributionStrategy"><a href="#tf分布式-–-DistributionStrategy" class="headerlink" title="tf分布式 – DistributionStrategy"></a>tf分布式 – DistributionStrategy</h2><p>以下内容摘抄自：<br><a href="http://jcf94.com/2018/10/21/2018-10-21-tfunpacking9/" target="_blank" rel="noopener">TensorFlow 拆包（九）：High Level APIs \| Chenfan Blog</a></p>
<p><strong>看一下官方文档中对 <code>DistributionStrategy</code> 的设计思想。</strong></p>
<p>首先是一些底层的概念：</p>
<ul>
<li>Wrapped values：跨设备相关的变量可以被封装为两种类别，PerDevice 对象表示的变量在每个设备上的值不同，Mirrored 对象表示的变量在每个设备上的值都相同</li>
<li>Unwrapping and merging：考虑前面提过的这个函数 <code>call_for_each_tower(fn, w)</code>，<code>fn</code> 是模型函数，<code>w</code> 代表一些 Wrapped values。这个函数的调用过程中就包含了变量的 unwrapping 和 merging，假定在设备 <code>d0</code> 上 <code>fn(w0)</code> 得到的结果是 <code>(x, a, v0)</code>，在设备 <code>d1</code> 上 <code>fn(w1)</code> 得到的结果是 <code>(x, b, v1)</code>。首先在调用函数之前，<code>w</code> 需要被解包变成 <code>w0</code> 和 <code>w1</code> 然后分别调用 <code>fn</code> 函数。返回的结果有三种情况，第一个值都返回了一个相同的对象 <code>x</code>，则最终 <code>merge</code> 之后还是对象 <code>x</code>；第二个值是每个设备不一样的，则 <code>merge</code> 之后是一个 <code>PerDevice</code> 对象（其实就是个设备和对应值的 <code>map</code>）；第三个值是每个设备返回的分别是一组 <code>Mirrored</code> 对象的成员，则 <code>merge</code> 之后是一个 <code>Mirrored</code> 对象。所以 <code>call_for_each_tower(fn, w)</code> 在这里返回得到的就是一组 <code>(x, PerDevice{...}, Mirrored{...})</code></li>
<li>Tower context vs. Cross-tower context：Tower context 指的是对每个设备的封装上下文，通常对每个设备分别跑一遍模型函数就需要这种封装；Cross-tower context 指的是跨设备的封装上下文，比如说像 <code>reduce()</code> 这种所有设备共同参与的一个操作就需要这种封装</li>
<li>Worker devices vs. parameter devices：负责计算的设备和存参数的设备，没啥好说的。</li>
</ul>
<p>更新一个变量的常规操作如下：</p>
<ol>
<li>把输入数据集封装在 <code>d.distribute_dataset()</code> 中，然后创建一个 iterator</li>
<li>对每一个设备共同调用 d.call_for_each_tower() 来分别创建网络模型，并且最终各自得到一组梯度/变量对：<code>d0</code> 上有 <code>{(g0, v0), (g1, v1), ...}</code>，<code>d1</code> 上有 <code>{(g&#39;0, v0), (g&#39;1, v1), ...}</code> 等等这样</li>
<li>调用<code>d.reduce(VariableAggregation.SUM, t, v)</code> 或者 <code>d.batch_reduce()</code> 来对梯度求和，并且对应到各自的变量上：<code>{(Sum(g0, g&#39;0), v0), (Sum(g1, g&#39;1), v1), ...}</code></li>
<li>调用 <code>d.update(v)</code> 来对每一个变量进行更新</li>
</ol>
<p>3、4 两步如果用 <code>Optimizer</code> 中的 <code>apply_gradients()</code> 方法可以自动完成（……这就是 <code>Optimizer</code> 后来加进去那部分代码的作用），或者在一个 <code>Cross-tower context</code> 中调用 <code>_distributed_apply()</code> 方法也可以。常规的网络层都应该在 <code>Tower context</code> 中被调用。</p>
]]></content>
      <categories>
        <category>实战问题</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>
