<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>二叉树</title>
    <url>/2020/03/04/%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
    <content><![CDATA[<h3 id="二叉树"><a href="#二叉树" class="headerlink" title="二叉树"></a>二叉树</h3><h4 id="二叉树-1"><a href="#二叉树-1" class="headerlink" title="二叉树"></a>二叉树</h4><p>在计算机科学中，二叉树是每个结点最多有两个子树的树结构。通常子树被称作“左子树”（left subtree）和“右子树”（right subtree）。二叉树常被用于实现二叉查找树和二叉堆。</p>
<h4 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h4><p>二叉查找树（Binary Search Tree），（又：二叉搜索树，二叉排序树）它或者是一棵空树，或者是具有下列性质的二叉树： 若它的左子树不空，则左子树上<strong>所有结点</strong>的值<strong>均小于</strong>它的根结点的值； 若它的右子树不空，则右子树上<strong>所有结点</strong>的值<strong>均大于</strong>它的根结点的值； 它的左、右子树也分别为二叉查找树。</p>
<h4 id="二叉堆"><a href="#二叉堆" class="headerlink" title="二叉堆"></a>二叉堆</h4><h3 id="二叉树遍历"><a href="#二叉树遍历" class="headerlink" title="二叉树遍历"></a>二叉树遍历</h3><h4 id="遍历介绍"><a href="#遍历介绍" class="headerlink" title="遍历介绍"></a>遍历介绍</h4><p>二叉树的遍历分<strong>广度优先遍历</strong>和<strong>深度优先遍历</strong></p>
<p><img src="/images/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.jpg" alt="二叉树"></p>
<ol>
<li><p>广度优先遍历(层次遍历)</p>
<p>结果为：abcdfeg</p>
</li>
<li><p>深度优先遍历<br>深度优先遍历有三种方法：前序遍历，中序遍历和后序遍历</p>
<ul>
<li><p>前序遍历：abdefgc</p>
<p>根节点 -&gt; 左子树 -&gt; 右子树</p>
</li>
<li><p>中序遍历：debgfac</p>
<p>左子树 -&gt; 根节点 -&gt; 右子树</p>
</li>
<li><p>后序遍历：edgfbca</p>
<p>左子树 -&gt; 右子树 -&gt; 根节点</p>
</li>
</ul>
</li>
</ol>
<a id="more"></a>

<h4 id="代码块"><a href="#代码块" class="headerlink" title="代码块"></a>代码块</h4><h5 id="广度优先遍历"><a href="#广度优先遍历" class="headerlink" title="广度优先遍历"></a>广度优先遍历</h5><h5 id="深度优先遍历"><a href="#深度优先遍历" class="headerlink" title="深度优先遍历"></a>深度优先遍历</h5><p>二叉树深度优先遍历的三种方法大同小异，唯一需要改变的只是函数helper中子树和根的代码的运行顺序</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preoderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        self.result = []</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        self.helper(root)</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            self.result.append(root.val)</span><br><span class="line">            self.helper(root.left)</span><br><span class="line">            self.helper(root.right)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        self.result = []</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        self.helper(root)</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            self.helper(root.left)</span><br><span class="line">            self.result.append(root.val)</span><br><span class="line">            self.helper(root.right)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postorderTraversal</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        self.result = []</span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        self.helper(root)</span><br><span class="line">        <span class="keyword">return</span> self.result</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, root)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> root:</span><br><span class="line">            self.helper(root.left)</span><br><span class="line">            self.helper(root.right)</span><br><span class="line">            self.result.append(root.val)</span><br></pre></td></tr></table></figure>


<h3 id="二叉查找树查找"><a href="#二叉查找树查找" class="headerlink" title="二叉查找树查找"></a>二叉查找树查找</h3><ul>
<li>DP(Dynamic programming)：动态规划</li>
<li>DFS(Depth-First-Search)：深度优先搜索</li>
</ul>
<h4 id="给出一个n，求1…n能够得到的所有二叉树个数"><a href="#给出一个n，求1…n能够得到的所有二叉树个数" class="headerlink" title="给出一个n，求1…n能够得到的所有二叉树个数"></a>给出一个n，求1…n能够得到的所有二叉树个数</h4><p>(en: given n, how many structurally unique BST’s that store values 1…n?)</p>
<ul>
<li>n = 0，{}为空集，所以只有一种二叉树，即空树</li>
<li>n = 1，{1}也只有一种，就是1</li>
<li>n = 2，{1, 2}有两种，1 - 2， 2 - 1</li>
<li>n = 3，{1, 2, 3}<br><img src="../../public/images/%E4%BA%8C%E5%8F%89%E6%A0%911-3.png" alt="二叉树1-3"><br>综上我们可以发现：</li>
<li>n = 1时，如果根为1，有{}个二叉树</li>
<li>n = 2时，如果根为1，有{2}个二叉树，即n=1的二叉树个数；如果根为2，只有{1}个二叉树，即n=1的二叉树个数</li>
<li>n = 3时，如果根为1，有{2, 3}个二叉树，即n=2的二叉树个数；如果根为2，有{1}*{3}个二叉树个数；如果根为3，同根为1的情况</li>
</ul>
<p>当我们知道N时，它的二叉树个数即为每个{1,…,N}中数字为根时的二叉树个数的总和。</p>
<p>当其中某个元素i为根时，它的二叉树个数即 <strong>{1,…,i-1}二叉树个数</strong> x <strong>{i+1,…,N}二叉树个数</strong>，而 <strong>{1,…,i-1}二叉树个数</strong> 就是 <strong>n=i-1的二叉树个数</strong> ，<strong>{i+1,…,N}二叉树个数</strong> 就是 <strong>n=N-i的二叉树个数</strong></p>
<p>所以若先定义n为0，1，2时的二叉树个数<code>dp = [1, 1, 2]</code>，则:</p>
<ul>
<li>n = 3：<code>dp[3] = dp[0]*dp[2]+dp[1]*dp[1]+dp[2]*dp[0]</code></li>
<li>n = 4：<code>dp[4] = dp[0]*dp[3]+dp[1]*dp[2]+dp[2]*dp[1]+dp[3]*dp[0]</code></li>
<li>…</li>
</ul>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">numTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type n: int</span></span><br><span class="line"><span class="string">        :rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        dp = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">        <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">            <span class="keyword">return</span> dp[n]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dp += [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-2</span>)]  <span class="comment"># 后面创建多个</span></span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, n + <span class="number">1</span>):</span><br><span class="line">                <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">1</span>, i+<span class="number">1</span>):</span><br><span class="line">                    dp[i] += dp[j<span class="number">-1</span>] * dp[i-j]</span><br><span class="line">            <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></table></figure>

<h4 id="给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树"><a href="#给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树" class="headerlink" title="给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树"></a>给出一个n，求1…n能够得到的所有BST二叉查找树，并输出所有树</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">分别以1...n中每个元素，如i，为根：</span><br><span class="line">    根i的左子树</span><br><span class="line">        求根i的左子树：求1...i-1的所有二叉查找树（同原始问题求1...n），所以以1...i-1中的每个元素，如j，为根：</span><br><span class="line">            根j的左子树</span><br><span class="line">                ...</span><br><span class="line">            根j的右子树</span><br><span class="line">                ...</span><br><span class="line">    根的右子树</span><br><span class="line">        求根i的右子树：求i+1...n的所有二叉查找树（同原始问题求1...n），所以以i+1...n中的每个元素，如k，为根：</span><br><span class="line">            根k的左子树</span><br><span class="line">                ...</span><br><span class="line">            根k的右子树</span><br><span class="line">                ...</span><br></pre></td></tr></table></figure>
<p>具体代码如下</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TreeNode</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        self.val = x</span><br><span class="line">        self.left = <span class="literal">None</span></span><br><span class="line">        self.right = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">generateTrees</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">return</span> self.helper(<span class="number">1</span>, n)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(self, start, end)</span>:</span></span><br><span class="line">        result = []</span><br><span class="line">        <span class="keyword">if</span> start &gt; end:</span><br><span class="line">            <span class="keyword">return</span> [<span class="literal">None</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(start, end+<span class="number">1</span>):</span><br><span class="line">            leftTree = self.helper(start, i<span class="number">-1</span>)</span><br><span class="line">            rightTree = self.helper(i+<span class="number">1</span>, end)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> l <span class="keyword">in</span> range(len(leftTree)):</span><br><span class="line">                <span class="keyword">for</span> r <span class="keyword">in</span> range(len(rightTree)):</span><br><span class="line">                    root = TreeNode(i)</span><br><span class="line">                    root.left = leftTree[l]</span><br><span class="line">                    root.right = rightTree[r]</span><br><span class="line">                    result.append(root)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure>

<h3 id="二叉树最大深度和最小深度"><a href="#二叉树最大深度和最小深度" class="headerlink" title="二叉树最大深度和最小深度"></a>二叉树最大深度和最小深度</h3><h4 id="二叉树最大深度"><a href="#二叉树最大深度" class="headerlink" title="二叉树最大深度"></a>二叉树最大深度</h4><p>二叉树最大深度可以用一下代码来求：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxDepth</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span> <span class="comment"># 60%</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        type: root:Tree</span></span><br><span class="line"><span class="string">        rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.maxDepth = <span class="number">0</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(root, depth)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">                self.maxDepth = max(depth, self.maxDepth)</span><br><span class="line">            <span class="keyword">if</span> root:</span><br><span class="line">                helper(root.left, depth+<span class="number">1</span>)</span><br><span class="line">                helper(root.right, depth+<span class="number">1</span>)</span><br><span class="line">        helper(root, <span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> self.maxDepth</span><br></pre></td></tr></table></figure>
<p>leetcode上没有问题，但是代码本身是有一个问题要注意的（也不完全算漏洞）</p>
<p>比如在下图中</p>
<p><img src="/images/%E4%BA%8C%E5%8F%89%E6%A0%91%E7%A4%BA%E4%BE%8B%E5%9B%BE.jpg" alt="二叉树"></p>
<p>当遍历到<code>d</code>的时候，<code>d</code>没有左子树，<code>d.left==None</code>，所以会进入到代码<code>root==None</code>，那么此时的<code>depth=3</code>，这个<code>depth</code>其实只是到<code>d</code>的二叉树深度。</p>
<p>因为需要求的是<strong>最大深度</strong>，如果此节点<code>d</code>没有右子树，那么这条路径的深度就是<code>depth=3</code>；如果此节点有右子树，右子树的深度肯定比<code>depth=3</code>大，那么<code>maxDepth</code>就会被右子树的深度所取代，也就不会有 <strong>到节点<code>d</code>的路径深度<code>depth=3</code></strong> 出现了，这个问题得以被解决，但是在二叉树<strong>最小深度</strong>中，这个问题就暴露出来，不能直接套用上述代码了。</p>
<h4 id="二叉树最小深度"><a href="#二叉树最小深度" class="headerlink" title="二叉树最小深度"></a>二叉树最小深度</h4><p>因为上面描述的原因，所以要对代码进行改进：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">minDepth</span><span class="params">(self, root: TreeNode)</span> -&gt; int:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        type: root:Tree</span></span><br><span class="line"><span class="string">        rtype: int</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> root == <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        self.minDepth = []</span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">helper</span><span class="params">(root, depth)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> root.left == <span class="literal">None</span> <span class="keyword">and</span> root.right == <span class="literal">None</span>:</span><br><span class="line">                self.minDepth.append(depth)</span><br><span class="line">            <span class="keyword">if</span> root.left:</span><br><span class="line">                helper(root.left, depth+<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> root.right:</span><br><span class="line">                helper(root.right, depth+<span class="number">1</span>)</span><br><span class="line">        helper(root, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> min(self.minDepth)</span><br></pre></td></tr></table></figure>
<p>代码不算简洁，但是在leetcode上跑到了94。69%，效果很不错，开心开心。</p>
<h3 id="验证一个二叉树是否是二叉查找树"><a href="#验证一个二叉树是否是二叉查找树" class="headerlink" title="验证一个二叉树是否是二叉查找树"></a>验证一个二叉树是否是二叉查找树</h3><p>虽然二叉查找树具体到定义上比较复杂，但是可以发现，如果把二叉查找树<strong>中序遍历</strong>，则能得到一个<strong>有序数组</strong>，否则得到的数组并非完全有序的。</p>
<p>以后如果有子树、根之间的比较等等，不妨看看是否可以通过遍历方式入手来简化。</p>
]]></content>
      <categories>
        <category>leetcode</category>
      </categories>
      <tags>
        <tag>二叉树</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
  <entry>
    <title>使用hexo搭建博客</title>
    <url>/2020/02/19/%E4%BD%BF%E7%94%A8hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h2 id="Hexo-github"><a href="#Hexo-github" class="headerlink" title="Hexo + github"></a>Hexo + github</h2><h3 id="基本需求"><a href="#基本需求" class="headerlink" title="基本需求"></a>基本需求</h3><ol>
<li><p><code>github</code>上创建仓库，仓库名必须严格是<code>username.github.io</code></p>
</li>
<li><p>创建本地写博客的hexo文件夹MyHexo，然后进入文件夹，执行命令行<code>hexo init</code>，如果报错，会提示输入<code>npm install hexo --save</code></p>
</li>
<li><p>进入根目录下的<code>_config.yml</code>，增加</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">deploy:</span><br><span class="line">  type: git</span><br><span class="line">  repository: git@github.com:username&#x2F;username.github.io.git</span><br><span class="line">  branch: master</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行命令<code>hexo g</code>，如果报错，一般是因为没有安装<code>git</code>，执行<code>npm install hexo-deployer-git --save</code>安装<code>hexo</code>下的<code>git</code>，然后重新<code>hexo g</code></p>
</li>
<li><p>执行<code>hexo d</code>，如果没有关联<code>git</code>和<code>hexo</code>，会自动提醒输入</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Username for &#39;https:&#x2F;&#x2F;github.com&#39;:</span><br><span class="line">Password for &#39;https:&#x2F;&#x2F;github.com&#39;:</span><br></pre></td></tr></table></figure>
<p> 这里我用了<code>ssh</code>钥匙。</p>
<ul>
<li>执行命令<code>ssh-keygen -t rsa -C github_de_email@gmail.com</code></li>
<li>进入<code>~/.ssh</code>复制文件<code>id_rsa.pub</code>里的公匙，进入到<code>github</code>里的<code>setting</code>，进入<code>SSH and GPG keys</code>，新建SSH钥匙<code>new SSH key</code>，粘贴公匙，<code>title</code>可以随意取或者不取，然后确定</li>
<li>运行<code>ssh -T git@github.com</code>，如果看到<code>Hi username! You&#39;ve successfully authenticated, but GitHub does not provide shell access.</code>证明SSH连接成功。</li>
</ul>
</li>
<li><p>自己博客的域名为 <a href="https://shijiaod.github.io/" target="_blank" rel="noopener">https://shijiaod.github.io/</a><br>有时<code>hexo d</code>后页面没有变化，可以尝试重启电脑…</p>
</li>
</ol>
<a id="more"></a>

<h4 id="踩过的坑之血与泪的教训"><a href="#踩过的坑之血与泪的教训" class="headerlink" title="踩过的坑之血与泪的教训"></a>踩过的坑之血与泪的教训</h4><ol>
<li><p><strong>⚠️ 注 ⚠️</strong>：硬是弄了一天才发现，是仓库名见错了，所以怎么<code>hexo d</code>都报错，说<code>ERROR: Repository not found.</code>，后来发现仓库名后面应该跟上<code>github.io</code>，即完整仓库名应该是<code>username.github.io</code>而不是<code>username</code>！</p>
</li>
<li><p>当时在<code>hexo d</code>一直报错要输入<code>username</code>和<code>password</code>时，尝试使用<code>git remote add origin</code>来连接<code>github</code>，但是更换到正确的仓库名后，发现自己删除掉以前建立的<code>origin</code>也不影响，可见这里其实不需要更多的步骤</p>
</li>
<li><p>执行命令<code>ssh -T git@github.com</code>后出现</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">The authenticity of host &#39;github.com (13.229.188.59)&#39; can&#39;t be established.</span><br><span class="line">RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.</span><br><span class="line">Are you sure you want to continue connecting (yes&#x2F;no)?</span><br></pre></td></tr></table></figure>
<p> 是因为没有配置<code>ssh</code>，或者本地缺少一个文件夹（什么文件夹我也不清楚…）<br> 如果是没有配置<code>ssh</code>见上方如何配置，如果是后者，输入<code>yes</code>，而非回车或者<code>y</code></p>
</li>
<li><p>在升级npm，安装hexo下的git，重新安装hexo时，都会报错</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">No receipt for &#39;com.apple.pkg.CLTools_Executables&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">No receipt for &#39;com.apple.pkg.DeveloperToolsCLILeo&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">No receipt for &#39;com.apple.pkg.DeveloperToolsCLI&#39; found at &#39;&#x2F;&#39;.</span><br><span class="line"></span><br><span class="line">gyp: No Xcode or CLT version detected!</span><br><span class="line">gyp ERR! configure error</span><br><span class="line">gyp ERR! stack Error: &#96;gyp&#96; failed with exit code: 1</span><br><span class="line">gyp ERR! stack     at ChildProcess.onCpExit (&#x2F;usr&#x2F;local&#x2F;lib&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;node-gyp&#x2F;lib&#x2F;configure.js:351:16)</span><br><span class="line">gyp ERR! stack     at ChildProcess.emit (events.js:210:5)</span><br><span class="line">gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal&#x2F;child_process.js:272:12)</span><br><span class="line">gyp ERR! System Darwin 19.3.0</span><br><span class="line">gyp ERR! command &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;node&quot; &quot;&#x2F;usr&#x2F;local&#x2F;lib&#x2F;node_modules&#x2F;npm&#x2F;node_modules&#x2F;node-gyp&#x2F;bin&#x2F;node-gyp.js&quot; &quot;rebuild&quot;</span><br><span class="line">gyp ERR! cwd &#x2F;Users&#x2F;shijiaodeng&#x2F;Documents&#x2F;MyBlog&#x2F;shijiaod&#x2F;node_modules&#x2F;nunjucks&#x2F;node_modules&#x2F;fsevents</span><br><span class="line">gyp ERR! node -v v12.13.1</span><br><span class="line">gyp ERR! node-gyp -v v5.0.5</span><br><span class="line">gyp ERR! not ok</span><br></pre></td></tr></table></figure>
<p> 是升级了mac的原因（到版本10.15.3），所以下载并安装Xcode并同意它的相关协议，问题就解决了。具体详见<a href="https://segmentfault.com/a/1190000021394623?utm_source=tag-newest" target="_blank" rel="noopener">这片文章</a></p>
</li>
</ol>
<h3 id="内容更丰富的博客"><a href="#内容更丰富的博客" class="headerlink" title="内容更丰富的博客"></a>内容更丰富的博客</h3><h4 id="博客主题"><a href="#博客主题" class="headerlink" title="博客主题"></a>博客主题</h4><ol>
<li><p><code>hexo</code>根目录下安装主题nexT<code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code></p>
</li>
<li><p><code>hexo</code>根目录下修改文件<code>_config.yml</code>中的主题为<code>nexT</code>：<code>theme: next</code></p>
</li>
<li><p>到next主题下更改配置文件<code>/hexo/theme/next/_config.yml</code>中的<code>scheme: pisce</code>，里面有四种主题可以选，<code>pisce</code>是经典的旁边有小栏框的格式</p>
</li>
<li><p>我喜欢把<code>sidebar</code>放在右边:<code>/hexo/theme/next/_config.yml</code>中改为</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sidebar:</span><br><span class="line"># Sidebar Position.</span><br><span class="line"># position: left</span><br><span class="line">position: right</span><br></pre></td></tr></table></figure>
</li>
<li><p>进入主题目录下的languages文件夹中，<code>cp zh-CN.yml zh-Hans.yml</code>，然后再进入<code>hexo</code>根目录下修改<strong>语言</strong>、名字等一些基本信息（我也不知道为什么要改成<code>zh-Hans</code>而不是直接用<code>zh-CN</code>…）</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Site</span><br><span class="line">title: 诗娇的博客</span><br><span class="line">subtitle: &#39;&#39;</span><br><span class="line">description: &#39;&#39;</span><br><span class="line">keywords:</span><br><span class="line">author: Shijiao DENG</span><br><span class="line">language: zh-Hans</span><br><span class="line">timezone: &#39;&#39;</span><br></pre></td></tr></table></figure>
</li>
<li><p>在<code>/hexo/theme/next/_config.yml</code>中配置<code>avatar</code>设置图像</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: #&#x2F;images&#x2F;avatar.gif  #头像图片路径 图片放置在hexo&#x2F;public&#x2F;images</span><br><span class="line">  rounded: false  #是否显示圆形头像，true表示圆形，false默认</span><br><span class="line">  opacity: 0.7  #透明度0~1之间</span><br><span class="line">  rotated: false  #是否旋转 true表示旋转，false默认</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li><p>进入主题目录下的<code>_config.yml</code>修改主页目录，可以更改配置里的上下顺序来更改他们在主页各自排列的顺序，比如<code>about</code>本来在<code>home</code>下面，我们把它移到了最下面</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">menu:</span><br><span class="line">  home: &#x2F; || home</span><br><span class="line">  #about: &#x2F;about&#x2F; || user</span><br><span class="line">  tags: &#x2F;tags&#x2F; || tags</span><br><span class="line">  categories: &#x2F;categories&#x2F; || th</span><br><span class="line">  archives: &#x2F;archives&#x2F; || archive</span><br><span class="line">  schedule: &#x2F;schedule&#x2F; || calendar</span><br><span class="line">  #sitemap: &#x2F;sitemap.xml || sitemap</span><br><span class="line">  #commonweal: &#x2F;404&#x2F; || heartbeat</span><br><span class="line">  about: &#x2F;about&#x2F; || user</span><br></pre></td></tr></table></figure>
<p> 也可以自己增加一个目录内容，不过这些配置都要与主题目录下的languages文件中对应的<code>yml</code>文档里配置相关联。比如你在站点根目录中的配置文件设置<code>language</code>为<code>zh-Hans</code>，那么就要进入到主题目录下的languages文件中修改<code>zh-Hans.yml</code>，这样才能显示出菜单项新增的中文内容（以something为例子）</p>
</li>
<li><p>前面通过修改next主题下的<code>_config.yml</code>文件中的<code>menu</code>选项，可以在主页面的菜单栏添加”标签”选项，但是此时点击”标签”，跳转的页面会显示”page not found”。此时我们要新建一个页面</p>
<p> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A201.png" alt="图一"><br> 在新建的<code>index.md</code>文件中添加<code>type: &quot;tags&quot;</code><br> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A202.png" alt="图二"><br> <code>categories</code>等同理</p>
</li>
<li><p>设置头像：</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: &#x2F;images&#x2F;avatar.gif  #头像图片路径 图片放置在next&#x2F;source&#x2F;images</span><br><span class="line">  rounded: false  #是否显示圆形头像，true表示圆形，false默认</span><br><span class="line">  opacity: 0.7  #透明度0~1之间</span><br><span class="line">  rotated: false  #是否旋转 true表示旋转，false默认</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>/hexo/theme/next/_config.yml</code>的<code>menu_settings</code>如果设置<code>icon: false</code>则无前面的图标，<code>badges: true</code>则标签都会显示数字</p>
</li>
<li><p>社交设置<code>/hexo/theme/next/_config.yml</code>里的<code>social</code></p>
</li>
<li><p>自动在文章中生成目录，在文章最前面加入<code>[toc]</code>，具体设置如下，但自己试了没有效果</p>
 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">toc:</span><br><span class="line">  enable: false #是否启动侧边栏</span><br><span class="line">  number: true  #自动将列表编号添加到toc。</span><br><span class="line">  wrap: false #true时是当标题宽度很长时，自动换到下一行</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="添加站内搜索"><a href="#添加站内搜索" class="headerlink" title="添加站内搜索"></a>添加站内搜索</h4><ol>
<li>安装站内搜索插件 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></li>
<li>在根目录下的<code>_config.yml</code>添加 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></li>
<li>在<code>themes/next/_config.yml</code>文件中搜索<code>local_search</code>,进行设置 <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true  # 设置为true</span><br><span class="line">  trigger: auto  # auto &#x2F;  manual，auto 自动搜索、manual：按回车[enter ]键手动搜索</span><br><span class="line">  top_n_per_article: 3 # 每篇博客显示搜索的结果数</span><br><span class="line">  unescape: true</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h4 id="不显示整篇文章"><a href="#不显示整篇文章" class="headerlink" title="不显示整篇文章"></a>不显示整篇文章</h4><p>在文章中想要显示的部分后面加上<code>&lt;!--more--&gt;</code>，从这里开始，后面文章会隐藏，只会显示”阅读全文”按钮</p>
<h4 id="更多惊喜"><a href="#更多惊喜" class="headerlink" title="更多惊喜"></a>更多惊喜</h4><p>还有很多细节，可参见以下两个网站：</p>
<p><a href="https://www.jianshu.com/p/3a05351a37dc" target="_blank" rel="noopener">网站一</a></p>
<p><a href="https://eirunye.github.io/2018/09/15/Hexo搭建GitHub博客—打造炫酷的NexT主题—高级—四/#more" target="_blank" rel="noopener">网站二</a></p>
<h2 id="如何把本地写好的-md文件推倒github网站上"><a href="#如何把本地写好的-md文件推倒github网站上" class="headerlink" title="如何把本地写好的.md文件推倒github网站上"></a>如何把本地写好的.md文件推倒github网站上</h2><ul>
<li>进入到根目录（<code>/Documents/xxxx.github.io/</code>）下，输入命令行<code>hexo g</code> (<code>hexo generate</code>)生成静态页面</li>
<li>输入命令<code>hexo d</code> (<code>hexo deploy</code>)上传到<code>github</code>上<br>这时就可以在网站上看到刚刚写的博客了。<br>如果想要先本地预览效果，可以使用<code>hexo s</code> (<code>hexo server</code>)</li>
</ul>
<h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h3><h3 id="添加评论等"><a href="#添加评论等" class="headerlink" title="添加评论等"></a>添加评论等</h3><h3 id="增加归档页面文章数目"><a href="#增加归档页面文章数目" class="headerlink" title="增加归档页面文章数目"></a>增加归档页面文章数目</h3>]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown语法</title>
    <url>/2020/02/19/Markdown%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><ol>
<li>安装插件<code>Markdown All in One</code>，包含了最常用的<code>Markdown</code>优化。</li>
<li>安装插件<code>Markdown Preview Github Styling</code>，专门针对<code>github pages</code>的预览，功能有限，但是正是我需要的</li>
<li>如果不是针对<code>github</code>的预览，则可以安装<code>Markdown Preview Enhanced</code>，这个插件应用更普遍</li>
</ol>
<a id="more"></a>

<h2 id="Markdown语法"><a href="#Markdown语法" class="headerlink" title="Markdown语法"></a>Markdown语法</h2><ol>
<li><p><strong>标题</strong>：标题有共有六个等级，在前面加上一到六个 “#”</p>
</li>
<li><p><strong>正文</strong>：正文中想要换行，必须要多跳一行，如果在代码中只换一行，那么其实没有换行</p>
</li>
<li><p><strong>代码</strong>：正文中的代码块，在前后加上”`”，如果是一段代码段落，前后分别加上”```”</p>
<p> <strong>注：可以根据不同的语言配置不同的代码着色</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">wenzi</span><span class="params">(self, n)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    print(i)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>列表</strong></p>
<p> 有序列表：输入数字 + 一个点”.” + 一个空格</p>
<p> 无序列表：输入”-“/“*”/“+” + 一个空格</p>
<ul>
<li>无序列表<ul>
<li>与前面的表示符号无关<ul>
<li>只与缩紧行数相关</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>加粗、倾斜和删除</strong></p>
<p> <strong>加粗</strong></p>
<p> <em>倾斜</em></p>
<p> <del>删除线</del></p>
</li>
<li><p><strong>引用</strong></p>
<blockquote>
<p>此处是引用</p>
</blockquote>
<blockquote>
<p>此处是多层引用第一层</p>
<p>此处是多层引用第二层</p>
</blockquote>
<blockquote>
<p>多层嵌套第一层</p>
<blockquote>
<p>多层嵌套第二层</p>
<blockquote>
<p>多层嵌套第三层</p>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><p><strong>插入连接</strong></p>
<p> <a href="https://markdown-zh.readthedocs.io/en/latest/" target="_blank" rel="noopener">Markdown中文文档</a></p>
</li>
<li><p><strong>插入图片</strong></p>
<p> <img src="images/avatar.jpeg" alt="avatar"></p>
<ul>
<li>可以在图片上传到<code>github</code>后，用github上图片的地址链接，这样网页上可以正常显示</li>
<li>可以把包含图片的文件夹docs放到本地/hexo/public/images里，用相对路径/public/images/docs/images.jpg来调用</li>
<li>更多方式参考<a href="https://fuhailin.github.io/Hexo-images/" target="_blank" rel="noopener">这个文章</a></li>
</ul>
</li>
<li><p><strong>文字的个性设置</strong>：可以直接用html语法对正文进行编辑，达到想要的展示效果</p>
<ul>
<li><p>居中：</p>
  <center>这一行居中</center>
</li>
<li><p>换色 + 变化大小：</p>
<p>  接下来就是见证奇迹的时刻<br>  <font color="#989898"> 我爱用的注释颜色 </font><br>  <font color="#FF0000"> 我可以设置这一句的颜色哈哈 </font><br>  <font size=6> 我还可以设置这一句的大小嘻嘻 </font><br>  <font size=5 color="#FF0000"> 我甚至可以设置这一句的颜色和大小呵呵</font> </p>
</li>
<li><p>分段、分行<br>  分段：<code>&lt;p&gt;&lt;/p&gt;</code><br>  分行：<code>&lt;br&gt;</code></p>
</li>
<li><p>有序、无序列表</p>
  <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;ul&gt;</span><br><span class="line">    &lt;li&gt;无序列表&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ul&gt;</span><br><span class="line"></span><br><span class="line">&lt;ol&gt;</span><br><span class="line">    &lt;li&gt;有序列表&lt;&#x2F;li&gt;</span><br><span class="line">&lt;&#x2F;ol&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong>数学公式</strong>：主题目录下<code>/hexo/theme/next/_config.yml</code>设置<code>mathjax</code>里的<code>enable: true</code><br>并且在需要使用数学公式的博客里打开公式开关：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">title: index.html</span><br><span class="line">date: 2016-12-28 21:01:30</span><br><span class="line">tags:</span><br><span class="line">mathjax: true</span><br><span class="line">---</span><br></pre></td></tr></table></figure></li>
</ol>
]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>python的点滴</title>
    <url>/2020/02/19/python%E7%82%B9%E6%BB%B4/</url>
    <content><![CDATA[<h2 id="一些函数的使用"><a href="#一些函数的使用" class="headerlink" title="一些函数的使用"></a>一些函数的使用</h2><h3 id="基本数据类型的时间复制度"><a href="#基本数据类型的时间复制度" class="headerlink" title="基本数据类型的时间复制度"></a>基本数据类型的时间复制度</h3><p><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A61.png" alt="图片一"></p>
<a id="more"></a>

<p><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A62.png" alt="图片二"><br><img src="/images/python%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E7%9A%84%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A63.png" alt="图片三"></p>
<h3 id="map-zip"><a href="#map-zip" class="headerlink" title="map, zip"></a>map, zip</h3><p>map和zip配合使用，实现矩阵转置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">原矩阵</span><br><span class="line">matrix &#x3D; [</span><br><span class="line">  [1,2,3],</span><br><span class="line">  [4,5,6],</span><br><span class="line">  [7,8,9]</span><br><span class="line">]</span><br><span class="line">class Solution:</span><br><span class="line">    def rotate(self, matrix):  </span><br><span class="line">        matrix[:] &#x3D; map(list,zip(*matrix))</span><br><span class="line">        return matrix</span><br><span class="line">print(Solution().rotate(matrix))</span><br><span class="line">[</span><br><span class="line">    [1,4,7],</span><br><span class="line">    [2,5,8],</span><br><span class="line">    [3,6,9]</span><br><span class="line">    ]</span><br></pre></td></tr></table></figure>

<p>更多关于<code>map</code>和<code>zip</code>的解释，请看<a href="https://blog.csdn.net/shawpan/article/details/69663710" target="_blank" rel="noopener">一行代码搞定矩阵旋转——python</a>。</p>
<h3 id="defaultdict-function-factory"><a href="#defaultdict-function-factory" class="headerlink" title="defaultdict(function_factory)"></a>defaultdict(function_factory)</h3><p><strong>defaultdict</strong></p>
<p>dict subclass that calls a factory function to supply missing values</p>
<p><code>defaultdict</code>构建的是一个类似<code>dictionary</code>的对象，其中<code>keys</code>值自行确定赋值，但是<code>values</code>的类型是<code>fucntion_factory</code>的类实例，而且<strong>具有默认值</strong>。比如<code>defaultdict(list)</code>创建一个<code>dict</code>,对于任何一个还不存在的<code>dict[newkey]</code>都已经有一个默认<code>list</code>使得<code>dict[newkey].append</code>可以直接运行而不需要先运行<code>dict[newkey]=[]</code>。</p>
<p>例子：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">s &#x3D; [(&#39;yellow&#39;, 1), (&#39;blue&#39;, 2), (&#39;yellow&#39;, 3), (&#39;blue&#39;, 4), (&#39;red&#39;, 1)]</span><br><span class="line">import collections</span><br><span class="line">d &#x3D; collections.defaultdict(list)</span><br><span class="line">for k, v in s:</span><br><span class="line">    d[k].append(v)</span><br><span class="line">list(d.items())</span><br><span class="line">[(&#39;blue&#39;, [2, 4]), (&#39;red&#39;, [1]), (&#39;yellow&#39;, [1, 3])]</span><br></pre></td></tr></table></figure>

<h3 id="pandas聚合和分组运算之groupby"><a href="#pandas聚合和分组运算之groupby" class="headerlink" title="pandas聚合和分组运算之groupby"></a>pandas聚合和分组运算之groupby</h3><p>根据<code>key1</code>的值<code>a</code>,<code>b</code>来分组求<code>data1</code>的平均值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; df</span><br><span class="line">data1     data2 key1 key2</span><br><span class="line">0 -0.410673  0.519378    a  one</span><br><span class="line">1 -2.120793  0.199074    a  two</span><br><span class="line">2  0.642216 -0.143671    b  one</span><br><span class="line">3  0.975133 -0.592994    b  two</span><br><span class="line">4 -1.017495 -0.530459    a  one</span><br><span class="line">&gt;&gt;&gt; grouped &#x3D; df[&#39;data1&#39;].groupby(df[&#39;key1&#39;])</span><br><span class="line">&gt;&gt;&gt; grouped</span><br><span class="line">&lt;pandas.core.groupby.SeriesGroupBy object at 0x04120D70&gt;</span><br><span class="line">&gt;&gt;&gt; grouped.mean()</span><br><span class="line">key1</span><br><span class="line">a      -1.182987</span><br><span class="line">b       0.808674</span><br><span class="line">dtype: float64</span><br></pre></td></tr></table></figure>

<p>根据<code>cpucore-</code>的类别，求所有数值形式列的平均值</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;&gt;&gt; data_mac.groupby(&#39;cpucore&#39;).mean()</span><br><span class="line">cpucore-	time_transform_bert	time_sess_run_bert	time_to_dict_bert	time_all_bert			</span><br><span class="line">1	0.003124	1.550403	0.000032	1.553559</span><br><span class="line">2	0.001235	0.730950	0.000034	0.732218</span><br><span class="line">3	0.001565	0.487936	0.000037	0.489538</span><br><span class="line">4	0.001103	0.323777	0.000029	0.324909</span><br><span class="line">5	0.001185	0.289874	0.000033	0.291093</span><br><span class="line">6	0.001226	0.284265	0.000033	0.285524</span><br><span class="line">0	0.001225	0.284462	0.000032	0.285719</span><br></pre></td></tr></table></figure>

<h3 id="is和-的区别"><a href="#is和-的区别" class="headerlink" title="is和==的区别"></a>is和==的区别</h3><p>在Python中一切都是对象。</p>
<p>Python中对象包含的三个基本要素，分别是：</p>
<ul>
<li>id(身份标识)</li>
<li>type(数据类型)</li>
<li>value(值)</li>
</ul>
<p>对象之间比较是否相等可以用<code>==</code>，也可以用<code>is</code>。</p>
<p><code>is</code> 和 <code>==</code> 都是对对象进行比较判断作用的，但对对象比较判断的内容并不相同。下面来看看具体区别在哪</p>
<ul>
<li><p><strong><code>is</code> 比较的是两个对象的 <code>id</code> 值是否相等，也就是比较两个对象是否为同一个实例对象，是否指向同一个内存地址</strong></p>
</li>
<li><p><strong><code>==</code> 比较的是两个对象的内容是否相等，默认会调用对象的 <code>__eq__()</code> 方法</strong></p>
</li>
</ul>
<p><strong>==比较操作符和is同一性运算符区别</strong></p>
<p>==是python标准操作符中的比较操作符，用来比较判断两个对象的value(值)是否相等。</p>
<p>示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># python3.5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a </span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b == a</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a[:]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b <span class="keyword">is</span> a</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b == a</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="python中一些看似简单的运算需求"><a href="#python中一些看似简单的运算需求" class="headerlink" title="python中一些看似简单的运算需求"></a>python中一些看似简单的运算需求</h2><h3 id="求list的绝对值"><a href="#求list的绝对值" class="headerlink" title="求list的绝对值"></a>求list的绝对值</h3><p>求<code>list l = [-1,2,3,-4,-5,-6]</code>的绝对值的三种方法</p>
<ol>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list_abs &#x3D; list(map(abs, l))</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">list_abs &#x3D; [abs(i) for i in l]</span><br></pre></td></tr></table></figure>
</li>
<li><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line">list_abs &#x3D; list(np.abs(l))</span><br></pre></td></tr></table></figure>


</li>
</ol>
<h3 id="判断某个key值是否在dict中"><a href="#判断某个key值是否在dict中" class="headerlink" title="判断某个key值是否在dict中"></a>判断某个key值是否在dict中</h3><p>有两种方法</p>
<p>一：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#生成一个字典</span><br><span class="line">d &#x3D; &#123;&#39;name&#39;:&#123;&#125;,&#39;age&#39;:&#123;&#125;,&#39;sex&#39;:&#123;&#125;&#125;</span><br><span class="line">#打印返回值</span><br><span class="line">print d.has_key(&#39;name&#39;)</span><br><span class="line">#结果返回True</span><br></pre></td></tr></table></figure>
<p>二：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#生成一个字典</span><br><span class="line">d &#x3D; &#123;&#39;name&#39;:&#123;&#125;,&#39;age&#39;:&#123;&#125;,&#39;sex&#39;:&#123;&#125;&#125;</span><br><span class="line">#打印返回值，其中d.keys()是列出字典所有的key</span><br><span class="line">print name in d.keys()</span><br><span class="line">#结果返回True</span><br></pre></td></tr></table></figure>
<p>第二中<code>in</code>的方法更好更快，但是还是不知道他的速度是不是O(1)???????</p>
]]></content>
      <categories>
        <category>-周边辅助</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>数学公式</title>
    <url>/2020/02/19/%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F/</url>
    <content><![CDATA[<h1 id="最优法"><a href="#最优法" class="headerlink" title="最优法"></a>最优法</h1><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>牛顿法迭代法解非线性方程，是把非线性方程$f(x)=0$线性化的一种近似方法。把$f(x)$在点$x_0$的某领域内展开成泰勒级数$f(x)=f(x_0)+f^{‘}(x_0)(x-x_0)+ \frac{f^{‘’}(x_0)(x-x_0)^2}{2!}+…+ \frac{f^{n}(x_0)(x-x_0)^n}{n!}+R_n(x)$，取其线性部分，即展开式的前两项$f(x_0)+f^{‘}(x_0)(x-x_0)$，令其等于0，以此作为非线性方程$f(x)=0$的近似方程，若$f^{‘}(x_0) \neq 0$，则解为$x_1=x_0- \frac {f(x_0)} {f^{‘}(x_0)}$，这样我们得到牛顿迭代法的一个迭代关系式</p>
<p>Python代码实例展示求解方程$(x-3)^3=0$的根</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (x<span class="number">-3</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fd</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x<span class="number">-3</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newtonMethod</span><span class="params">(n, assum)</span>:</span></span><br><span class="line">    time = n</span><br><span class="line">    x = assum</span><br><span class="line">    Next = <span class="number">0</span></span><br><span class="line">    A = f(x)</span><br><span class="line">    B = fd(x)</span><br><span class="line">    print(<span class="string">'A = '</span> + str(A) + <span class="string">',B = '</span> + str(B) + <span class="string">',time = '</span> + str(time))</span><br><span class="line">    <span class="keyword">if</span> f(x) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> time, x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Next = x - A/B</span><br><span class="line">        print(<span class="string">'Next x = '</span> + str(Next))</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 1.</span></span><br><span class="line">    <span class="comment">## 这里设置的迭代条件不是当前值和最优值之间的距离，而是相比于上一个值的变化要小于某个值</span></span><br><span class="line">    <span class="comment">## 但我们要求的就是最优值，所以我们并不知道最优值的具体值，不能的到他们的距离</span></span><br><span class="line">    <span class="comment">## 所以要通过每次A的变化，来估计最优值</span></span><br><span class="line">    <span class="keyword">if</span> A - f(Next) &lt; <span class="number">1e-6</span>: </span><br><span class="line">        print(<span class="string">"Meet f(x) = 0, x = "</span> + str(Next))</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 2.</span></span><br><span class="line">    <span class="comment"># ## 所以这里其实是不是也可以通过x的变化，来估计最优值？</span></span><br><span class="line">    <span class="comment"># if A/B &lt; 1e-4:</span></span><br><span class="line">    <span class="comment">#     print("Meet f(x) = 0, x = " + str(Next))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## 3.</span></span><br><span class="line">    <span class="comment"># ## 这里当然也能用 if time &gt; m 来设置跳出循环的条件，但是相对 A - f(Next) 得到的值会更不精确一些</span></span><br><span class="line">    <span class="comment"># if time &gt; 10:</span></span><br><span class="line">    <span class="comment">#     print("Meet f(x) = 0, x = " + str(Next))</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> newtonMethod(n+<span class="number">1</span>, Next)</span><br><span class="line"></span><br><span class="line">newtonMethod(<span class="number">0</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<h2 id="其它迭代算法"><a href="#其它迭代算法" class="headerlink" title="其它迭代算法"></a>其它迭代算法</h2><h3 id="欧几里得算法"><a href="#欧几里得算法" class="headerlink" title="欧几里得算法"></a>欧几里得算法</h3><p>最大公约数：Greatest Common Divisor(GCD)</p>
<h4 id="算法简介"><a href="#算法简介" class="headerlink" title="算法简介"></a>算法简介</h4><p>欧几里德算法是用来求两个正整数最大公约数的算法。是由古希腊数学家欧几里德在其著作《The Elements》中最早描述了这种算法,所以被命名为欧几里德算法。<br>扩展欧几里德算法可用于RSA加密等领域。</p>
<p>假如需要求 1997 和 615 两个正整数的最大公约数,用欧几里德算法，是这样进行的：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1997 &#x2F; 615 &#x3D; 3 (余 152)</span><br><span class="line">615 &#x2F; 152 &#x3D; 4(余7)</span><br><span class="line">152 &#x2F; 7 &#x3D; 21(余5)</span><br><span class="line">7 &#x2F; 5 &#x3D; 1 (余2)</span><br><span class="line">5 &#x2F; 2 &#x3D; 2 (余1)</span><br><span class="line">2 &#x2F; 1 &#x3D; 2 (余0)</span><br></pre></td></tr></table></figure>
<p>至此，最大公约数为1<br>以除数和余数反复做除法运算，当余数为 0 时，取当前算式除数为最大公约数，所以就得出了 1997 和 615 的最大公约数 1。</p>
<h4 id="计算证明"><a href="#计算证明" class="headerlink" title="计算证明"></a>计算证明</h4><p>其计算原理依赖于下面的定理：</p>
<p>定理：两个整数的最大公约数等于其中较小的那个数和两数相除余数的最大公约数。最大公约数（Greatest Common Divisor）缩写为GCD。<br>$gcd(a,b) = gcd(b,a \mod b)$ (不妨设$a&gt;b$ 且$r=a \mod b$ ,$r$不为$0$)</p>
<h5 id="证法一"><a href="#证法一" class="headerlink" title="证法一"></a>证法一</h5><ul>
<li><p>正</p>
<ul>
<li>$a$可以表示成$a = kb + r$（$a$，$b$，$k$，$r$皆为正整数，且$r&lt;b$），则$r = a \mod b$</li>
<li>假设$d$是$a$,$b$的一个公约数，记作$d|a$,$d|b$，即$a$和$b$都可以被$d$整除。</li>
<li>而$r = a - kb$，两边同时除以$d$，$r/d=a/d-kb/d=m$，由等式右边可知$m$为整数，因此$d|r$</li>
<li>因此$d$也是$b$,$a \mod b$的公约数</li>
</ul>
</li>
<li><p>反</p>
<ul>
<li>假设$d$是$b$,$a \mod b$的公约数, 则$d|b$,$d|(a-k*b)$,$k$是一个整数。</li>
<li>进而$d|a$，因此$d$也是$a$,$b$的公约数</li>
<li>因此$(a,b)$和$(b,a \mod b)$的公约数是一样的，其最大公约数也必然相等</li>
</ul>
</li>
<li><p>得证</p>
</li>
</ul>
<h5 id="证法二"><a href="#证法二" class="headerlink" title="证法二"></a>证法二</h5><p>假设c = gcd(a,b),则存在m,n，使a = mc, b = nc;<br>令r = a mod b，即存在k，使r = a-kb = mc - knc = (m-kn)c;<br>故gcd(b,a mod b) = gcd(b,r) = gcd(nc,(m-kn)c) = gcd(n,m-kn)*c;<br>假设d = gcd(n,m-kn), 则存在x,y, 使n = xd, m-kn = yd; 故m = yd+kn = yd+kxd = (y+kx)d;<br>故有a = mc = (y+kx)dc, b = nc = xdc; 可得 gcd(a,b) = gcd((y+kx)dc,xdc) = dc;<br>由于gcd(a,b) = c, 故d = 1;<br>即gcd(n,m-kn) = 1, 故可得gcd(b,a mod b) = c;<br>故得证gcd(a,b) = gcd(b,a mod b).<br>注意:两种方法是有区别的。</p>
<h4 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h4><p>自己写的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    c = a%b</span><br><span class="line">    <span class="keyword">return</span> print(b) <span class="keyword">if</span> c==<span class="number">0</span> <span class="keyword">else</span> gcd(b, c)</span><br><span class="line"></span><br><span class="line">gcd(<span class="number">1997</span>, <span class="number">615</span>)</span><br></pre></td></tr></table></figure>

<p>百度百科上的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gcd</span><span class="params">(a, b)</span>:</span></span><br><span class="line">    <span class="keyword">while</span> a != <span class="number">0</span>:</span><br><span class="line">        a, b = b % a, a</span><br><span class="line">    <span class="keyword">return</span> print(b)</span><br><span class="line"></span><br><span class="line">gcd(<span class="number">1997</span>, <span class="number">615</span>)</span><br></pre></td></tr></table></figure>

<h4 id="扩展欧几里得算法"><a href="#扩展欧几里得算法" class="headerlink" title="扩展欧几里得算法"></a>扩展欧几里得算法</h4><p>扩展欧几里得算法（英语：Extended Euclidean algorithm）是欧几里得算法（又叫辗转相除法）的扩展。已知整数$a$、$b$，扩展欧几里得算法可以在求得$a$、$b$的最大公约数的同时，能找到整数$x$、$y$（其中一个很可能是负数），使它们满足贝祖等式</p>
<p>$ax+by=gcd(a,b)$</p>
<p>如果a是负数，可以把问题转化成$|a|(-x)+by=gcd(|a|,b)$，然后令$x^{‘}=(-x)$。</p>
<p>扩展欧几里得算法可以用来计算模反元素(也叫模逆元)，而模反元素在RSA加密算法中有举足轻重的地位。</p>
<h5 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ext_euclid</span><span class="params">(a, b)</span>:</span>     </span><br><span class="line">    <span class="keyword">if</span> b == <span class="number">0</span>:         </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>, <span class="number">0</span>, a     </span><br><span class="line">    <span class="keyword">else</span>:         </span><br><span class="line">        x, y, q = ext_euclid(b, a % b) </span><br><span class="line">        <span class="comment"># q = gcd(a, b) = gcd(b, a%b)   </span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 这一行怎么求的，有时间可以论证一下      </span></span><br><span class="line">        x, y = y, (x - (a // b) * y)     </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, y, q</span><br></pre></td></tr></table></figure>

<h3 id="斐波那契数列"><a href="#斐波那契数列" class="headerlink" title="斐波那契数列"></a>斐波那契数列</h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h4><p>斐波那契数列（Fibonacci sequence），又称黄金分割数列、因数学家列昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：1、1、2、3、5、8、13、21、34、……在数学上，斐波那契数列以如下被以递推的方法定义：F(1)=1，F(2)=1, F(n)=F(n - 1)+F(n - 2)（n ≥ 3，n ∈ N*）在现代物理、准晶体结构、化学等领域，斐波纳契数列都有直接的应用，为此，美国数学会从 1963 年起出版了以《斐波纳契数列季刊》为名的一份数学杂志，用于专门刊载这方面的研究成果。</p>
<h4 id="代码-2"><a href="#代码-2" class="headerlink" title="代码"></a>代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#################### 输出第n个斐波那契数 ####################</span></span><br><span class="line"><span class="meta">@fn_timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci1</span><span class="params">(n)</span>:</span></span><br><span class="line">    <span class="comment">## 这种方法，每次都要嵌套循环，不能记录上一次的结果之间用，所以时间复杂度非常高！！</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(n)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> fib(n<span class="number">-1</span>) + fib(n<span class="number">-2</span>)</span><br><span class="line">    <span class="keyword">return</span> fib(n)</span><br><span class="line"></span><br><span class="line"><span class="meta">@fn_timer</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci2</span><span class="params">(n)</span>:</span></span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    i = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= n:</span><br><span class="line">        l.append(l[i<span class="number">-1</span>]+l[i<span class="number">-2</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> l[i<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(fibonacci1(<span class="number">20</span>))</span><br><span class="line">print(fibonacci2(<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#################### 输出前n个斐波那契数 ####################</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fibonacci</span><span class="params">(n)</span>:</span></span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    i = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; n:</span><br><span class="line">        l.append(l[i<span class="number">-1</span>]+l[i<span class="number">-2</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line">print(fibonacci(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

<p>可以尝试用代码画出斐波那契函数的曲线 斐波那契弧线<br><img src="/public/images/image/fibonacci.jpg" alt="fibonacci"></p>
]]></content>
      <categories>
        <category>数学公式</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>牛顿法</tag>
        <tag>最优法</tag>
        <tag>迭代算法</tag>
      </tags>
  </entry>
  <entry>
    <title>MacBook</title>
    <url>/2020/02/19/MacBook/</url>
    <content><![CDATA[<h4 id="Xcode"><a href="#Xcode" class="headerlink" title="Xcode"></a>Xcode</h4><p>Xcode更新后，可能VScode不能用，显示<code>java</code>相关问题，这时可以选择</p>
<ol>
<li>打开<code>launch.json</code>文件</li>
<li>然后<code>add configuration</code></li>
<li>选择<code>python</code></li>
<li>删除<code>json</code>文件中关于<code>java</code>的语言设置，只保留<code>python</code>的</li>
</ol>
<p>这样就OK了</p>
]]></content>
      <categories>
        <category>周边辅助</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>算法刷题总结</title>
    <url>/2020/02/19/%E5%88%B7%E9%A2%98%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<ol>
<li>如果有搜索类循环，可以用dict来储存，这样搜索时间为O(1)，如果不行，可以考虑二叉树搜索，二叉树搜索时间小于O(n)<ol>
<li>如找重复的数值</li>
<li>小于/大于某个值的数值等</li>
</ol>
</li>
</ol>
]]></content>
      <categories>
        <category>数学公式</category>
      </categories>
      <tags>
        <tag>数学</tag>
        <tag>公式</tag>
        <tag>牛顿法</tag>
        <tag>最优法</tag>
        <tag>迭代算法</tag>
      </tags>
  </entry>
  <entry>
    <title>智能客服项目</title>
    <url>/2019/12/08/%E6%99%BA%E8%83%BD%E5%AE%A2%E6%9C%8D%E9%A1%B9%E7%9B%AE/</url>
    <content><![CDATA[<h2 id="智能客服项目"><a href="#智能客服项目" class="headerlink" title="智能客服项目"></a>智能客服项目</h2><p>智能客服项目主要有三个模块FAQ、任务型对话和闲聊，用户输入一个句子后是进入FAQ、任务型对话还是闲聊，是由一个大的顶层分类器来决定。</p>
<p>FAQ和闲聊都是单轮对话，所以用户后面的输入都会重新进入到顶层分类器再次进行分类。</p>
<p>任务型对话则是多轮的，如果一个句子被顶层分类器划分到任务型对话中，那么在这个任务结束以前，用户后面的输入都会不经过顶层分类器而直接进入到任务型对话中。<br>判断任务型对话是否结束有两种方式：</p>
<ul>
<li>一是这个任务已经完成</li>
<li>二是虽然任务还未完成，但是模型判断需要跳出任务，此时直接结束任务返回通过顶层分类器进行模块判别的进程。</li>
</ul>
<p>在所有的任务中，我们都需要先为这个任务建立一个baseline，通常为相关任务常用的相对简单的模型。</p>
<a id="more"></a>


<hr>
<h3 id="NLP"><a href="#NLP" class="headerlink" title="NLP"></a>NLP</h3><h4 id="词嵌入"><a href="#词嵌入" class="headerlink" title="词嵌入"></a>词嵌入</h4><p>在NLP中，词嵌入是一个非常重要和基本的点。不同于图片领域，可以直接将图片色彩值、像素值等直接拿来进行学习，在NLP领域中，字、词是需要转换成数值，然后才能用一些算法来进行学习，而这个将词/字转换成数值的方法，就叫做词嵌入 word embedding。</p>
<p>最简单的词嵌入就是ont-hot方法了，但是在都拥有大数据集的情况下，one-hot不仅使得词维度过大，而且并不能够表示文档、字词之间的表意关系。</p>
<p>词嵌入如此重要，我们在词嵌入方向也做了很多研究，并且现在已有很多种效果很好的词嵌入的方法。</p>
<ol>
<li><p>最开始自己在为法国农业银行做智能客服时使用的tf-idf就是一个比较经典的词嵌入方法，但是它仍然有维度过大和不能表示词表意之间的关系</p>
</li>
<li><p>我们这里主要要讲的就是我们经常用到的word2vec词向量方法。word2vec词向量主要有两种计算方法，一种是skip-gram，另一种就是CBOW</p>
<ol>
<li><p>skip-gram</p>
<p>skip-gram是通过在已知当前词的情况下，预测前n个词和后n个词</p>
<p>当前词作为中心词时，因此要预测前n个词和后n个词，所以是从2*n个词的地方来进行参数优化，即进行学习，所以得到的词向量更准确一些，因此对于一些低频词较多的情况下，可以酌情使用这种方法，但是计算量会更大一些</p>
</li>
<li><p>CBOW</p>
<p>CBOW是在已知前面n个词和后面n个词的情况下，对当前词进行预测</p>
<p>通过前n个词和后n个词来预测当前词，学习效率快，但是相对而言，优化的次数要少，因此得到的词向量会相对没有那么精确，但是因为计算量相对较小，而且这种方法得到的词向量已经满足常规使用需求了，所以这种方法的词向量被使用的更频发</p>
</li>
<li><p>两者都是通过对目标函数的优化，来优化隐藏层的参数W，而这个W就是我们的词向量</p>
</li>
<li><p>这两种计算方法其实计算量都特别大，所以有两种优化方法，一种是负采样，一种是softmax方法，我并没有细看</p>
</li>
</ol>
</li>
</ol>
<p>其实并没有规避one-hot维度大、没有词表意之间练习的问题，我们只是把这个问题提前解决了之后，再拿来使用，所以这应该也算是迁移学习吧</p>
<hr>
<h3 id="顶层分类器"><a href="#顶层分类器" class="headerlink" title="顶层分类器"></a>顶层分类器</h3><hr>
<h3 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h3><p>FAQ经历的漫长的演变。</p>
<ol>
<li><p>第一版</p>
<ul>
<li>最开始做的FAQ就是tf-idf加上余弦相似度</li>
<li>缺点：<ul>
<li>词包模型，没有句子中词汇出现的顺序信息</li>
<li>有的词汇出现频繁，但并不是很重要的词汇；相反有的词汇出现频率相对较小，但是确实非常重要的判别词汇，所以tf-idf给予字词的权重有待优化</li>
</ul>
</li>
<li>导致：<ul>
<li>有些句子可能在距离上更接近A句子，其实是相似于B句子，比如句式相似，只有一两个重点字不同</li>
</ul>
</li>
<li>为了优化这个方法对词汇权重计算的偏差问题（重要词汇的权重不够大），又找出了所有的重要词汇，并且计算每个句子中重要词汇的杰卡德系数</li>
<li>最后结合余弦相似度和杰卡德系数来判断两个句子的相似性</li>
<li>此时因为数据量小，句子相对简单，top1值并不高60%，但是top3值可以达到95%，剩下的95%是因为数据库中不存在相关语句</li>
</ul>
</li>
<li><p>第二版</p>
<ul>
<li>后来用的是深度学习加上余弦相似</li>
<li>深度学习主要是用word2vec和LSTM得到句子的embedding</li>
<li>有目标句子Anchor，同类（答案相同的）问题中抽取一个正例Positive，非同类问题中抽取一个负例Negative</li>
<li>d(a,p) &lt;= d(a,n)，所以d(a,p)-d(a,n) &lt;= 0，所以d(a,p)-d(a,n)+margin &lt;= 0</li>
<li>对于新输入的句子，训练好的模型唯一的用途就是embedding，用模型embedding出来的数值计算余弦相似度进行排序</li>
<li>缺点：<ul>
<li>因为负采样的原因不稳定，每一对&lt;a,p,n&gt;中，正采样可以用的样本过少，负采样样本较多，且大概率负样本很容易满足d(a,p)&lt;=d(a,n)的限制，因为一般情况下，负样本都比正样本离目标样本远的多</li>
</ul>
</li>
</ul>
</li>
<li><p>第三版</p>
<ul>
<li><p>loss中使用am-softmax。以前的<strong>loss</strong>是简单的取交叉熵，现在是</p>
<ul>
<li>对两个vector都做了归一化，即求内积变为求余弦相似度</li>
<li>增加类间距离，缩小类内距离：每个样本所属类的距离，必须<strong>远</strong>小于它跟其它类的距离。这个<strong>远</strong>可以是<strong>到其它类距离的1/2</strong>，可以是<strong>到其它类的距离减去某个值: dist()-m</strong>，等。这里用的是第二种，即余弦相似度减去某个值m</li>
<li>为了让概率P更均匀的分布在0-1之间，对余弦相似度进行了s倍的缩放</li>
</ul>
</li>
<li><p>用了triplet loss中online采样的变体：</p>
<ul>
<li>假设包含B个图片的banch有P个不同的人组成，每人有K个图片，即B=PK。两种在线采样策略分别是：<ul>
<li>batch all: 取目标样本下所有的正样本和所有的负样本（PK<em>(K-1)</em>(P-1)K个triplet，PK为所有样本数，K-1为正样本树，(P-1)K为负样本树）</li>
<li>batch hard: 取目标样本下距离最小的负样本和距离最大的正样本（PK个triplet）</li>
</ul>
</li>
<li>这两个采样策略，一个太复杂，计算量太大，一个太极端，不稳定。所以我们取了中和的方法，即<strong>取目标样本下每个类别中距离最小的m个负样本和距离最大的m个正样本，然后一一对应来计算（PK * m * (P-1)m个triplet with m &lt;&lt; K）</strong></li>
<li>offline采样：这个方法不够高效，因为最初要把所有的训练数据喂给神经网络，而且每过1个或几个epoch，可能还要重新对negative examples进行分类</li>
</ul>
</li>
<li><p>triplet loss中的分类是以问答相似/相同为标准的小类大约3500个，还有一个大类，即我们把FAQ问题整体分了个类，约840个类，FAQ整体数据大概10W。我们在得到一个新的输入句子时，会从840个类中分别抽取3个query，来计算相似度，我们取平均值最高的5个类，然后在只取这5个类中所有的问题来和新输入的句子进行相似度匹配，来得到需要输出的答案。</p>
<ul>
<li>第一次类的寻找，是为了节约时间，不让输入的query和10W个句子暴力匹配</li>
<li>第二次在5个类中找到需要的结果，是对具体回答的答案的寻找</li>
<li>是否直接输出找到的最匹配的答案，还有一个阈值来决定</li>
<li>如果我们对第一选项不确定，即没有超过相关阈值，那么我们可以推荐前三个相似度最高的问题，让用户选择</li>
<li>用户的选择，能够为我们带来新的数据，帮助优化模型</li>
<li>如果所有问题都没有超过推荐阈值，那么需要用户重新输入</li>
</ul>
</li>
<li><p>第三版使用了triplet loss改进后，top1值提高到了87%，top3值提高到了94%</p>
</li>
</ul>
</li>
</ol>
<h4 id="相似度计算方法"><a href="#相似度计算方法" class="headerlink" title="相似度计算方法"></a>相似度计算方法</h4><h5 id="levenshtein"><a href="#levenshtein" class="headerlink" title="levenshtein"></a>levenshtein</h5><p>当i=0 or j=0,res[i][j]=0<br>res[i][j]=min{res[i-1][j]+1,res[i][j-1]+1,res[i-1][j-1]+flag}, flag=0 if A[i]=B[j], flag=1 if A[i]!=B[j]</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">levenshtein</span><span class="params">(seq1, seq2)</span>:</span></span><br><span class="line">    size_x = len(seq1) + <span class="number">1</span></span><br><span class="line">    size_y = len(seq2) + <span class="number">1</span></span><br><span class="line">    matrix = np.zeros ((size_x, size_y))</span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(size_x):</span><br><span class="line">        matrix [x, <span class="number">0</span>] = x</span><br><span class="line">    <span class="keyword">for</span> y <span class="keyword">in</span> xrange(size_y):</span><br><span class="line">        matrix [<span class="number">0</span>, y] = y</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> x <span class="keyword">in</span> xrange(<span class="number">1</span>, size_x):</span><br><span class="line">        <span class="keyword">for</span> y <span class="keyword">in</span> xrange(<span class="number">1</span>, size_y):</span><br><span class="line">            <span class="keyword">if</span> seq1[x<span class="number">-1</span>] == seq2[y<span class="number">-1</span>]:</span><br><span class="line">                matrix [x,y] = min(</span><br><span class="line">                    matrix[x<span class="number">-1</span>, y] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x<span class="number">-1</span>, y<span class="number">-1</span>],</span><br><span class="line">                    matrix[x, y<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                matrix [x,y] = min(</span><br><span class="line">                    matrix[x<span class="number">-1</span>,y] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x<span class="number">-1</span>,y<span class="number">-1</span>] + <span class="number">1</span>,</span><br><span class="line">                    matrix[x,y<span class="number">-1</span>] + <span class="number">1</span></span><br><span class="line">                )</span><br><span class="line">    <span class="keyword">print</span> (matrix)</span><br><span class="line">    <span class="keyword">return</span> (matrix[size_x - <span class="number">1</span>, size_y - <span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<h5 id="最长公共子串"><a href="#最长公共子串" class="headerlink" title="最长公共子串"></a>最长公共子串</h5><p>当i=0 or j=0,res[i][j]=0<br>当A[i]=B[j],res[i][j]=res[i-1][j-1]+1<br>当A[i]!=B[j],res[i][j]=0</p>
<h5 id="最长公共子序列"><a href="#最长公共子序列" class="headerlink" title="最长公共子序列"></a>最长公共子序列</h5><p>当i=0 or j=0,res[i][j]=0<br>当A[i]=B[j],res[i][j]=res[i-1][j-1]+1<br>当A[i]!=B[j],res[i][j]=max{res[i-1][j],res[i][j-1]}</p>
<hr>
<h3 id="任务型对话"><a href="#任务型对话" class="headerlink" title="任务型对话"></a>任务型对话</h3><p>任务型对话主要有时效运费、查单、转寄退回等几个场景，这里主要用时效运费的场景来说明项目过程。</p>
<p>在时效运费场景中，NLU主要有两个模块，意图识别和NER。</p>
<h4 id="意图识别"><a href="#意图识别" class="headerlink" title="意图识别"></a>意图识别</h4><p>意图识别总共有8类，总数据量是1万5，留了1500个数据做测试集，3000个数据做验证集。</p>
<p>最开始使用的是stacking，后来随着数据量的增加，改用深度学习。<br>深度学习测试了cnn+rnn和cnn，发现cnn的效果比cnn+rnn好，应该是：</p>
<ul>
<li><p>数据本身都是比较短的句子，且序列性不强，所以rnn的优势没有体现出来</p>
</li>
<li><p>只有cnn更好的保存了提取的特征，优化了分类效果</p>
</li>
</ul>
<h5 id="如何优化分类问题"><a href="#如何优化分类问题" class="headerlink" title="如何优化分类问题"></a>如何优化分类问题</h5><h6 id="类别F1值分析"><a href="#类别F1值分析" class="headerlink" title="类别F1值分析"></a>类别F1值分析</h6><p>分析整体F1值，看主要影响效果的是哪一个/哪几个类，然后针对性分析，提高优化效果。<br>针对F1值过低的类别，仔细分析错误分类的数据，找出原因</p>
<h6 id="学习曲线"><a href="#学习曲线" class="headerlink" title="学习曲线"></a>学习曲线</h6><p>根据学习曲线，可以找出当前模型训练出最佳结果所需要的最少的数据数，可以避免资源的浪费，也可以排除结果不佳可能是数据集不够的困惑。</p>
<h5 id="说一下stacking以及它在这个项目中的应用"><a href="#说一下stacking以及它在这个项目中的应用" class="headerlink" title="说一下stacking以及它在这个项目中的应用"></a>说一下stacking以及它在这个项目中的应用</h5><p>stacking中我们用了两层分类器，第一层有三个基分类器RF，NB，LR，第二层分类器是线性核函数的SVM，folder=3，</p>
<ol>
<li><p>先用tf-idf提取特征</p>
</li>
<li><p>第二层的输入为第一层的输出，不包括原始数据</p>
</li>
<li><p>第二层分类器如果是更简单的分类器，效果更好，比如我们用的是线性核函数SVM。</p>
<ul>
<li><p>可能因为上一层的几个基分类器多维度提取特征已经比较复杂，所以第二层的分类器过于复杂会造成过拟合。</p>
</li>
<li><p>输出是概率，更符合分类的要求</p>
</li>
</ul>
</li>
<li><p>较小数据集上stacking的表现并不如意，有时甚至比单独的分类器的效果要差</p>
</li>
<li><p>基分类器中如果有效果特别差的，可以将其移除，可能优化最后的结果</p>
</li>
<li><p>stacking的预测需要时间相对较长</p>
</li>
</ol>
<h6 id="逻辑回归"><a href="#逻辑回归" class="headerlink" title="逻辑回归"></a>逻辑回归</h6><p>逻辑回归是一个判别模型<br>通过对激活函数sigmoid的引入，使得线性回归中的$W^T · x$映射到(0,1)之间，使逻辑回归成为一个概率预测问题，参数就是$W^T$<br>为了使逻辑回归公式$P(y|x) = p_1^y·p_0^{1-y}$最大，我们引入最大似然估计</p>
<p>逻辑回归假设数据符合伯努利分布，所以是一个参数模型。<br>通过引入sigmoid函数，将输出映射到[0,1]之间，</p>
<p>逻辑回归本来是二元分类，但是也可以用作多元分类。<br>如果逻辑回归中引入正则化，我们需要对特征进行标准化，在这标准化也可以加快训练。</p>
<p>最大似然(max最大化问题) 可以导出 loss funcition(min Cross Entropy最小化问题)</p>
<p>方法：</p>
<ul>
<li><p>one VS all，选择计算结果最高的那个类。</p>
</li>
<li><p>引入softmax</p>
</li>
</ul>
<p>缺点：样本不均衡，因为1 VS all</p>
<ol>
<li>损失函数</li>
</ol>
<p>欠拟合：增加特性，增加数据<br>过拟合：正则化，dropout，提前停止训练，减少模型复杂度</p>
<p><strong>在统计学中对变量进行线行回归分析，采用最小二乘法进行参数估计时，R平方为回归平方和与总离差平方和的比值，表示总离差平方和中可以由回归平方和解释的比例，这一比例越大知越好，模型越精确，回归效果越显著。R平方介于0~1之间，越接近1，回归拟合效果越好，一道般认为超过0.8的模型拟合优度比较高。</strong><br><img src="/public/images/image/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-R2.png" alt="逻辑回归-R2"></p>
<h6 id="SVM"><a href="#SVM" class="headerlink" title="SVM"></a>SVM</h6><p>今天终于感觉自己终于懂了SVM的一点皮毛。</p>
<ol>
<li><p><strong>SVM：Support Vector Machine支持向量机</strong></p>
<p> SVM一般用于二元分类，但是因为one-vs-all的存在，我们也可以用它来进行多元分类。</p>
<p> SVM的效果一般特别好，特点是计算量比较大，但是他对数据的分布没有要求，它通过最大化到超平面最近点的距离，来进行分类。</p>
</li>
<li><p><strong>Python &gt;&gt; sklearn中的SVM：</strong></p>
<p> SVC: Support Vector Classification 支持向量用于分类<br> SVR: Support Vector Regression 支持向量用于回归<br> LinearSVC: Linear SVC 核函数为<strong>线性核函数</strong>的支持向量</p>
</li>
<li><p><strong>SVM几种常用核函数：</strong></p>
<p><img src="../../public/images/image/SVM%E5%B8%B8%E7%94%A8%E6%A0%B8%E5%87%BD%E6%95%B0.svg" alt="核函数"></p>
</li>
<li><p><strong>核函数的选择</strong>（吴恩达教授说）：</p>
<ul>
<li>如果Feature的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM</li>
<li>如果Feature的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel (“rbf”)</li>
<li>如果Feature的数量比较小，而样本数量很多，需要手工添加一些feature变成第一种情况</li>
</ul>
</li>
<li><p>参考视频：<br><a href="https://www.bilibili.com/video/av75892058/" target="_blank" rel="noopener">SVM核函数–表示这个视频真的很好</a></p>
</li>
<li><p>自己对核函数的总结：</p>
<ul>
<li>核函数最重要的作用就是映射，不同的核函数提供不同的映射方法</li>
<li>核函数和normalization有点相似，都是对点的映射，只是目标和方式不同，normalization是为了让数据分布更均匀，而SVM中的核函数是为了让数据分布更开散、更易分割</li>
<li>核函数的加入，是的SVM的计算量增大，尤其是使用高斯核函数和多项核函数的时候</li>
<li>在文本分类项目中的stacking方法中，我们使用的是LinearSVC</li>
<li>一般情况下，对非线性数据使用默认的高斯核函数会有比较好的效果</li>
<li>自己只是了解皮毛，对于核函数公式的推导，如何知道一个核函数是否是有效核函数等，都还不是太清楚</li>
</ul>
</li>
</ol>
<h5 id="介绍一下CNN以及在这个项目中的应用"><a href="#介绍一下CNN以及在这个项目中的应用" class="headerlink" title="介绍一下CNN以及在这个项目中的应用"></a>介绍一下CNN以及在这个项目中的应用</h5><p>在CNN中，<strong>卷积</strong>的作用是<strong>特征提取</strong>（比如图像中的边缘检测），<strong>池化</strong>的作用是压缩特征数，对<strong>特征进行降维</strong>。</p>
<p>在CNN分类模型中，我们简单的使用了三个卷积核(大小为filter_size*embedding_size)，加入了L2正则化。</p>
<p><strong>如何让CNN也能解决序列问题</strong></p>
<ul>
<li>CNN在卷基层后，还是保留了句子的顺序关系的，但是在经过max pooling对特征降维后，就打乱了这种顺序，所以我们其实可以去掉max pooling，只让CNN做卷积</li>
<li>或者也可以在CNN中加入位置信息</li>
</ul>
<h5 id="碰到的问题"><a href="#碰到的问题" class="headerlink" title="碰到的问题"></a>碰到的问题</h5><ol>
<li><p>样本不均衡</p>
<ul>
<li><p>过采样：不易太过，否则容易过拟合，但是预测效果很差</p>
</li>
<li><p>欠采样：当数据样本足够多时才能使用</p>
<p>除了样本不均衡本身带来的训练问题，在测试集中，因为样本严重不均衡，且需要尽量保证测试集和训练集的分布一样，所以可能存在测试集中某些类只有几个数据，导致结果随机性较大不可靠，这时应该尽量把真实数据留在测试集中</p>
</li>
</ul>
</li>
<li><p>阈值难以确定<br>每一次的预测，有所有类别的概率[0.1, 0.2, 0.3, 0.4, 0.5]，然后取概率最大的那个类别当作预测结果。<br>但是因为这个结果在每个类别中的边界线并不稳定，所以我们尝试对预测概率[0.1, 0.2, 0.3, 0.4, 0.5]的熵的分析来进一步判别。</p>
</li>
<li><p>任务结束前如何判断用户是否更改意图，跳出当前场景</p>
<p> 我们增加了一个类others，包含所有肯定不属于当前场景的对话数据，比如：“我要投诉”，“今天天气适合登山”等等。</p>
<p> 对于需要跳出当前场景的对话，如果我们没有跳出，比不需要跳出场景对话但是却跳出了的结果要更严重，所以我们主要要考虑others类的准确率，其次是F1值和召回率。</p>
<p> 这里为了使得others相对于召回率有更大的准确率，我们增加了这个类的权重（求完各个类的交叉熵后，乘以权重系数，即增加这个类对loss的敏感度）。如果训练时间足够长，是否增加权重最后的结果都是相同的，但是增加权重后，模型会优先保证others这一类的准确率，所以我们可以让训练停在others有较大准确率、且总体结果比较符合我们要求的时候。即牺牲别的类的准确率来优先保证others的准确率。</p>
</li>
<li><p>过拟合</p>
<p> 表现为训练集的F1值不断上升但是验证集的变化不大。原因是数据量很多，但是类别很少，且句子都很简短。</p>
<p> 解决方法：</p>
<ul>
<li>加入L2正则化：加入L2正则化初期效果很好，后面慢慢还是会过拟合</li>
<li>停止训练：在发现训练集不断上升但是验证集结果变化不大时，及时停止训练。</li>
</ul>
</li>
<li><p>在计算损失函数时，不要允许概率直接等于0，而是加上一个极小的正数，这样会减少信息的丢失，整体的概率和与1有一点偏差并不影响</p>
</li>
</ol>
<h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><p>一共有15000个数据，12个类别（连上不属于这12个类别的数据，如“的”、“你们”等，共13个类别），如”出发地”，”出发时间”，”类型”，”物品”，”体积”,”重量”等。</p>
<p>训练集：14000，验证集：，测试集：1000。</p>
<p>baseline用的是LSTM+CRF，后来改为BiLSTM+CRF后，效果有一定提升。optimizer选用的是Adam。</p>
<p>准确率达97.4%，但F1值只有87.4%<br>主要是对时间的获取准确率太低，DUR和TIM分别只有43%和50%</p>
<h4 id="BiLSTM-CRF"><a href="#BiLSTM-CRF" class="headerlink" title="BiLSTM+CRF"></a>BiLSTM+CRF</h4><h5 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h5><ol>
<li>条件：判别式模型；随机场：无向图模型</li>
<li>判别模型P(Y|X)</li>
<li>打破了观测独立假设（朴素贝叶斯假设就是观测独立假设）</li>
<li>全局归一化（MEMM最大熵马尔可夫模型是局部归一化）</li>
<li>一般的深度学习模型只是学习了文本/特征上下文的关系(BiLSTM)，但是CRF的加入可以让它还学到label上下之间的关系，加入了一个相邻序列间转换的概率</li>
<li><img src="/public/images/image/BiLSTM+CRF&#32;loss.png" alt="BiLSTM+CRF的loss求解"></li>
</ol>
<h6 id="CRF和HMM、MEMM相比的优势"><a href="#CRF和HMM、MEMM相比的优势" class="headerlink" title="CRF和HMM、MEMM相比的优势"></a>CRF和HMM、MEMM相比的优势</h6><ul>
<li>CRF没有HMM严格的独立假设条件，所以可以容纳更多的上下文信息（HMM独立假设：输出仅如当前状态相关）</li>
<li>CRF相比MEMM，统计了全局的概率，考虑的数据在全局的分布，并归一化，克服了MEMM模型标记偏置（局部归一化的原因）的缺点</li>
<li>CRF是在给定需要标记的观察序列的条件下，计算整个标记序列的联合概率分布，而不是在给定当前状态条件下，计算下一个状态的概率分布</li>
</ul>
<p><strong>RF和HMM都利用了图的知识，但是CRF利用的是马尔科夫随机场（无向图），而HMM的基础是贝叶斯网络（有向图）。而且CRF也有：概率计算问题、学习问题和预测问题。大致计算方法和HMM类似，只不过不需要EM算法进行学习问题。</strong></p>
<p><strong>HMM和CRF对比： 其根本还是在于基本的理念不同，一个是生成模型，一个是判别模型，这也就导致了求解方式的不同。</strong></p>
<h6 id="自编码-VS-自回归"><a href="#自编码-VS-自回归" class="headerlink" title="自编码 VS 自回归"></a>自编码 VS 自回归</h6><p>自回归模型：ELMo，GPT，GPT2，XLNet<br>自编码模型：Bert</p>
<h6 id="ELMo"><a href="#ELMo" class="headerlink" title="ELMo"></a>ELMo</h6><p>双向自回归语言模型，使用的是从左到右、从右到左的两个LSTM的拼接，特征提取能力不如Transformer。只提供拼接后的词向量</p>
<h6 id="GPT"><a href="#GPT" class="headerlink" title="GPT"></a>GPT</h6><p>单向自回归语言模型，使用的是transformer，但是是left-to-right的单向transformer：transformer decoder，用到的self-attention是masked-multi-self-attention。可以fine-turning</p>
<h6 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h6><p>双向自编码语言模型，其中的mask有加噪音的效果，且有增强上下文信息获取的效果。可以fine-turning<br>优点：</p>
<ul>
<li>双向transformer，所以可以获取上下文信息</li>
<li>用到transformer，可以并行运算</li>
</ul>
<p>缺点：</p>
<ul>
<li>mask在预测中没有，影响模型的泛化能力</li>
<li>缺乏生成能力</li>
<li>没有考虑预测的mask间的相关性</li>
</ul>
<h6 id="XLNet"><a href="#XLNet" class="headerlink" title="XLNet"></a>XLNet</h6><p>优点：</p>
<ul>
<li>用随机采样句子分解的顺序，来解决自回归模型不能联系上下文的问题</li>
<li>解决了预测词间没有依赖的问题</li>
<li>attention mask是的模型可以看到自己那个词attent到它前面分解的词，但是不attent到它看不到的后面分解的词</li>
<li>原自回归模型中永远都是预测下一个词，但是这里因为打乱了句子的分解顺序，所以不知道下一个预测的词是哪个位置的词，或者说预测哪里都一样，所以就变成了bow词包性质的预测，那么建模就没有那么强的序列性。所以我们决定，不用知道下一个词在哪里，我们就预测当前位置的词，那么我们会天然把当前输入encoder到信息中。但是这回带来另一个问题，就是可能要预测的当前词就是输入，那就没有任何意义。但是当我们到另外一个位置时，当前位置的信息需要encoder到里面，所以发生了一个矛盾点，即当前位置信息，某些情况需要encoder到信息中，某些情况又不能encoder到信息中。所以我们就提出了two stream attention，把feature拆成两组，一组encoder当前输入，一组不用，context stream encoder当前输入，用作fine-turning，query stream 不 encoder当前输入，用作预测，并且两个stream参数共享</li>
</ul>
<h6 id="ALBert"><a href="#ALBert" class="headerlink" title="ALBert"></a>ALBert</h6><p>改进：</p>
<ul>
<li>词向量降维</li>
<li>权重共享</li>
<li>更轻，效果更好</li>
<li>NSP 改为了 SOP，即负样本为反转句子</li>
<li>增加了数据量和训练时间</li>
</ul>
<p>缺点：</p>
<ul>
<li>减少了内存占用，但是并没有减少计算量</li>
</ul>
<h6 id="RoBerta"><a href="#RoBerta" class="headerlink" title="RoBerta"></a>RoBerta</h6><p>改进：</p>
<ul>
<li>静态mask变为动态mask:而RoBERTa一开始把预训练的数据复制10份，每一份都随机选择15%的Tokens进行Masking，也就是说，同样的一句话有10种不同的mask方式</li>
<li>with NSP vs without NSP:roberta用的是full-sentence</li>
<li>更大的mini-batch：256 -&gt; 2K</li>
<li>更多的数据和更长的训练时间</li>
</ul>
<h6 id="为什么用Transformer？Transformer相比于RNN和CNN有什么异同？"><a href="#为什么用Transformer？Transformer相比于RNN和CNN有什么异同？" class="headerlink" title="为什么用Transformer？Transformer相比于RNN和CNN有什么异同？"></a>为什么用Transformer？Transformer相比于RNN和CNN有什么异同？</h6><ul>
<li>Transformer和CNN都不存在序列依赖问题，可以并行计算（计算能力不是问题？？），但是Transformer相对于CNN保留了序列关系，他有RNN的优点，却没有RNN计算能力相对差的缺点</li>
<li>CNN中卷基层无法捕捉远距离特征（依赖于卷积层的设定），但是Transformer中，self-attention可以获取前后文所有词与当前词的相关性</li>
<li>RNN和Transformer都可以解决句子中的依赖问题（有点同上面提到的序列问题相似），而是Transformer做的更好，他可以得到句子中每个单词间的相关性，并且一步到位</li>
<li>Transformer的缺点是，当输入文本特别长的时候，他的计算量会飞速增长。</li>
<li>Transformer中的self-attention不存在梯度消失的问题，所以可以更好的看到更远距离的信息</li>
</ul>
<h6 id="Viterbi算法"><a href="#Viterbi算法" class="headerlink" title="Viterbi算法"></a>Viterbi算法</h6><h5 id="Lattice-LSTM"><a href="#Lattice-LSTM" class="headerlink" title="Lattice+LSTM"></a>Lattice+LSTM</h5><p>NER中一般采用BIEO标注的方法，这种方法一般需要把字一个个的分开标注，所以用的是字向量。<br>字向量：</p>
<ul>
<li>优点<ul>
<li>嵌入字向量，不会有OOV</li>
<li>语料多少，不会影响向量整体大小所占内存</li>
</ul>
</li>
<li>缺点<ul>
<li>缺少字词之间的语义关系</li>
</ul>
</li>
</ul>
<p>词向量</p>
<ul>
<li>优点<ul>
<li>完整的嵌入词语信息</li>
</ul>
</li>
<li>缺点<ul>
<li>性能依赖分词精度</li>
<li>开放领域，跨领域分词是一个难题</li>
<li>会有OOV影响模型性能</li>
<li>语料大时，词典容量大，占用内存资源大</li>
</ul>
</li>
</ul>
<p>可见NER中用字向量有很大优势，可以在用词向量的基础上，也加入词向量来嵌入词语的信息吗？</p>
<p>如果使用分词，对词向量进行标注<br>Lattice是除了使用词向量外，还把字向量作为特征也加入进去了，所以整个计算复杂度升高了很多。</p>
<h5 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h5><p>在测试完Lattice LSTM后，发现整体F1值虽然增加了2%，但是速度却下降了特别多（将近5倍），所以最后还是选用的BiLSTM+CRF</p>
<hr>
<h3 id="Bert-1"><a href="#Bert-1" class="headerlink" title="Bert"></a>Bert</h3><h4 id="Bert的两大亮点"><a href="#Bert的两大亮点" class="headerlink" title="Bert的两大亮点"></a>Bert的两大亮点</h4><ul>
<li>MASK机制：随机mask一些token来预测</li>
<li>next sentence predict：判断句子B是否是句子A的下一句话</li>
</ul>
<p>有三个输入：字向量，位置向量，句子向量<br>15%的词mask，这15%中有80%是真的用MASK来代替，10%是原有单词，10%是随机选择的单词<br>优点：</p>
<ul>
<li>只有80%的字为MASK是因为微调时，输入数据中不会有MASK</li>
<li>10%错误是因为这样模型不能100%确定当前字一定是正确的，所以迫使模型更多的依赖上下文</li>
</ul>
<h4 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h4><p><a href="https://www.bilibili.com/video/av56239558/?spm_id_from=333.788.videocard.0" target="_blank" rel="noopener">超厉害的transformer讲解视频，看完就明朗！</a></p>
<ol>
<li><p>transformer最重要的组成部分就是self-attention</p>
</li>
<li><p>attention的计算中，$alpha_{1,i} = q^1 · k^i / \sqrt{d}$，这里除以$\sqrt{d}$是因为作者认为在前面求内积时，如果$q^1$和$k^i$的dimension越大，自然$q^1 · k^i$内积也会<strong>相对</strong>越大，所以要除以$q^1$和$k^i$的dimension $d$来平衡。</p>
</li>
<li><p>self-attention输出的seq中，输出$y_i$是同CNN中一样，可以并行计算的</p>
</li>
<li><p><img src="/public/images/image/self-attention.png" alt="self-attention"></p>
</li>
<li><p>表示位置position的向量$e^i$不是学出来的，而是开始的时候就设置了的，通过$e^i$的数值，可以直接判断出当前输入在句子中是在什么位置</p>
</li>
<li><p>为什么直接把位置信息$e^i$加到$a^i$上，而不是concat呢？这样会不会造成信息混乱呢？</p>
<p>那么我们试着不把$e^i$直接加入到$a^i$，而是在输入处，让位置信息$e$和输入信息$x$直接concat，得到比如$X$，他们乘以把$W_e$和$W_x$ concat在一起的矩阵$W$，我们会发现$W·X = W_e · e + W_x · x = W_e · e + a$，所以最后得到的结果还是相加的，具体情况如下图<br><img src="/public/images/image/self-attention-2.png" alt="positioninfo"></p>
</li>
<li><p>layer norm VS batch norm</p>
</li>
<li><p>为什么layer norm一般会搭配RNN使用？</p>
</li>
<li><p>在transformer的decoder中，中间的multi-head attention的前两个输入来自于encoder的输出，最后一个的输入来自于decoder，前两个是$Q$和$K$，他们是用来计算attention的，后面一个是$V$，计算出来的attention矩阵和$V$进行运算来看attention和$V$的关联度。</p>
</li>
<li><p>multi-head attention里之所以用几个attention是因为不同的attention专注点不同，如果句子里有很多需要关联的关系来学习的化，multi-head attention就会大大提升效果</p>
</li>
</ol>
<h4 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h4><p>常见的两种attentio机制</p>
<h4 id="数据增广"><a href="#数据增广" class="headerlink" title="数据增广"></a>数据增广</h4><ul>
<li>数据增广中，我们不需要用到segment embedding，因为我们都是同一个句子，但是意图分类中我们可以加入intent embedding</li>
<li>随机mask 15%的token，其中10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]</li>
<li>当作一个seq2seq模型来使用，让模型直接输出句子，即mask会被替换成别的单词/词语</li>
<li><ol>
<li>用词向量比字向量好，用字向量会出现错误的词组</li>
<li>词向量需要自己重新训练，中文的Bert是字向量，所以失去了迁移模型的优势</li>
</ol>
</li>
</ul>
<hr>
<h3 id="纠错"><a href="#纠错" class="headerlink" title="纠错"></a>纠错</h3><h4 id="N-gram"><a href="#N-gram" class="headerlink" title="N-gram"></a>N-gram</h4><h4 id="levenshtein-1"><a href="#levenshtein-1" class="headerlink" title="levenshtein"></a>levenshtein</h4><h4 id="最长公共子串-1"><a href="#最长公共子串-1" class="headerlink" title="最长公共子串"></a>最长公共子串</h4><hr>
<h3 id="概述历程"><a href="#概述历程" class="headerlink" title="概述历程"></a>概述历程</h3><h4 id="法国"><a href="#法国" class="headerlink" title="法国"></a>法国</h4><h4 id="顺丰"><a href="#顺丰" class="headerlink" title="顺丰"></a>顺丰</h4><h3 id="几大模块"><a href="#几大模块" class="headerlink" title="几大模块"></a>几大模块</h3><h2 id="待写文章"><a href="#待写文章" class="headerlink" title="待写文章"></a>待写文章</h2><ol>
<li><p>过拟合欠拟合</p>
</li>
<li><p>样本不均衡</p>
</li>
<li><p>batch norm详述及优缺点</p>
</li>
<li><p>batch norm和layer norm的不同</p>
</li>
</ol>
<h3 id="体会"><a href="#体会" class="headerlink" title="体会"></a>体会</h3><p>自己的优势是对整个产品流程的理解，和对产品如何更好面向客户的技术上的着重点的理解</p>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>面试</title>
    <url>/2019/12/08/%E9%9D%A2%E8%AF%95/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>FAQ</title>
    <url>/2019/12/08/FAQ/</url>
    <content><![CDATA[<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了降本增效，我们规划了八大任务型场景，并以FAQ为辅，构建智能在线客服。总场景覆盖率是87.6%，其中FAQ覆盖率为16.7%</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>｜顶层分类器｜ =&gt; ｜FAQ｜+｜任务型场景｜+｜闲聊｜</p>
<h2 id="FAQ-1"><a href="#FAQ-1" class="headerlink" title="FAQ"></a>FAQ</h2><p>出去任务型囊括的几大场景外，还有一些并不足以构建场景，但是用户会有疑问的一些问题，我们通过FAQ进行回答。</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>总共大概9W数据，3W原始数据，5.6W扩充数据，总共800多个类别。扩充数据大部分是通过人工术语扩充，即针对一个标准问题，我们要扩充为不同的句式问法。<br>测试集在扩充数据以前，按分布取的8000样本。</p>
<p>（吴恩达的那个测试集数量决定的方法，在这里不太实用，因为数据不够，所以直接取25%也是不错的选择，5%是照顾一下样本少的类别）</p>
<h3 id="清洗"><a href="#清洗" class="headerlink" title="清洗"></a>清洗</h3><p>连续的数字统一清洗为连续的0，单位也要统一</p>
<p>长短句：</p>
<ul>
<li>有不少长句是因为地址过长，所以我们会提取很长的地址，来做数据增广，让模型更多的学会句式，而忽略长地址的影响</li>
<li>有不少长句是因为用户不满，抱怨的句子，这时需要通过顶层分类器，或者任务型场景里的分类器，分到“抱怨”这个类别，然后转人工</li>
<li>还有一些长句，我们会人工进行压缩，只取有用的部分</li>
</ul>
<p>问题：</p>
<ul>
<li>很多非常长的句子，哪怕不是抱怨，我们也会转人工</li>
<li>几次说一个问题，或者对上一个问题追问，我们在FAQ中目前无法解决。比如“我要寄快递”这句话我们回复完了后，如果用户再说一句“加急”，如果是在FAQ中，我们会不能够回答“加急”，会让用户再次组织语言，或者转人工</li>
</ul>
<h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p>扩充数据大部分是通过人工术语扩充，即针对一个标准问题，我们要扩充为不同的句式问法。</p>
<ul>
<li>句式/问法扩充</li>
<li>句子顺序交换</li>
<li>过采样</li>
</ul>
<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>使用的是jieba分词（jieba分词的逻辑是什么样的？效果为什么这么好？）</p>
<p>经过比较后，我们选用公司内部领域内的词向量，效果要比word2vec要好，OOV会少一些，而且不允许词向量微调效果要更好。词向量的效果也比字向量的好。</p>
<p>什么模型中比较的结果？每个模型都是这个比较结果吗？</p>
<ul>
<li>初步设置了一个baseline，用的孪生网络 LSTM + cosine similarty</li>
<li>在这个模型中比较的结果，然后应用到后面的模型中</li>
</ul>
<p>word2vec向量如何得到的</p>
<ul>
<li>有 skip-gram 和 CBOW 两种</li>
</ul>
<p>训练加速方法的逻辑是什么</p>
<ul>
<li>负采样<ul>
<li>取一个正样本，5个负样本，计算得到正样本和不得到负样本的概率</li>
<li>负样本是通过词频大小来取的</li>
<li>num_word/total_num_word 来得到每个词的词频占比，然后在[0, 1]中随机选择数字</li>
</ul>
</li>
<li>分层 softmax<ul>
<li>哈夫曼树</li>
<li>最小路径权重（词频）</li>
</ul>
</li>
</ul>
<p>用的什么训练</p>
<ul>
<li>gensim.word2vec</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>800多个类是怎么分的？</p>
<ul>
<li>人工分的</li>
<li>很大程度上依赖于答案，所以业务同事直接给的类别和答案</li>
</ul>
<p>数据是怎么样的？</p>
<ul>
<li>ID，标准样本，答案，类别</li>
<li>样本最后都按ID以向量的形式储存，选定匹配的序号后，可以直接从取出向量计算</li>
<li>只需要把用户的输入重新过一遍网络来获取他的向量</li>
</ul>
<p>用了哪些模型</p>
<ul>
<li>SVM</li>
<li>LSTM + attention，[a, a_chapeau, a-a_chapeau, a*a_chapeau] + LSTM + CNN =&gt; “分类” and “fc =&gt; similary”</li>
<li>am-softmax + LSTM</li>
</ul>
<p>分别效果如何</p>
<ul>
<li>am-softmax + LSTM 的效果最好</li>
<li>SVM次之</li>
<li>LSTM + attention 最差</li>
</ul>
<p>为什么</p>
<ul>
<li>SVM本身分类效果就很好，配合if-idf + word2vec的特征提取，泛化效果特别好，但是需要大量内存和时间</li>
<li>为了加强句子的匹配，我们尝试了更复杂的模型</li>
<li>LSTM + attention模型过拟合很严重，而且泛化能力较弱，分类效果没有SVM好，最后匹配结果的准确度也下降很多</li>
<li>最后我们尝试了 word2vec + am-softmax + LSTM，既可以分类，也可以匹配，还可以进行多标签分类，结果也达到了期望的效果<ul>
<li>我们真正要做的是特征提取模型</li>
<li>用分类作为训练方案</li>
<li>最后对模型提取的特征进行对比排序</li>
<li>使用 triplet-loss 增加的类间距，能更好的找到阈，很好的增加了直答的比例，但是对于一些多标签的分类，</li>
</ul>
</li>
</ul>
<p>有没有别的模型的想法？还有哪些可以改进的地方？</p>
<p>重难点是什么？</p>
<ul>
<li>SVM虽然分类效果很好，但是可以用来做匹配的信息过少（只有 word embedding），或者需要重新设计一个模型计算相似度，比较复杂</li>
<li>第二种方法更适合文本的直接匹配，不适合先分类+匹配的做法，且计算量很大</li>
<li>am-softmax 主要是在正负样本的选择上</li>
</ul>
<p>如何进行的错误样本分析？</p>
<p>是否引入了熵</p>
<ul>
<li>二分类器，也可以引入熵</li>
<li>二分类器，还可以是多标签的</li>
</ul>
<p>如何处理多意图问题/引入了多标签吗？sigmoid</p>
<ul>
<li>本身是一个二分类问题</li>
<li>在计算的过程中，就是看当前样本是正样本还是负样本</li>
<li>所以相当于有800多个二分类器？？？</li>
</ul>
<p>整体准确率要达到多少？</p>
<p>是否使用了学习曲线</p>
<p>有没有过拟合</p>
<ul>
<li>有</li>
</ul>
<p>loss如何计算的</p>
<ul>
<li>用d_an,d_ap,y，计算出am-softmax</li>
</ul>
<p>是否用了shuffle</p>
<ul>
<li>用了</li>
</ul>
<p>dropout多少</p>
<ul>
<li>0.4（原文0.5）</li>
</ul>
<p>为什么加入高斯噪音？</p>
<ul>
<li>model文件里Model的self.noise</li>
</ul>
<p>triplet-loss里的距离为什么用欧式距离？</p>
<ul>
<li>计算两个句子的距离用的是欧式距离</li>
<li>triplet文件里TripletLoss的dist_mat=euclidean_dist</li>
</ul>
<p>如何计算相似度的？</p>
<ul>
<li>先用欧式距离计算d_an,d_ap，加入y计算他们的am-softmax</li>
<li>相似度就是用的欧式距离</li>
</ul>
<p>还可以怎样计算相似度</p>
<ul>
<li>内积</li>
<li>余弦</li>
<li>曼哈顿</li>
<li>多层感知器网络（MLP）</li>
</ul>
<p>为什么相似度用的欧式距离？有没有尝试过别的方法？</p>
<ul>
<li>最先尝试的内积和余弦，但是效果没有欧式距离好</li>
<li>后来也尝试过曼哈顿</li>
</ul>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p>覆盖率，成功率，减少客服80%咨询时间，平均降低人工咨询市场50s</p>
<h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p>样本不均衡</p>
<ul>
<li>数据增广</li>
<li>采样<ul>
<li>过采样：复制 + SMOTE过采样 + 加入噪音等生成数据</li>
<li>欠采样：删除数据 + 将多样本类的样本分为N分，只取每份的中心数据作为样本</li>
</ul>
</li>
<li>loss权重调节</li>
<li>组合集成方法，组合多个分类器，每个分类器每个类的数据是随机分成的<strong>少数类数据的量</strong></li>
<li>特征选择（CNN其实就是做了特征选择）</li>
</ul>
<p>准确率如何提升</p>
<ul>
<li>错误样本分析<ul>
<li>数据清洗，特征提取，比如“？”</li>
</ul>
</li>
<li>改善模型</li>
<li>改善loss使用am-softmax</li>
<li>引入熵</li>
<li>多标签分类</li>
</ul>
<p>多意图分类+错分类，如何解决？</p>
<ul>
<li>多标签分类</li>
<li>使用sigmoidcrossentropywithlogits</li>
<li>引入熵</li>
<li>引入am-softmax</li>
</ul>
<p>过拟合</p>
<ul>
<li>dropout</li>
<li>early stopping</li>
<li>shuffle</li>
</ul>
<p>上线后的问题？</p>
<ul>
<li>新的术语，</li>
<li>不存在FAQ里的问题，</li>
<li>闲聊，</li>
<li>抱怨，</li>
<li>多轮（信息分几次发送）</li>
<li>未登录词</li>
</ul>
<h3 id="还有哪些可以改进的点"><a href="#还有哪些可以改进的点" class="headerlink" title="还有哪些可以改进的点"></a>还有哪些可以改进的点</h3><ul>
<li>多轮，这是个大问题（在FAQ也做信息提取，保存信息）</li>
<li>更好的做长句的扩充和数据收集，让模型对长句的泛化能力更好，比如对长句的判别（加attention+cnn）</li>
<li>阈值问题，并不一定会游泳，但是会想尝试使用交叉法来看一下结果</li>
</ul>
<h3 id="自己觉得骄傲的点有哪些"><a href="#自己觉得骄傲的点有哪些" class="headerlink" title="自己觉得骄傲的点有哪些"></a>自己觉得骄傲的点有哪些</h3>]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>面试感触</title>
    <url>/2019/12/08/%E9%9D%A2%E8%AF%95%E6%84%9F%E8%A7%A6/</url>
    <content><![CDATA[<h2 id="恒昌第三轮面试"><a href="#恒昌第三轮面试" class="headerlink" title="恒昌第三轮面试"></a>恒昌第三轮面试</h2><p>今年下午两点和恒昌第三轮面试完。</p>
<p>面试官整体给我的感觉是一种艺术家气息，举例用的是文艺复兴时期绘画的例子，他说那时候有钱人才能用蓝色，特殊人，比如天使才可以用蓝色还说那时候画家并不是创作画家，而是甲方要什么，画家就要一点不差的按照他们的要求来画，和我们现在的算法工程师一样，都是工人，并不是工程师。工程师是可以从现实中抽象建模的人。但这并不是一个人题，因为凡事都是需要经历这样一个过程的，从一点一点的模仿，到熟练，再到创新。我们工程目前还处于一个比较笨的状态，还是工人角色，远没到创新的时候，而且现在人工智能的浪潮也再一次落下，我们已经在尾声，如果以后我出去说自己是做深度学习的，可能就找不到工作了。</p>
<p>听了它说的，我越来越觉得，真的不同的人有不同的看法，他会问我是否学过scala，但是爱奇艺的面试官会说他不会因为我会Hadoop或者不会Hadoop而决定是否录用我。</p>
<p>虽然两者并不矛盾，但是凸显了他们对这个应聘者、甚至这个领域工作者的一个不同的要求和需求。</p>
]]></content>
      <categories>
        <category>La Vie</category>
      </categories>
  </entry>
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <url>/2019/12/07/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
    <content><![CDATA[<h2 id="什么是梯度消失-梯度爆炸"><a href="#什么是梯度消失-梯度爆炸" class="headerlink" title="什么是梯度消失/梯度爆炸"></a>什么是梯度消失/梯度爆炸</h2><p>梯度消失或梯度爆炸是指在神经网络权重更新的反向传播中，$\frac{\partial Loss}{\partial W}$过小或过大的现象</p>
<h2 id="梯度消失-梯度爆炸会产生什么问题"><a href="#梯度消失-梯度爆炸会产生什么问题" class="headerlink" title="梯度消失/梯度爆炸会产生什么问题"></a>梯度消失/梯度爆炸会产生什么问题</h2><p>梯度爆炸会导致权重的更新过大（表现为LOSS出现Nan）<br>梯度消失会导致权重的更新过慢或者几乎不更新</p>
<a id="more"></a>

<h2 id="为什么会有梯度消失和梯度爆炸"><a href="#为什么会有梯度消失和梯度爆炸" class="headerlink" title="为什么会有梯度消失和梯度爆炸"></a>为什么会有梯度消失和梯度爆炸</h2><p>一种是任务梯度消失/梯度爆炸的原因是反向传播过程中的叠成导致的<br>另一种认为反向传播不该背这个锅，反向传播求导只是一种求最优解的方式，最根本的原因还是网络结构的问题（线性和非线性公式连成组合）</p>
<blockquote>
<p>看到<a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">有个文章</a>说我们不应该纠结于梯度消失和梯度爆炸，应该从不同的角度看LSTM，比如选择性、信息不变性等等，觉得这是个很好的想法。当然，他对于RNN和DNN中梯度消失和梯度爆炸的看法也相对详细正确，下面会细说。</p>
</blockquote>
<p>无论是因为网络结构还是反向传播，不可否认的是梯度消失和梯度传播确实来源于对权重求导后很多小于1的数值的连乘，下面我门主要看一下为什么会产生这些小于1的数值。</p>
<h3 id="几个激活函数"><a href="#几个激活函数" class="headerlink" title="几个激活函数"></a>几个激活函数</h3><p>在进入反向传播求导前，先认识一下神经网络中我们经常用到的激活函数</p>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>sigmoid 常见于CNN中，在RNN和LSTM中一般不会用到sigmoid<br>sigmoid的公式为$S(x)=\frac{1}{1+e^{-x}}$<br>其函数和导数图如下</p>
<p><img src="./images/image/sigmoid.png" alt="sigmoid"></p>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>LSTM和RNN中会用到tanh，详情可以看上一篇文章<br>tanh的公式为$tanh(x)=\frac{e^{x}-e^{-x}}{e^{x}+e^{x}}$<br>其函数和导数图如下</p>
<p><img src="/images/image/tanh.png" alt="tanh"></p>
<h4 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h4><p>Relu主要是为了解决梯度消失和梯度爆炸的问题<br>其公式为：<br>$<br>relu(x)=\begin{cases}<br>0, x&lt;0 \\<br>x, x&gt;0<br>\end{cases}<br>$</p>
<p>其函数和导数图如下</p>
<p><img src="/images/image/relu.jpg" alt="relu"></p>
<p>relu的优点：</p>
<ul>
<li>解决了梯度消失/梯度爆炸的问题</li>
<li>计算速度快</li>
<li>加速了网络训练（这点我有点怀疑，是指计算速度快呢还是指效果更好，更容易收敛找到最优解呢？如果是第二点，如何证明的？）<br>relu缺点：</li>
<li>负数部分恒为0，导致一些神经元无法激活（但是有很多relu的改进版已经解决了这个问题，比如leakrelu）</li>
<li>输出不是以0为中心</li>
</ul>
<blockquote>
<p><strong>问题：如果大于0的函数都返回其本身，不是失去了使用非线性函数的意义吗？</strong></p>
</blockquote>
<h3 id="为什么会产生梯度消失和梯度爆炸"><a href="#为什么会产生梯度消失和梯度爆炸" class="headerlink" title="为什么会产生梯度消失和梯度爆炸"></a>为什么会产生梯度消失和梯度爆炸</h3><p>DNN和RNN中的梯度消失和梯度爆炸是不同的</p>
<h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><p>写比较麻烦，下次有时间再看看专门推一遍公式写一遍吧，这里直接贴吧：</p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播w导数解释"></p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADb%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播b导数解释"></p>
<p>没有手动推导，所以曲折的理解过程如下：</p>
<ol>
<li><p>我当时看到两个博客里的公式不一样很困惑，后来发现是自己粗心，因为第一个是对W求导，第二个是对b求导。但是看到很多博客里其实都写的是第二种情况。<br>其实更应该考虑的是第一种情况，因为在神经网络中，bias是为了防止wx等于零起的辅助作用，真正有意义的是讨论W。</p>
</li>
<li><p>虽然真正有意义的是讨论W，但是图二中对b求导的公式其实和真正对w求导的公式差不多，因为他们前面的所有结果都是相同的，唯一不同的是最后一步对b求导结果是1，而对w求导结果是$\delta’(z)$，也就是图一中的$f_1$。所以图二中的公式可以看作是图一公式的详细版。所以在反向传播中，决定导数数值的不仅是激活函数的函数，也有W本身。</p>
</li>
</ol>
<p>如上所示，我们可以看到$loss$对$W_1$求导，其实是$\delta’(z) \cdot w_i$的连乘，在上面我们已经了解到，sigmoid和tanh的导数，都小于零，w一般都会初始化为均值为0，方差为1的数值，我们可以有$\vert \delta’(z) \cdot w_i \vert&lt;1$($\vert \delta’(z) \cdot w_i \vert&lt;\frac{1}{4}$当激活函数为sigmoid时)。因此会有梯度消失。</p>
<p>当然，当w特别大是，我们会有$\vert \delta’(z) \cdot w_i \vert&gt;1$，就会导致梯度爆炸。</p>
<p>从上我们可以看到DNN中的梯度爆炸或者梯度消失，本质是大于1或者小于1的数的<strong>连乘</strong>（注意这里不是幂次方，RNN中才是幂次方，下面会细说）导致的。</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>RNN中很重要的一点是权重共享。一样贴图</p>
<p><img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="rnn反向传播w导数解释"></p>
<p>这里有几个点需要解释：</p>
<ul>
<li><p>为什么DNN中就连乘，但是RNN的导数中有加法呢？<br>DNN中写出的公式是<strong>每一层</strong>$w_i$前的求导公式<br>RNN这里写出的是<strong>所有层数</strong>的$w_x$前的求导公式的和，因为RNN是权重共享，所以对$W_x$的更新就是每一层需要对$W_x$更新的和。<br>因此第三层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%89%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第三层导数">)，第二层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第二层导数">)，第一层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%80%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第一层导数"></p>
</li>
<li><p>公式中$\frac{\partial S_i}{\partial S_i-1}$和DNN中的$\frac{\partial f_i}{\partial f_i-1}$是一样的性质，唯一不同的是这里$\frac{\partial S_i}{\partial S_i-1}=\frac{\partial S_i-1}{\partial S_i-2}=tanh’\cdot W_s$，所以这里其实是一个小于1的数的<strong>幂次方</strong></p>
</li>
</ul>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p>关于DNN和RNN的梯度消失和梯度爆炸，<a href="https://www.zhihu.com/question/34878706" target="_blank" rel="noopener">这里</a>解释的非常详细。</p>
<p>我也自己来总结以下加深印象</p>
<ul>
<li><p>DNN中每层都有不同的参数，所以每一层都各是各的梯度。RNN中因为权重共享，所以梯度为所有层数梯度的总和（上面提到过）</p>
</li>
<li><p><strong>每一层中</strong>：DNN是小于1的梯度的<strong>连乘</strong>，而RNN中因为$\frac{\partial S_i}{\partial S_i-1}=\frac{\partial S_i-1}{\partial S_i-2}$的关系，所以是<strong>幂次方</strong></p>
</li>
<li><p>RNN中的梯度不会真正的消失，它是每一层梯度的和，RNN中的梯度消失主要是指：离的越远，梯度的传递越弱，所以梯度被邻近的梯度主导，导致模型难以学到远距离信息的长期依赖性问题</p>
</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>附加一点LSTM的</p>
<ul>
<li><p>在LSTM中，有很多条路径，cell state这条路径扮演的角色有点像ResNet残差连接，使前面的数据可以不受中间数据的影响传输到后面。</p>
</li>
<li><p>如果去除cell state这条路径，LSTM中该消失还是消失，该爆炸也爆炸，但是有了cell state，<em>梯度=消失梯度+cell state的梯度</em>，我们看到如果cell state的梯度不消失，就可以一定程度上缓解梯度消失，在第一版没有遗忘门的LSTM中，这里的cell state的梯度=1，后来加入了一个遗忘门，但是因为tanh的取值在-1和1之间，并且多数情况趋于1或者-1，所以加入遗忘门后，LSTM也可以一定程度上缓解梯度消失问题。</p>
</li>
</ul>
<h5 id="为什么说是缓解呢"><a href="#为什么说是缓解呢" class="headerlink" title="为什么说是缓解呢"></a>为什么说是缓解呢</h5><ul>
<li><p>我们不能保证每一层都有前面传过来的残差连接，即我们不能保证哪几层cell state的门就一定是0（怎么解释有待再详细思考）</p>
</li>
<li><p>cell state趋于1或者-1，且有不趋于1和-1的时候，这个时候梯度也是小于1的数</p>
</li>
<li><p><strong>那如果每一层的门的数值不一样，是不是也会导致$\frac{\partial S_i}{\partial S_i-1} \neq \frac{\partial S_i-1}{\partial S_i-2}$？所以这里其实也不是幂次方的关系？</strong></p>
</li>
</ul>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><ol>
<li><p>梯度爆炸一般的解决方法是使用gradient clipping(梯度裁剪)，即通过设置梯度的值域范围来限制梯度过大。</p>
</li>
<li><p>L2正则化：$Loss=(y-W^tx)^2+\alpha \cdot \Vert W \Vert^2$，如果权重过大，可以通过Loss的减小来一定程度控制W过大。</p>
</li>
</ol>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>在神经网络中，梯度消失出现的更多一些，也更难解决。</p>
<ol>
<li><p>用tanh取代sigmoid激活函数。因为tanh导数的范围是-1到1，相对sigmoid的导数0到$\frac{1}{4}$，tanh可以相对sigmoid在一定程度缓解梯度消失</p>
</li>
<li><p>使用relu激活函数，当x&gt;0时，relu导数是1。但是觉得他会引起一些其他问题，并且在x&gt;0时为线性函数，所以有点失去了使用激活函数的意义，上面已经提到过了</p>
</li>
<li><p>batchnorm(简称BN)非常复杂，以后专门弄一篇研究一下。在反向传播中$\delta’(z) \cdot w_i$有W的存在，我们通过强行拉回偏离的W的分布到均值为0方差为1的正太分布上，消除了W带来的放大和缩小的问题，来一定程度缓解梯度消失（梯度爆炸）的问题</p>
<p> 有文章说这里是通过BN改变激活函数的输入，使其落在激活函数比较敏感的区域，让输入的微小变化造成Loss较大的变化来缓解梯度消失，这一点还要详细思考一下。</p>
<p> a. 我们用BN对数据进行缩放，强行改变数据输入，是否会对模型有影响？</p>
<p> b. BN中，如果batch size过小，会导致数据不准确，过大会对内存有要求</p>
<p> c. 这里norm的是输入x还是权重w？：两个都不是，而是激活函数的输入$Wx+b$</p>
</li>
<li><p>残差结构：LSTM中cell state的应用，就类似于残差结构，可以一定程度上让在序列前面的信息不受影响的（较完整的）传输到后面的序列中</p>
</li>
<li><p>谨慎选择随机初始化权重，如xavier初始化</p>
</li>
</ol>
<h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><p>非常感谢<a href="https://zhuanlan.zhihu.com/p/76772734" target="_blank" rel="noopener">这篇文章</a>，很全面的解释了很多我的疑惑。</p>
<h3 id="hexo-markdown"><a href="#hexo-markdown" class="headerlink" title="hexo-markdown"></a>hexo-markdown</h3><ol>
<li><p>又一个新发现，在markdown中，如果使用<code>$</code>，公式会和字在同一层展示，如果使用<code>$$</code>，公式会自己单独用一行展示。如下：</p>
<ul>
<li><code>$</code>：$y=f(x)$</li>
<li><code>$$</code>：$$y=f(x)$$</li>
</ul>
</li>
<li><p>尝试了很多插入图片的方法。只有复制github上图片连接最好用，这样在markdown，首页和文章中都可以正确显示图片。如果只是在<code>_config.yml</code>中修改<code>post_asset_folder:true</code>，再使用相对路径来显示图片，那么图片只能在文章首页正确显示，但是不能在文章中正确显示，并且在markdown中的相对路径和在网页上的，不一定是一样的。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
        <tag>DNN</tag>
        <tag>RNN</tag>
        <tag>LSTM</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2019/12/06/LSTM/</url>
    <content><![CDATA[<h2 id="LSTM来源"><a href="#LSTM来源" class="headerlink" title="LSTM来源"></a>LSTM来源</h2><p>LSTM由RNN变化而来，所以在介绍LSTM前，我们先了解一下RNN。</p>
<p>RNN是一类用来处理序列数据的神经网络，当前状态不仅受到当前输入$x_t$的影响，也受前面状态$h_{t-1}$的影响</p>
<p>$h_t=\delta (W^{h_{t-1}} \cdot h_{t-1} + W^{x_t} \cdot x_t)$<br>$y=\delta (W^y \cdot h_t)$</p>
<pre><code>ignore bais here</code></pre><p>且神经元是共享权重W的，即:<br>$W^{h_{t-1}}=W^{h_{t}}$<br>$W^{x_{t-1}}=W^{x_t}$<br>$W^{y-1}=W^y$</p>
<a id="more"></a>

<h2 id="RNN问题"><a href="#RNN问题" class="headerlink" title="RNN问题"></a>RNN问题</h2><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>RNN在实际应用中又一个很大的问题，就是长期依赖性问题，即一个输出$y_t$主要受到其前面几个输入和状态的影响，对较长时间前的输入和状态则非常不敏感，这是由于反向传播中梯度消失引起的。<br>为了解决这一问题，我们引入了LSTM（门机制）。</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>我们在计算RNN反向传播的梯度时会发现，因为RNN是一个共享权重的神经网络，所以当权重W可以特征分解为$W=Q \cdot \Lambda \cdot Q^T$且Q为正交矩阵时，则梯度包含特征值矩阵的幂次方，因此当特征值&gt;1时，则会梯度爆炸，反之则会梯度消失。<br>梯度爆炸比较容易察觉，一般表现为loss变为Nan<br>梯度消失则相对比较难以察觉，一般表现为loss几乎不变，但是loss几乎不变不代表就是产生了梯度消失</p>
<h2 id="LSTM基本描述"><a href="#LSTM基本描述" class="headerlink" title="LSTM基本描述"></a>LSTM基本描述</h2><p>LSTM一定程度上解决了RNN梯度消失和梯度爆炸的问题，得益于三个门的引入</p>
<h3 id="LSTM的三个门"><a href="#LSTM的三个门" class="headerlink" title="LSTM的三个门"></a>LSTM的三个门</h3><p>在LSTM中引入了三个门：输入门，输出门和遗忘门</p>
<p>遗忘门：控制了上一个cell的状态$C_{t-1}$中有多少信息进入到当前状态$C_t$中<br>输入门：控制了有多少信息会被保存到cell状态$C_t$中<br>输出门：控制了当前cell的状态$C_t$(cell state)有多少信息会输出到下一个神经元（即隐藏单元输出hidden state）</p>
<p><img src="/images/image/LSTM3-chain.png" alt="LSTM"></p>
<h3 id="LSTM特点"><a href="#LSTM特点" class="headerlink" title="LSTM特点"></a>LSTM特点</h3><p>LSTM一定程度上将RNN中反向传播中梯度中的叠成关系转换成叠加关系，所以缓解了梯度消失和梯度爆炸的问题。</p>
<h3 id="LSTM和GRU的比较"><a href="#LSTM和GRU的比较" class="headerlink" title="LSTM和GRU的比较"></a>LSTM和GRU的比较</h3><p>LSTM有三个门，GRU只有两个（更新门update和相关门revelance），所以LSTM的运算更为复杂。<br>GRU是在LSTM后出来的，为了缓解LSTM的计算问题，但是在一些实际应用中，GRU的效果并不比LSTM差，所以应用越来越广泛。</p>
<p><img src="/images/image/GRU.png" alt="GRU"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>关于RNN的长期依赖性问题，看了很多文章，一直不能确到底是因</p>
<ul>
<li><p>$h_t=Q^T \cdot \Lambda^t \cdot Q \cdot h_{(0)}$导致的在<strong>前向传播</strong>中$h_{(0)}$对$h_t$的影响因为幂次方的关系而非常小</p>
</li>
<li><p>还是因为梯度消失导致的长期依赖关系.</p>
<p>直到看到Andrew Ng在<a href="https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns" target="_blank" rel="noopener">视频</a>中明确指出：RNN的长期依赖性问题来源于<strong>梯度消失</strong>，并且在<a href="http://pelhans.com/2019/04/24/deepdive_tensorflow-note9/#长期依赖问题" target="_blank" rel="noopener">文章</a>中看到公式$\Delta_{h^{(0)}}$的求解，才理解：RNN的长期依赖性问题来源于反向传播中因共享权重W产生的$\Delta$的幂次方，造成了梯度消失，而使得当前状态对前面序列的依赖过小。</p>
</li>
</ul>
</li>
<li><p>在LSTM中有几种激活函数：</p>
<ul>
<li>sigmoid主要用在门的计算中，因为sigmoid很容易趋近于0或者1，所以使得门可以控制保存（等于1时）或不保存（等于0时）信息</li>
<li>softmax一般用于输出y之前</li>
<li>tanh用于求$\tilde{C}^t$和$h_{t}$的时候</li>
</ul>
</li>
<li><p>hexo中并不支持LaTex格式写公式的问题见<a href="http://stevenshi.me/2017/06/26/hexo-insert-formula/" target="_blank" rel="noopener">此文章</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>RNN</tag>
        <tag>LSTM</tag>
        <tag>长期依赖性</tag>
      </tags>
  </entry>
  <entry>
    <title>Chitchat</title>
    <url>/2019/01/16/chitchat/</url>
    <content><![CDATA[<h2 id="Chitchat"><a href="#Chitchat" class="headerlink" title="Chitchat"></a>Chitchat</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>chitchat持续的时间很长了。每次进步不大，但是至少都在前进。因为持续时间长，每次想法和改动的地方比较多，也是学到了不少东西，想要把这些都记录下来，所以开了这个文档。</p>
<p>chitchat 目前已经自己改进到v1.5了。虽然每一次改动的内容并不大，但是每一次都是一个尝试。</p>
<a id="more"></a>

<h4 id="到目前为止，每个version的改进简介"><a href="#到目前为止，每个version的改进简介" class="headerlink" title="到目前为止，每个version的改进简介"></a>到目前为止，每个version的改进简介</h4><ul>
<li><p>v1.1 是所有以前的版本，但是因为隔的太久，当时又没有记录，所以都不太记得了。</p>
</li>
<li><p>v1.2 是自己重构代码，除此之外，和第一个版本的差别不大。</p>
</li>
<li><p>v1.3 是在<code>seq2seq</code>的基础上加入了<code>bert</code>的<code>get_pooled_output</code>。</p>
</li>
<li><p>v1.4</p>
<ul>
<li>v1.4 这个版本在v1.2版本上改变了loss。因为在同时训练了v1.3版本和v1.2版本后进行比较，发现相同时间内训练出来的结果（因为版本v1.3中加入了<code>bert</code>，所以相同时间内训练出来的<code>epoch</code>数相差很大），v1.2版本的更好。因为考虑到时间成本，所以v1.4版本是在v1.2版本的基础上进行改进的。具体内容可以看v1.4的<code>README.md</code>。</li>
<li>v1.4里也加入了一些“问候”，“再见”等的规则。</li>
</ul>
</li>
<li><p>v1.5 <code>seq2seq</code>中在<code>train</code>时，一般用上一步正确的输出作为下一步的输入，但是这里改成用上一步真实的输出作为下一步的输入。具体内容可见这个文件夹下的《变形seq2seq，即在training时，上一步的输出成为下一步的输入》</p>
</li>
<li><p>又把<code>learning_rate</code>调小了，在训练了100个epoch，但是结果还是很糟糕。</p>
</li>
<li><p>v1.6 尝试在v1.4的基础上把<code>layer</code>从3改成6</p>
</li>
</ul>
<h3 id="闲聊"><a href="#闲聊" class="headerlink" title="闲聊"></a>闲聊</h3><p>闲聊原本数据是120万左右，后来经过数据清洗后，剩下80万左右的数据。<br>数据清洗：</p>
<ol>
<li>删除重复对话</li>
<li>将多轮对话剪切成单轮对话</li>
<li>删除回复过短的回答，或者不需要的回答/问题（如 字符，笑脸符等）</li>
<li>删除过长对话（30字以内）</li>
<li>统一回复（如 哈哈哈哈哈哈 =&gt; 哈哈哈哈哈，如 我不知道啊，哈哈哈 =&gt; 我不知道啊）</li>
</ol>
<h4 id="流程设计"><a href="#流程设计" class="headerlink" title="流程设计"></a>流程设计</h4><ol>
<li>“问候”和“再见”等，是用规则写的</li>
<li>如果没有匹配到“问候”或者“再见”，就会进入到模型生成回复</li>
<li>模型是seq2seq+attention</li>
<li>attention是哪个attention，对结果影响并不大</li>
<li>把数据分为几块，比如5-10个字的句子一起，10-20个字的句子一起，等等，然后让句子从短到长进行输入，一是让训练更稳定，二是能一定程度上加快速度</li>
</ol>
<h4 id="模型问题"><a href="#模型问题" class="headerlink" title="模型问题"></a>模型问题</h4><ol>
<li>无意义回答过多 =&gt; 加入anti-LM</li>
<li>相似回答过多 =&gt; 加入余弦相似到loss中，有一定的效果，但是很难统计，因为可能对某个测试集，效果明显，对某些测试集，效果就不是那么明显</li>
<li>简单问题正确回答的百分比 =&gt; “问候”等对话进行了规则处理</li>
<li>过于简短回答的百分比 =&gt; 数据集上进行了处理</li>
</ol>
<h5 id="anti-LM"><a href="#anti-LM" class="headerlink" title="anti-LM"></a>anti-LM</h5><p>通常情况下，我们的损失函数loss为${\hat T}=\underset{T}{argmax} {logp(T|S)}$。从损失函数我们可以看出，在给定输入$S$的情况下，模型会选择输出概率最大的句子，这回倾向输出出现频率大的句子，通常这些句子为无意义句子。<br>所以我们引入互信息作为新的目标函数，以两个句子的相关性作为度量标准。互信息公式如下：<br>$log \frac{p(S|T)}{p(S)p(T)}$<br>所以表达式可以写成</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\log p(T)-\log p(S)}$</p>
<p>但因为$S$为输入，$\log p(S)$是固定的，所以表达式也为</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\log p(T)}$</p>
<p>对惩罚项$-\log (T)$前加入一个参数，即为</p>
<p>${\hat T}=\underset{T}{argmax} {logp(T|S)-\lambda logp(T)}$</p>
<p>我们发现虽然目标函数改为互信息MMI，但是损失函数最大的改变就是加入了$-log(T)$，而$p(T)$表示一个句子出现的概率，所以这个损失函数其实是加了一个对出现概率大的句子的惩罚，$\lambda$就是那个惩罚系数。</p>
<p>惩罚项$-logp(T)$中的</p>
<p>$p(T)=\prod_{k=1}^{N_{t}}p(t_k|t_1,t_2,…,t_{k-1})$</p>
<p>在句子$T$的生成中，第一个字$t_1$的生成一定程度上决定了后面字$t_k$的生成，所以$t_1$相对于$t_k$而言，对句子$T$的影响更大。又因为如果我们直接让</p>
<p>$p(T)=\prod_{k=1}^{N_{t}}p(t_k|t_1,t_2,…,t_{k-1})$</p>
<p>实际生成的句子可能会更容易有语法问题。所以我们取了一个折中的方法，让</p>
<p>$U(T)=\prod_{k=1}^{N_{t}}p(t_k|t_1,t_2,…,t_{k-1}) \cdot g(k)$ </p>
<p>with<br>$$ g(k)=<br>  \begin{cases}<br>  1 &amp; {if k \leq \gamma} \<br>  0 &amp; {if k \geq \gamma}<br>  \end{cases}<br>  $$<br>取代$p(T)$，使得损失函数为</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\lambda \log U(T)}$</p>
<p>另外实际应用中，还加入了响应句子长度的因素$\gamma N_t$，作为模型相应的依据，因此将目标函数修正如下</p>
<p>$Score(T) = p(T|S)-\lambda U(T) + \gamma N_t$</p>
<p>所以损失函数应该是</p>
<p>$\underset{T}{argmin} {-\log p(T|S)+\lambda \log p(T)-\log (\gamma N_t)}$</p>
<h5 id="anti-bidi"><a href="#anti-bidi" class="headerlink" title="anti-bidi"></a>anti-bidi</h5><p>由上面的推导，我们从MMI得到</p>
<p>${\hat T}=\underset{T}{argmax} {\log p(T|S)-\lambda \log p(T)}$</p>
<p>我们又知道</p>
<p>$\log p(T)=\log p(T|S) + \log p(S) - \log p(S|T)$</p>
<p>带入得到</p>
<p>${\hat T}=\underset{T}{argmax} {(1-\lambda)\log p(T|S)+\lambda \log p(S|T)}$</p>
<p>因为$p(S|T)$需要通过生成的$T$来再次通过训练好的seq2seq模型再计算一遍，计算量特别大，所以我们通过beam search只取前$N$个$T$，比如200个，再计算$S$。</p>
<p>但是在使用beam search的时候，我们知道，beam search有很多输出只是一两个字或者只是符号发生了变化，减少了取$N$个不同输出$T$的意义，因为本身输出句子的多样性不能满足这里的要求，所以我们对beam search进行了更改。</p>
<p>一般beam search在到下一节点时，我们是通过混合所有父节点的字节点，统一进行比较，然后取整体概率最大的$N$个选择，这里我们更改为，针对每一个父节点，我们先对其对应的字节点按先后顺序将其概率从上往下减去一个越来越大的数，使得我们能更好的保存更多不同的父节点，自然句子的多样性也就增加了。具体例子见下图<img src="/public/images/image/beam_search_anti_bidi.png" alt="beamsearch_antibidi"><br>另外，也在anti-bidi中，做了同anti-LM相同的处理，增加了回复长度的影响。</p>
<p>闲聊用的是seq2seq，loss中加入了anti-lm来减少无意义回复，加入了问句与回答的余弦相似性来减少重复问题的回答</p>
<h5 id="anti-bidi-VS-anti-LM"><a href="#anti-bidi-VS-anti-LM" class="headerlink" title="anti-bidi VS anti-LM"></a>anti-bidi VS anti-LM</h5><ol>
<li>两者虽然都是使用的MMI，但是使用的公式不一样，背后的逻辑原理也不一样</li>
<li>anti-bidi比anti-LM要更耗时，因为他要经过两次模型，且非常依赖于beam search生成的句子的多样性</li>
</ol>
<h5 id="anti-LM的具体实现"><a href="#anti-LM的具体实现" class="headerlink" title="anti-LM的具体实现"></a>anti-LM的具体实现</h5><p>如何计算P(T)<br>首先应该意识到P(T)应该是一个语言模型学习出来的结果，而不简单的是联合概率的乘积。那么如何得到这个模型学习出来的结果呢？在实现时需要在decoder的输入端输入一个全零的初始化状态，具体做法可以输入一句话长度的pad标志符，让decoder生成结果，得到P(T)</p>
<p>详见<code>just_another_seq2seq_master</code>里<code>train-anti.py</code>中第86～88行和第98～101行。</p>
<p>下面代码见前三行和中间的<code>add_loss</code>：</p>
<ol>
<li>前三行代码：输入为一定维度的PAD</li>
<li><code>add_loss</code>计算的是输入为PAD时(即初始状态下)的输出，即$P(T)$</li>
<li><code>add_loss *= -0.5</code>表示每次计算完$P(T)$后乘以$-0.5$，即$\lambda=0.5$</li>
<li>我们没有用$U(T)$，但是并没有明显输出语句不通顺的情况</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dummy_encoder_inputs = np.array([</span><br><span class="line">    np.array([WordSequence.PAD]) <span class="keyword">for</span> _ <span class="keyword">in</span> range(batch_size)])</span><br><span class="line">dummy_encoder_inputs_lengths = np.array([<span class="number">1</span>] * batch_size)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">1</span>, n_epoch + <span class="number">1</span>):</span><br><span class="line">    costs = []</span><br><span class="line">    bar = tqdm(range(steps), total=steps,</span><br><span class="line">                desc=<span class="string">'epoch &#123;&#125;, loss=0.000000'</span>.format(epoch))</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> bar:</span><br><span class="line">        x, xl, y, yl = next(flow)</span><br><span class="line">        x = np.flip(x, axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        add_loss = model.train(sess,</span><br><span class="line">                                dummy_encoder_inputs,</span><br><span class="line">                                dummy_encoder_inputs_lengths,</span><br><span class="line">                                y, yl, loss_only=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        add_loss *= <span class="number">-0.5</span></span><br><span class="line">        <span class="comment"># print(x, y)</span></span><br><span class="line">        cost, lr = model.train(sess, x, xl, y, yl,</span><br><span class="line">                                return_lr=<span class="literal">True</span>, add_loss=add_loss)</span><br><span class="line">        costs.append(cost)</span><br><span class="line">        bar.set_description(<span class="string">'epoch &#123;&#125; loss=&#123;:.6f&#125; lr=&#123;:.6f&#125;'</span>.format(</span><br><span class="line">            epoch,</span><br><span class="line">            np.mean(costs),</span><br><span class="line">            lr</span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">    model.save(sess, save_path)</span><br></pre></td></tr></table></figure>

<h4 id="seq2seq"><a href="#seq2seq" class="headerlink" title="seq2seq"></a>seq2seq</h4><h4 id="anti-lm"><a href="#anti-lm" class="headerlink" title="anti-lm"></a>anti-lm</h4><h5 id="最大互信息"><a href="#最大互信息" class="headerlink" title="最大互信息"></a>最大互信息</h5><h4 id="attention-LSTM"><a href="#attention-LSTM" class="headerlink" title="attention+LSTM"></a>attention+LSTM</h4><h5 id="attention"><a href="#attention" class="headerlink" title="attention"></a>attention</h5><h5 id="transformer"><a href="#transformer" class="headerlink" title="transformer"></a>transformer</h5><h4 id="模型评估"><a href="#模型评估" class="headerlink" title="模型评估"></a>模型评估</h4><p>闲聊的评估是带有非常大的主观性的，而且场景不同，用户不同，对相同结果的满意度也会不一样，所以我们对模型的评估希望从更客观、更基础的地方来进行评估和改进：</p>
<ol>
<li>无意义回答的频率/百分比</li>
<li>相似回答的频率/百分比</li>
<li>正确回答的百分比</li>
<li>简单问题正确回答的百分比</li>
<li>相同问题，是否能给出相同答案</li>
<li>过于简短回答的百分比</li>
<li>对话内容大多无意义，无法深入交谈具体事情</li>
</ol>
<h4 id="努力方向"><a href="#努力方向" class="headerlink" title="努力方向"></a>努力方向</h4><ol>
<li>模型优化</li>
<li>数据集质量优化+数据集扩充</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>chitchat</tag>
        <tag>seq2seq</tag>
      </tags>
  </entry>
  <entry>
    <title>protobuf</title>
    <url>/2019/01/16/protobuf/</url>
    <content><![CDATA[<h4 id="protobuf"><a href="#protobuf" class="headerlink" title="protobuf"></a>protobuf</h4><ul>
<li><code>protobuf</code>：把消息序列化的工具</li>
<li><code>rpc</code>： 远程方法调用</li>
<li><code>redis</code>：缓存</li>
<li><code>mysql</code>：持久储存</li>
</ul>
<h4 id="搭建微服务实例"><a href="#搭建微服务实例" class="headerlink" title="搭建微服务实例"></a>搭建微服务实例</h4><p><a href="https://blog.goodaudience.com/ml-client-server-using-grpc-in-python-3cba7693d1f5" target="_blank" rel="noopener">ML Client/Server Using gRPC in Python – Good Audience</a></p>
<h4 id="安装Protocol-Buffers："><a href="#安装Protocol-Buffers：" class="headerlink" title="安装Protocol Buffers："></a>安装Protocol Buffers：</h4><ol>
<li>ruby -e “$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;" target="_blank" rel="noopener">https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</a></li>
<li>brew install protobuf</li>
<li>安装gRPC：pip install grpcio</li>
<li>安装gRPC：pip install grpcio-tools</li>
</ol>
<a id="more"></a>

<h4 id="编译proto"><a href="#编译proto" class="headerlink" title="编译proto"></a>编译proto</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m grpc_tools.protoc -I .&#x2F;protos --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;intent.proto</span><br></pre></td></tr></table></figure>

<ul>
<li><code>-I</code><br>指定生成的<code>pb2.py</code>和<code>pb2_grpc.py</code>文件的储存路径</li>
<li><code>./protos/intent.proto</code><br>需要编译的<code>.proto</code>文件的位置</li>
</ul>
<p>如果文件原来的位置如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I .&#x2F;protos&#x2F; --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br><span class="line">- intent_pb2.py</span><br><span class="line">- intent_pb2_grpc.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I . --python_out&#x3D;. --grpc_python_out&#x3D;. .&#x2F;protos&#x2F;price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">- protos</span><br><span class="line">- intent.proto</span><br><span class="line">- intent_pb2.py</span><br><span class="line">- intent_pb2_grpc.py</span><br><span class="line">- client.py</span><br><span class="line">- server.py</span><br><span class="line">- predict.py</span><br></pre></td></tr></table></figure>

<p>没弄明白为什么，不过为了方便函数之间的调用，一般使用第一种方法。</p>
<h4 id="http-VS-grpc"><a href="#http-VS-grpc" class="headerlink" title="http VS grpc"></a>http VS grpc</h4><ol>
<li><code>http</code>和<code>grpc</code>都是通过<code>tcp</code>运行。</li>
<li>不同的是<code>grpc</code>会将信息转换成二进制格式，因此传输更快，所需内存更小，效率更高。<br>但是因为<code>grpc</code>中，双方都需要编写相同的<code>.proto</code>，彼此传输的格式要固定，因此要求相对更严格一些。而http则没有这种要求，双方并没有格式的约定，所以<code>http</code>也更大一些，速度相对更慢一些。</li>
<li><code>http</code>通过<code>&quot;\\r\\n&quot;</code>来分隔信息。<code>grpc</code>则要通过<code>.proto</code>文件里的<code>message</code>严格规范信息的数目、类型等。</li>
</ol>
]]></content>
      <categories>
        <category>工作总结</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title>tensorflow实战应用</title>
    <url>/2019/01/16/tensorflow/</url>
    <content><![CDATA[<h2 id="tensorflow-重新加载并继续训练模型"><a href="#tensorflow-重新加载并继续训练模型" class="headerlink" title="tensorflow 重新加载并继续训练模型"></a>tensorflow 重新加载并继续训练模型</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ckpt_file &#x3D; tf.train.latest_checkpoint(path_save_model)</span><br><span class="line">if ckpt_file:</span><br><span class="line">    saver.restore(sess, ckpt_file)</span><br></pre></td></tr></table></figure>

<p>这里<code>ckpt_file</code>是文件<code>checkpoint</code>里<code>model_checkpoint_path</code>的值。<br>只要<code>model_checkpoint_path</code>指定的<code>checkpoint</code>在路径下存在，就会直接加载这个<code>checkpoint</code>继续训练。否则就会重新从<code>0</code>开始。</p>
<a id="more"></a>

<h3 id="tensorboard"><a href="#tensorboard" class="headerlink" title="tensorboard"></a>tensorboard</h3><p><code>log/</code>中会因为重新加载，有几个分开的文件，但是对于同一个变量（如<code>loss</code>）会在同一个图内显示所有其随着<code>steps</code>变化的曲线。<br><img src="/images/image/tensorboard.png" alt="tensorboard_tf.train.latest_checkpoint.png"></p>
<hr>
<h2 id="用tensorflow实现seq2seq的细节问题"><a href="#用tensorflow实现seq2seq的细节问题" class="headerlink" title="用tensorflow实现seq2seq的细节问题"></a>用tensorflow实现seq2seq的细节问题</h2><h3 id="1-maximum-iteration"><a href="#1-maximum-iteration" class="headerlink" title="1. maximum_iteration"></a>1. maximum_iteration</h3><p>在<code>dynamic_decode</code>中有参数<code>maximum_iteration</code>。</p>
<p>这个参数在<code>training</code>的时候并没有什么作用，因为在<code>training</code>时，<code>TrainingHelper</code>中有参数<code>sequence_length</code>，在解码到<code>sequence_length</code>的长度后，<code>TrainingHelper</code>会通过参数<code>finished</code>来告诉模型，解码已经结束。</p>
<p>在<code>inference</code>的时候，则会因为没有<code>sequence_length</code>的限制，而会不知道该在哪里停止，或者解码的句子过长。所以<code>maximum_iteration</code>在<code>inference</code>的时候，最好是有设定的。</p>
<h3 id="2-具体的seq2seq模型的输入和输出实例"><a href="#2-具体的seq2seq模型的输入和输出实例" class="headerlink" title="2. 具体的seq2seq模型的输入和输出实例"></a>2. 具体的seq2seq模型的输入和输出实例</h3><p><code>encoder_inputs</code>：length = [3, 4]</p>
<pre><code>&lt;sos&gt;, a, b, c, &lt;eos&gt;, &lt;pad&gt;
&lt;sos&gt;, c, b, a, d, &lt;eos&gt;</code></pre><p><code>decoder_inputs</code>: length = [2, 3]</p>
<pre><code>&lt;sos&gt;, e, f, &lt;eos&gt;
&lt;sos&gt;, e, f, g</code></pre><p><code>decoder_outputs</code>: length = [2, 3]</p>
<pre><code>e, f, &lt;eos&gt;, &lt;eos&gt;
e, f, g, &lt;eos&gt;</code></pre><p>可以看出，<code>&lt;sos&gt;</code>是不会出现在<code>decoder_outputs</code>里面的，无论是在<code>training</code>还是<code>inference</code>里，<code>decoder</code>的第一个输入都是<code>&lt;sos&gt;</code>，<code>decoder</code>的第一个输出就直接是句子的第一个字/词。</p>
<hr>
<h2 id="变形seq2seq，即在training时，上一步的输出成为下一步的输入。"><a href="#变形seq2seq，即在training时，上一步的输出成为下一步的输入。" class="headerlink" title="变形seq2seq，即在training时，上一步的输出成为下一步的输入。"></a>变形seq2seq，即在training时，上一步的输出成为下一步的输入。</h2><p><strong>用<code>GreedyEmbeddingHelper</code>替换<code>TrainingHelper</code></strong></p>
<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>在seq2seq的training中，我们一般用<code>target</code>，即上一步正确的输出，作为下一步的输入。但是因为这样，同样一句话在train时的输出，后infer时的输出是完全不一样的（同事说是因为模型训练的还不够好，这点有待考证）。为了使自己在infer时的输出更可控，我改变了train时的模型，下一步的输入不再是<code>target</code>（上一步的<strong>正确输出</strong>），而是上一步模型的<strong>真实输出</strong>。</p>
<h3 id="2-困难"><a href="#2-困难" class="headerlink" title="2. 困难"></a>2. 困难</h3><ul>
<li>这样最大的困难就是在<code>sequence_loss</code>的计算时会因为shape不同而报错。<br>因为少了y_label的约束，真实输出的句子长度是完全不可控的，但是在计算<code>sequence_loss</code>时计算的是两者<code>label</code>的熵（<code>nn_ops.sparse_softmax_cross_entropy_with_logits</code>），所以我们需要统一两者的<code>shape</code>。</li>
</ul>
<ul>
<li>真实输出<code>self.decoder_logits_train=outputs.rnn_outputs</code>。<code>self.decoder_logits_train</code>的<code>shape</code>是<code>[batch_size, sequence_len, vocab_size]</code>。这里的<code>sequence_len</code>实际上是这个<code>batch_size</code>中，真实输出的<strong>最长句子的长度</strong>。<br>同样，这个<code>batch_size</code>的<code>targets</code>，也会有个最长句子的长度<code>self.max_decode_length</code>。我们只需要让每个<code>batch_size</code>中的<code>sequence_len=self.max_decode_length</code>就可以了。<br>如果<code>sequence_len</code>太短，就加到<code>self.max_decode_length</code>的长度，如果太长，就用<code>tf.slice</code>给切断。<br>但是问题又来了：<ul>
<li>因为<code>self.decoder_logits_train.shape[1]</code> <code>type</code>的问题，又因为它的值是<code>None</code>，想要比较它和<code>self.max_decode_length</code>非常困难</li>
<li>就算比较了两者的值，想要把<code>self.decoder_logits_train</code>的第二维<code>tf.concat</code>到<code>self.max_decode_length</code>也要进行<code>tf.subtract(self.max_decode_length, self.decoder_logits_train.shape[1])</code>的运算。也会增加困难</li>
</ul>
</li>
</ul>
<h3 id="3-解决方案"><a href="#3-解决方案" class="headerlink" title="3. 解决方案"></a>3. 解决方案</h3><p>好在我们在数据整理的时候去除了过长的句子，使得<code>targets</code>的最大长度不会超过50。所以<code>self.max_decode_length</code>不会大于50。</p>
<p>我们可以直接<code>tf.concat</code> <code>self.decoder_logits_train</code>和一个<code>shape</code>为<code>[batch_size, 50, vocab_size]</code>的矩阵，然后用<code>tf.slice</code>把第二维按<code>self.max_decoce_length</code>来切断。这样无论<code>self.decoder_logits_train</code>是过长还是过短，我们都能得到最终想要的结果。</p>
<h3 id="4-遗留问题"><a href="#4-遗留问题" class="headerlink" title="4.遗留问题"></a>4.遗留问题</h3><ul>
<li>因为这里我们知道<code>self.max_decode_length</code>的最大长度，所以可以取巧，如果不知道的话，可能更麻烦一些。</li>
<li>当句子过短时，会因为后面都是用<code>&lt;pad&gt;</code>补上，因此可以计算出正确的<code>loss</code>。<br>但是当句子过长时，因为我们把<code>self.max_decode_length</code>后面部分全部截断，因此这里<code>loss</code>只计算了前面一部分的句子，后面部分的句子并不可控（可能后面部分非常长，但是仍然只有截断时的那个字符的差别产生<code>loss</code>——即<strong>真实输出</strong>文字，<strong>正确输出</strong>是”\<eos>“）。<br>所以<strong>可能</strong>会导致模型生成特别特别长的句子，但是只有前半部分是我们想要的。</li>
</ul>
<hr>
<h2 id="tf分布式-–-DistributionStrategy"><a href="#tf分布式-–-DistributionStrategy" class="headerlink" title="tf分布式 – DistributionStrategy"></a>tf分布式 – DistributionStrategy</h2><p>以下内容摘抄自：<br><a href="http://jcf94.com/2018/10/21/2018-10-21-tfunpacking9/" target="_blank" rel="noopener">TensorFlow 拆包（九）：High Level APIs \| Chenfan Blog</a></p>
<p><strong>看一下官方文档中对 <code>DistributionStrategy</code> 的设计思想。</strong></p>
<p>首先是一些底层的概念：</p>
<ul>
<li>Wrapped values：跨设备相关的变量可以被封装为两种类别，PerDevice 对象表示的变量在每个设备上的值不同，Mirrored 对象表示的变量在每个设备上的值都相同</li>
<li>Unwrapping and merging：考虑前面提过的这个函数 <code>call_for_each_tower(fn, w)</code>，<code>fn</code> 是模型函数，<code>w</code> 代表一些 Wrapped values。这个函数的调用过程中就包含了变量的 unwrapping 和 merging，假定在设备 <code>d0</code> 上 <code>fn(w0)</code> 得到的结果是 <code>(x, a, v0)</code>，在设备 <code>d1</code> 上 <code>fn(w1)</code> 得到的结果是 <code>(x, b, v1)</code>。首先在调用函数之前，<code>w</code> 需要被解包变成 <code>w0</code> 和 <code>w1</code> 然后分别调用 <code>fn</code> 函数。返回的结果有三种情况，第一个值都返回了一个相同的对象 <code>x</code>，则最终 <code>merge</code> 之后还是对象 <code>x</code>；第二个值是每个设备不一样的，则 <code>merge</code> 之后是一个 <code>PerDevice</code> 对象（其实就是个设备和对应值的 <code>map</code>）；第三个值是每个设备返回的分别是一组 <code>Mirrored</code> 对象的成员，则 <code>merge</code> 之后是一个 <code>Mirrored</code> 对象。所以 <code>call_for_each_tower(fn, w)</code> 在这里返回得到的就是一组 <code>(x, PerDevice{...}, Mirrored{...})</code></li>
<li>Tower context vs. Cross-tower context：Tower context 指的是对每个设备的封装上下文，通常对每个设备分别跑一遍模型函数就需要这种封装；Cross-tower context 指的是跨设备的封装上下文，比如说像 <code>reduce()</code> 这种所有设备共同参与的一个操作就需要这种封装</li>
<li>Worker devices vs. parameter devices：负责计算的设备和存参数的设备，没啥好说的。</li>
</ul>
<p>更新一个变量的常规操作如下：</p>
<ol>
<li>把输入数据集封装在 <code>d.distribute_dataset()</code> 中，然后创建一个 iterator</li>
<li>对每一个设备共同调用 d.call_for_each_tower() 来分别创建网络模型，并且最终各自得到一组梯度/变量对：<code>d0</code> 上有 <code>{(g0, v0), (g1, v1), ...}</code>，<code>d1</code> 上有 <code>{(g&#39;0, v0), (g&#39;1, v1), ...}</code> 等等这样</li>
<li>调用<code>d.reduce(VariableAggregation.SUM, t, v)</code> 或者 <code>d.batch_reduce()</code> 来对梯度求和，并且对应到各自的变量上：<code>{(Sum(g0, g&#39;0), v0), (Sum(g1, g&#39;1), v1), ...}</code></li>
<li>调用 <code>d.update(v)</code> 来对每一个变量进行更新</li>
</ol>
<p>3、4 两步如果用 <code>Optimizer</code> 中的 <code>apply_gradients()</code> 方法可以自动完成（……这就是 <code>Optimizer</code> 后来加进去那部分代码的作用），或者在一个 <code>Cross-tower context</code> 中调用 <code>_distributed_apply()</code> 方法也可以。常规的网络层都应该在 <code>Tower context</code> 中被调用。</p>
]]></content>
      <categories>
        <category>实战问题</category>
      </categories>
      <tags>
        <tag>seq2seq</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
  <entry>
    <title>docker</title>
    <url>/2019/01/16/docker/</url>
    <content><![CDATA[<h2 id="docker-挂载"><a href="#docker-挂载" class="headerlink" title="docker 挂载"></a>docker 挂载</h2><p>前面是本地文件，后面是镜像文件：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus&#x3D;3 -v &#x2F;Users&#x2F;shijiao&#x2F;Documents&#x2F;Bert&#x2F;:&#x2F;shijiao&#x2F;mkl conda-origin bash</span><br></pre></td></tr></table></figure>

<p><code>--cpus=8</code>设置的是cpu利用百分比，而不是个数，因为报错中，<code>Range of CPUs</code>是小数，而不是整数。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus&#x3D;8 -v &#x2F;Users&#x2F;shijiao&#x2F;Documents&#x2F;Bert&#x2F;:&#x2F;shijiao&#x2F;mkl conda-origin bash</span><br><span class="line">Error response from daemon: Range of CPUs is from 0.01 to 6.00, as there are only 6 CPUs available.</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<hr>
<h2 id="SF-配置并使用-Docker"><a href="#SF-配置并使用-Docker" class="headerlink" title="SF 配置并使用 Docker"></a>SF 配置并使用 Docker</h2><h3 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h3><ol>
<li><p>Homebrew 安装命令：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">brew cask install docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 Docker：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 --name webserver nginx</span><br></pre></td></tr></table></figure>

<p> 打开 <code>http://localhost</code>，看到 <code>Welcome to nginx!</code> 说明 Docker 安装成功！</p>
<ul>
<li>-d: 让容器在后台运行</li>
<li>-p: 将容器内部使用的网络端口映射到我们使用的主机上</li>
</ul>
</li>
<li><p>停止 Nginx 服务器并删除：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker stop webserver</span><br><span class="line">docker rm webserver</span><br></pre></td></tr></table></figure>
</li>
<li><p>国内镜像加速</p>
<p> Docker -&gt; Preferences -&gt; Daemon -&gt; Registry mirrors 添加：<code>https://registry.docker-cn.com</code></p>
</li>
<li><p>测试加速器</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker info</span><br></pre></td></tr></table></figure>

<p> 如有以下配置内容，说明加速器配置成功：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">Registry Mirrors:</span><br><span class="line">    https://registry.docker-cn.com/</span><br></pre></td></tr></table></figure>



</li>
</ol>
<h3 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h3><blockquote>
<p>个人偏好：仅配置一个镜像 (我以<strong><em>湖人总冠军</em></strong>命名 <code>lakers_champion</code>，不包含任何代码数据)，仅仅把需要安装的package封装好<br>这样就避免不用每个项目都给它单独弄一个镜像，10G+占空间啊！</p>
</blockquote>
<ol>
<li><p>本地build镜像</p>
<p> 新建 Dockerfile，e.g.,</p>
 <figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> okwrtdsh/anaconda3:pytorch-<span class="number">10.0</span>-cudnn7</span><br><span class="line"><span class="keyword">MAINTAINER</span> kun.xie@sfmail.sf-express.com</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> apt-get update \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install --upgrade pip \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install tqdm \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install jieba \\</span></span><br><span class="line"><span class="bash">&amp;&amp; pip install gensim \\</span></span><br><span class="line"><span class="bash">&amp;&amp; apt-get install -y vim</span></span><br><span class="line">&amp;&amp; apt-get install -y git</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com tensorflow tensorboardX</span></span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple cupy pynvrtc --trusted-host mirrors.aliyun.com</span><br><span class="line">&amp;&amp; pip install git+https://github.com/salesforce/pytorch-qrnn</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com torchtext</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com jieba gensim</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com opencc-python-reimplemented</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span></span><br><span class="line">&amp;&amp; conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> conda install jupyter -y --quiet</span></span><br><span class="line">&amp;&amp; conda install pytorch torchvision -c pytorch</span><br><span class="line">&amp;&amp; conda install scikit-learn -y --quiet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD . /kxie</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> mkdir kxie</span></span><br><span class="line"><span class="comment">### 其实以上安装的package可以简单点写到文件requirements.txt中，以下一行命令就能完成安装操作 -- 因为懒没做！ ###</span></span><br><span class="line"><span class="comment"># RUN pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com -r ./requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="bash"> /kxie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="bash"> [ <span class="string">"/bin/bash"</span> ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li><p>加了 命令 <code>CMD [&quot;/bin/bash&quot;]</code>后，我们直接<code>docker run -it name_of_mirror</code>后就能直接进入<code>/bin/bash/</code>了。</p>
</li>
<li><p>我们也可以在运行时指定别的命令，如<code>docker run -it name_of_mirror cat /etc/os-release</code>，就是用<code>cat /etc/os-release</code>替换了<code>/bin/bash</code>命令。</p>
<p>随后在当前目录下 <code>build</code>：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">docker build -t 10.202.107.19/sfai/lakers_champion .</span><br></pre></td></tr></table></figure>
<p>可以查看本地已经build好的镜像：<code>docker images</code></p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br><span class="line">REPOSITORY                                            TAG                 IMAGE ID               CREATED                   SIZE</span><br><span class="line">10.202.107.19/sfai/lakers_champion   latest               25473e876cd6        About an hour ago   11.1GB</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>把本地build好的镜像push到Harbor</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">docker login 10.202.107.19</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
</li>
<li><p>在server上把镜像从Harbor中pull下来</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ssh 01366808@10.202.90.201</span></span><br><span class="line">sudo docker pull 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>

<p> <code>docker images</code> 同样可以查看server上目前已有的镜像</p>
</li>
<li><p>server上运行</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">sudo nvidia-docker run -i -t --rm -p &lt;随便设置_1&gt;:8888 -p &lt;随便设置_2&gt;:6006 -v /HDATA/1/01366808/shijiao/test_map:/shijiao 10.202.107.19/sfai/tf1.12-cuda10-cudnn7-speech:latest bash</span><br></pre></td></tr></table></figure>
 <p><font color="#989898">
 <ul>
 <li>`<随便设置_1>` & `<随便设置_2>` 按个人喜好自行更改，避免冲突</li>
 <li>我把所有的代码以及数据放在了server上个人文件夹`~/kxie/`里面</li>
 <li>之后打开Jupyter Notebook页面的时候由于映射关系，能看到所有数据以及代码！很方便！！</li>
 </ul>
 </font></p>

<p> 运行 <code>ternsorboard</code> &amp; <code>Jupyter Notebook</code>：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/bin/bash -</span></span><br><span class="line">nohup tensorboard --host=0.0.0.0 --logdir=/data/<span class="built_in">log</span>/kxie &amp;</span><br><span class="line">/opt/conda/bin/jupyter notebook  --ip=<span class="string">'*'</span> --no-browser --allow-root --NotebookApp.token= --notebook-dir=<span class="string">'/shijiao'</span></span><br></pre></td></tr></table></figure>
<p> 这样就可以打开网页 <code>http://10.202.90.201:&lt;随便设置_1&gt;</code> 跑代码了！Jupyter的可视化还是很方便的，从本地copy paste也容易！</p>
<p> <strong>具体命令分析，可以看新工作笔记第二页</strong></p>
</li>
</ol>
<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h3 id="想修改镜像内容怎么办？"><a href="#想修改镜像内容怎么办？" class="headerlink" title="想修改镜像内容怎么办？"></a>想修改镜像内容怎么办？</h3><p>进入docker镜像里面改：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion /bin/bash </span><br><span class="line">修改内容 <span class="comment"># 然后退出并记住 ***Container ID***</span></span><br><span class="line">docker commit &lt;ID&gt; 10.202.107.19/sfai/lakers_champion</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">server上重新pull: sudo docker pull 10.202.107.19/sfai/lakers_champion</span><br></pre></td></tr></table></figure>

<p>e.g., 修改本地镜像，使得能够使用<code>fzf</code>:</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion</span><br><span class="line"><span class="comment"># after enter into this image:</span></span><br><span class="line"><span class="built_in">cd</span></span><br><span class="line">git <span class="built_in">clone</span> --depth 1 https://github.com/junegunn/fzf.git ~/.fzf</span><br><span class="line">~/.fzf/install</span><br><span class="line">vi .vimrc</span><br><span class="line"><span class="built_in">set</span> rtp+=~/.fzf</span><br><span class="line"><span class="built_in">exit</span> <span class="comment"># and remember container id</span></span><br><span class="line">docker commit &lt;container id&gt; 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/junegunn/fzf" target="_blank" rel="noopener">GitHub - junegunn/fzf: A command-line fuzzy finder</a></p>
<p>将本地文件添加到镜像中：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker cp &lt;<span class="built_in">local</span> <span class="built_in">source</span>&gt; &lt;Container ID&gt;:&lt;dest&gt;</span><br><span class="line"><span class="comment"># 然后同上commit</span></span><br></pre></td></tr></table></figure>

<p>Terminal过段时间就会中断 <code>timed out: Connection to 10.202.90.201 closed.</code> 如何重新进入正在后台运行的Docker？<br>(当然也有办法永不中断，自行Google)</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># ssh 01366808@10.202.90.201</span></span><br><span class="line">sudo docker <span class="built_in">exec</span> -it &lt;ID&gt; bash</span><br></pre></td></tr></table></figure>

<p>可以用 <code>docker ps</code> 查看Docker进程以及ID</p>
<p>删除镜像：<code>sudo docker rmi &lt;ID&gt;</code></p>
<p>结束运行的容器：<code>sudo docker stop &lt;ID&gt;</code></p>
<p>修改镜像名字或者tag：</p>
<figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">docker tag &lt;old_name:old_tag&gt; &lt;new_name:new_tag&gt;</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line">docker tag &lt;ID&gt; &lt;new_name:new_tag&gt;</span><br></pre></td></tr></table></figure>

<h3 id="Sync"><a href="#Sync" class="headerlink" title="Sync"></a>Sync</h3><p><font color="#989898">好像server上不连网的，没法Git同步，影响效率！</font></p>

<ol>
<li><p>从本地scp到服务器上:</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">scp -r &lt;<span class="built_in">source</span>&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>rsync</code>：</p>
 <figure class="highlight zsh"><table><tr><td class="code"><pre><span class="line">rsync -av -e ssh --exclude=<span class="string">'excluded dir'</span> &lt;<span class="built_in">source</span>&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>

<p> <code>--exclude=&#39;excluded dir&#39;</code>：本地<source>中有些目录的内容是你不想传到server上的</p>
 <p><font color="#989898">
 <ul>
 <li>一般情况：本地修改完代码，使用上面rysnc命令同步到server上 (或者简单点通过Jupyter copy paste)，然后跑吧！</li>
 <li>交换 source dest 位置实现反向同步</li>
 </ul>
 </font></p>
</li>
</ol>
]]></content>
      <categories>
        <category>实战问题</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
</search>
