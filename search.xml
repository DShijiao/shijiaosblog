<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Docker安装和基础使用</title>
    <url>/2019/01/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/Docker%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Docker安装和基础使用"><a href="#Docker安装和基础使用" class="headerlink" title="Docker安装和基础使用"></a>Docker安装和基础使用</h1><h2 id="SF-配置并使用-Docker"><a href="#SF-配置并使用-Docker" class="headerlink" title="SF 配置并使用 Docker"></a>SF 配置并使用 Docker</h2><h3 id="Install-Docker"><a href="#Install-Docker" class="headerlink" title="Install Docker"></a>Install Docker</h3><ol>
<li><p>Homebrew 安装命令：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">brew cask install docker</span><br></pre></td></tr></table></figure>
</li>
<li><p>测试 Docker：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -d -p 80:80 --name webserver nginx</span><br></pre></td></tr></table></figure>

<p> 打开 <code>http://localhost</code>，看到 <code>Welcome to nginx!</code> 说明 Docker 安装成功！</p>
<ul>
<li>-d: 让容器在后台运行</li>
<li>-p: 将容器内部使用的网络端口映射到我们使用的主机上</li>
</ul>
</li>
<li><p>停止 Nginx 服务器并删除：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker stop webserver</span><br><span class="line">docker rm webserver</span><br></pre></td></tr></table></figure>
</li>
<li><p>国内镜像加速</p>
<p> Docker -&gt; Preferences -&gt; Daemon -&gt; Registry mirrors 添加：<code>https://registry.docker-cn.com</code></p>
</li>
<li><p>测试加速器</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker info</span><br></pre></td></tr></table></figure>

<p> 如有以下配置内容，说明加速器配置成功：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Registry Mirrors:</span><br><span class="line">    https://registry.docker-cn.com/</span><br></pre></td></tr></table></figure></li>
</ol>
<h3 id="Docker-Image"><a href="#Docker-Image" class="headerlink" title="Docker Image"></a>Docker Image</h3><blockquote>
<p>个人偏好：仅配置一个镜像 (我以<em><strong>湖人总冠军</strong></em>命名 <code>lakers_champion</code>，不包含任何代码数据)，仅仅把需要安装的package封装好<br>这样就避免不用每个项目都给它单独弄一个镜像，10G+占空间啊！</p>
</blockquote>
<ol>
<li><p>本地build镜像</p>
<p> 新建 Dockerfile，e.g.,</p>
 <figure class="highlight dockerfile"><table><tr><td class="code"><pre><span class="line"><span class="keyword">FROM</span> okwrtdsh/anaconda3:pytorch-<span class="number">10.0</span>-cudnn7</span><br><span class="line"><span class="keyword">MAINTAINER</span> kun.xie@sfmail.sf-express.com</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> apt-get update \\</span></span><br><span class="line"><span class="language-bash">&amp;&amp; pip install --upgrade pip \\</span></span><br><span class="line"><span class="language-bash">&amp;&amp; pip install tqdm \\</span></span><br><span class="line"><span class="language-bash">&amp;&amp; pip install jieba \\</span></span><br><span class="line"><span class="language-bash">&amp;&amp; pip install gensim \\</span></span><br><span class="line"><span class="language-bash">&amp;&amp; apt-get install -y vim</span></span><br><span class="line">&amp;&amp; apt-get install -y git</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com tensorflow tensorboardX</span></span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple cupy pynvrtc --trusted-host mirrors.aliyun.com</span><br><span class="line">&amp;&amp; pip install git+https://github.com/salesforce/pytorch-qrnn</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com torchtext</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com jieba gensim</span><br><span class="line">&amp;&amp; pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com opencc-python-reimplemented</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="language-bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="language-bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/pytorch/</span></span><br><span class="line">&amp;&amp; conda config --<span class="keyword">add</span><span class="language-bash"> channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/conda-forge/</span></span><br><span class="line">&amp;&amp; conda config --set show_channel_urls yes</span><br><span class="line"></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> conda install jupyter -y --quiet</span></span><br><span class="line">&amp;&amp; conda install pytorch torchvision -c pytorch</span><br><span class="line">&amp;&amp; conda install scikit-learn -y --quiet</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ADD . /kxie</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">mkdir</span> kxie</span></span><br><span class="line"><span class="comment">### 其实以上安装的package可以简单点写到文件requirements.txt中，以下一行命令就能完成安装操作 -- 因为懒没做！ ###</span></span><br><span class="line"><span class="comment"># RUN pip install -i https://mirrors.aliyun.com/pypi/simple --trusted-host mirrors.aliyun.com -r ./requirements.txt</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /kxie</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [ <span class="string">&quot;/bin/bash&quot;</span> ]</span></span><br></pre></td></tr></table></figure>
<ul>
<li>加了 命令 <code>CMD [&quot;/bin/bash&quot;]</code>后，我们直接<code>docker run -it name_of_mirror</code>后就能直接进入<code>/bin/bash/</code>了。</li>
<li>我们也可以在运行时指定别的命令，如<code>docker run -it name_of_mirror cat /etc/os-release</code>，就是用<code>cat /etc/os-release</code>替换了<code>/bin/bash</code>命令。</li>
</ul>
<p> 随后在当前目录下 <code>build</code>：<br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">docker build -t 10.202.107.19/sfai/lakers_champion .</span><br></pre></td></tr></table></figure><br> 可以查看本地已经build好的镜像：<code>docker images</code><br> <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ docker images</span><br><span class="line">REPOSITORY                                            TAG                 IMAGE ID               CREATED                   SIZE</span><br><span class="line">10.202.107.19/sfai/lakers_champion   latest               25473e876cd6        About an hour ago   11.1GB</span><br></pre></td></tr></table></figure></p>
</li>
<li><p>把本地build好的镜像push到Harbor</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#! /bin/bash</span><br><span class="line">docker login 10.202.107.19</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
</li>
<li><p>在server上把镜像从Harbor中pull下来</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ssh 01366808@10.202.90.201</span><br><span class="line">sudo docker pull 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
<p> <code>docker images</code> 同样可以查看server上目前已有的镜像</p>
</li>
<li><p>server上运行</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">sudo nvidia-docker run -i -t --rm -p &lt;随便设置_1&gt;:8888 -p &lt;随便设置_2&gt;:6006 -v /HDATA/1/01366808/shijiao/test_map:/shijiao 10.202.107.19/sfai/tf1.12-cuda10-cudnn7-speech:latest bash</span><br></pre></td></tr></table></figure>
 <p><font color="#989898">
 <ul>
 <li>`<随便设置_1>` & `<随便设置_2>` 按个人喜好自行更改，避免冲突</li>
 <li>我把所有的代码以及数据放在了server上个人文件夹`~/kxie/`里面</li>
 <li>之后打开Jupyter Notebook页面的时候由于映射关系，能看到所有数据以及代码！很方便！！</li>
 </ul>
 </font></p>
 
<p> 运行 <code>ternsorboard</code> &amp; <code>Jupyter Notebook</code>：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">#!/bin/bash -</span><br><span class="line">nohup tensorboard --host=0.0.0.0 --logdir=/data/log/kxie &amp;</span><br><span class="line">/opt/conda/bin/jupyter notebook  --ip=&#x27;*&#x27; --no-browser --allow-root --NotebookApp.token= --notebook-dir=&#x27;/shijiao&#x27;</span><br></pre></td></tr></table></figure>
<p> 这样就可以打开网页 <code>http://10.202.90.201:&lt;随便设置_1&gt;</code> 跑代码了！Jupyter的可视化还是很方便的，从本地copy paste也容易！</p>
<p> <strong>具体命令分析，可以看新工作笔记第二页</strong></p>
</li>
</ol>
<h2 id="docker-挂载"><a href="#docker-挂载" class="headerlink" title="docker 挂载"></a>docker 挂载</h2><p>前面是本地文件，后面是镜像文件：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus=<span class="number">3</span> -v <span class="regexp">/Users/</span>shijiao<span class="regexp">/Documents/</span>Bert<span class="regexp">/:/</span>shijiao/mkl conda-origin bash</span><br></pre></td></tr></table></figure>

<p><code>--cpus=8</code>设置的是cpu利用百分比，而不是个数，因为报错中，<code>Range of CPUs</code>是小数，而不是整数。</p>
<figure class="highlight subunit"><table><tr><td class="code"><pre><span class="line">sudo docker run -it --cpus=8 -v /Users/shijiao/Documents/Bert/:/shijiao/mkl conda-origin bash</span><br><span class="line"><span class="keyword">Error </span>response from daemon: Range of CPUs is from 0.01 to 6.00, as there are only 6 CPUs available.</span><br></pre></td></tr></table></figure>


<h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h3 id="想修改镜像内容怎么办？"><a href="#想修改镜像内容怎么办？" class="headerlink" title="想修改镜像内容怎么办？"></a>想修改镜像内容怎么办？</h3><p>进入docker镜像里面改：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion /bin/bash </span><br><span class="line">修改内容 # 然后退出并记住 ***Container ID***</span><br><span class="line">docker commit &lt;ID&gt; 10.202.107.19/sfai/lakers_champion</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">server上重新pull: sudo docker pull 10.202.107.19/sfai/lakers_champion</span><br></pre></td></tr></table></figure>
<p>e.g., 修改本地镜像，使得能够使用<code>fzf</code>:</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker run -it 10.202.107.19/sfai/lakers_champion</span><br><span class="line"># after enter into this image:</span><br><span class="line">cd</span><br><span class="line">git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf</span><br><span class="line">~/.fzf/install</span><br><span class="line">vi .vimrc</span><br><span class="line">set rtp+=~/.fzf</span><br><span class="line">exit # and remember container id</span><br><span class="line">docker commit &lt;container id&gt; 10.202.107.19/sfai/lakers_champion:latest</span><br><span class="line">docker push 10.202.107.19/sfai/lakers_champion:latest</span><br></pre></td></tr></table></figure>
<p><a href="https://github.com/junegunn/fzf">GitHub - junegunn&#x2F;fzf: A command-line fuzzy finder</a></p>
<p>将本地文件添加到镜像中：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker cp &lt;local source&gt; &lt;Container ID&gt;:&lt;dest&gt;</span><br><span class="line"># 然后同上commit</span><br></pre></td></tr></table></figure>
<p>Terminal过段时间就会中断 <code>timed out: Connection to 10.202.90.201 closed.</code> 如何重新进入正在后台运行的Docker？<br>(当然也有办法永不中断，自行Google)</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># ssh 01366808@10.202.90.201</span><br><span class="line">sudo docker exec -it &lt;ID&gt; bash</span><br></pre></td></tr></table></figure>

<p>可以用 <code>docker ps</code> 查看Docker进程以及ID</p>
<p>删除镜像：<code>sudo docker rmi &lt;ID&gt;</code></p>
<p>结束运行的容器：<code>sudo docker stop &lt;ID&gt;</code></p>
<p>修改镜像名字或者tag：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">docker tag &lt;old_name:old_tag&gt; &lt;new_name:new_tag&gt;</span><br><span class="line"># or</span><br><span class="line">docker tag &lt;ID&gt; &lt;new_name:new_tag&gt;</span><br></pre></td></tr></table></figure>
<h3 id="Sync"><a href="#Sync" class="headerlink" title="Sync"></a>Sync</h3><p><font color="#989898">好像server上不连网的，没法Git同步，影响效率！</font></p>

<ol>
<li><p>从本地scp到服务器上:</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">scp -r &lt;source&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>rsync</code>：</p>
 <figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">rsync -av -e ssh --exclude=&#x27;excluded dir&#x27; &lt;source&gt; 01366808@10.202.90.201:~/&lt;dest&gt;</span><br></pre></td></tr></table></figure>
<p> <code>--exclude=&#39;excluded dir&#39;</code>：本地<source>中有些目录的内容是你不想传到server上的</p>
 <p><font color="#989898">
 <ul>
 <li>一般情况：本地修改完代码，使用上面rysnc命令同步到server上 (或者简单点通过Jupyter copy paste)，然后跑吧！</li>
 <li>交换 source dest 位置实现反向同步</li>
 </ul>
 </font></p></li>
</ol>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>Elasticsearch 的安装与使用</title>
    <url>/2022/07/06/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/Elasticsearch-%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Elasticsearch-的使用"><a href="#Elasticsearch-的使用" class="headerlink" title="Elasticsearch 的使用"></a>Elasticsearch 的使用</h1><h2 id="ES-安装"><a href="#ES-安装" class="headerlink" title="ES 安装"></a>ES 安装</h2><h3 id="下载与安装"><a href="#下载与安装" class="headerlink" title="下载与安装"></a>下载与安装</h3><p>直接到官网下载文件，并放置某个路径 <code>path/</code> 下<br>如果需要启动，进入 <code>path/</code>，运行 <code>bin/elasticsearch</code> 即可</p>
<h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><h5 id="密码"><a href="#密码" class="headerlink" title="密码"></a>密码</h5><p>   可能遇到需要密码的问题。此时有两种方法：</p>
<ul>
<li>一个是进入 <code>elasticsearch.yml</code> 文件设置 <code>xpack.security.enabled: false</code>。不过这样有安全隐患，只能用于测试。且7.x的版本没有相关参数</li>
<li>二是设置一个密码<ul>
<li>运行 <code>bin/elasticserch-setup-passwords interactive</code> </li>
<li>数据密码 <code>xxxxx</code></li>
<li><code>curl http://host:port -u username:password</code> 验证是否成功</li>
</ul>
</li>
</ul>
<h5 id="内网访问"><a href="#内网访问" class="headerlink" title="内网访问"></a>内网访问</h5><p>   如果访问ES是在同一网络内另一台机器上，则需要设置 <code>elasticsearch.yml</code> 中的 <code>network.host</code> 为 <code>0.0.0.0</code> 允许内网不同机器访问</p>
<h2 id="ES-使用"><a href="#ES-使用" class="headerlink" title="ES 使用"></a>ES 使用</h2><h3 id="创建索引"><a href="#创建索引" class="headerlink" title="创建索引"></a>创建索引</h3><p>命令： </p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">PUT http:<span class="regexp">//</span>host:port/test123 </span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;settings&quot;</span>:&#123;</span><br><span class="line">        <span class="string">&quot;number_of_shards&quot;</span>:<span class="number">5</span>, <span class="regexp">//</span> 指定索引分片数量</span><br><span class="line">        <span class="string">&quot;number_of_replicas&quot;</span> : <span class="number">1</span> <span class="regexp">//</span> 指定索引副本分片数量</span><br><span class="line">    &#125;,</span><br><span class="line">    <span class="string">&quot;mappings&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;id&quot;</span>: &#123; <span class="regexp">//</span> 必须唯一</span><br><span class="line">                <span class="string">&quot;type&quot;</span>: <span class="string">&quot;long&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;classe&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;sentence&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span></span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;vector&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;type&quot;</span>: <span class="string">&quot;dense_vector&quot;</span>,</span><br><span class="line">                <span class="string">&quot;dims&quot;</span>: <span class="number">5</span></span><br><span class="line">            &#125;</span><br><span class="line">            <span class="regexp">//</span> ...</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="导入文档"><a href="#导入文档" class="headerlink" title="导入文档"></a>导入文档</h3><h4 id="导入单个文档"><a href="#导入单个文档" class="headerlink" title="导入单个文档"></a>导入单个文档</h4><p><code>id</code> 为 <code>3</code> 时：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">PUT http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_doc/<span class="number">3</span></span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;id&quot;</span>: <span class="number">3</span>,</span><br><span class="line">    <span class="string">&quot;sentence&quot;</span>: <span class="string">&quot;我是第三句中文&quot;</span>,</span><br><span class="line">    <span class="string">&quot;vector&quot;</span>: [-<span class="number">0.5</span>, <span class="number">0.5</span>, -<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">5</span>]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="批量导入文档"><a href="#批量导入文档" class="headerlink" title="批量导入文档"></a>批量导入文档</h4><p>批量导入主要使用 <code>bulk</code>，<code>Python</code> 和 <code>http</code> 都有接口</p>
<p>使用 <code>bulk</code> 一次性导入数据不能超过 <code>100MB</code></p>
<p>无论是 <code>Python</code> 还是 <code>http</code> ，都需要将每个文档转换成 <code>json</code> 格式导入。某个 <code>json</code> 文档具体格式如下：</p>
<figure class="highlight prolog"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;_index&quot;</span>: <span class="string">&quot;test123&quot;</span>,</span><br><span class="line">    <span class="string">&quot;_id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">    <span class="string">&quot;_source&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;id&quot;</span>: <span class="number">1</span>,</span><br><span class="line">        <span class="string">&quot;classe&quot;</span>: <span class="string">&quot;FAQ&quot;</span>,</span><br><span class="line">        <span class="string">&quot;sentence&quot;</span>: <span class="string">&quot;我是第一个文档&quot;</span>,</span><br><span class="line">        <span class="string">&quot;vector&quot;</span>: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span> <span class="number">4.0</span>, <span class="number">5.0</span>]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>若使用 <code>http</code> 批量导入，命令行： <code>curl -XPOST -H &quot;Content-Type:application/json&quot; &quot;http://127.0.0.1:9200/test123/_bulk?pretty&quot; --data-binary @example.json</code></p>
<figure class="highlight json"><table><tr><td class="code"><pre><span class="line"><span class="comment">// example.json</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test123&quot;</span><span class="punctuation">,</span><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">1</span><span class="punctuation">,</span><span class="attr">&quot;classe&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FAQ&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sentence&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我是第一个文档&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vector&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1.0</span><span class="punctuation">,</span> <span class="number">2.0</span><span class="punctuation">,</span> <span class="number">3.0</span> <span class="number">4.0</span><span class="punctuation">,</span> <span class="number">5.0</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test123&quot;</span><span class="punctuation">,</span><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span><span class="attr">&quot;classe&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FAQ&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sentence&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我是第二个文档&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vector&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1.0</span><span class="punctuation">,</span> <span class="number">2.0</span><span class="punctuation">,</span> <span class="number">3.0</span> <span class="number">4.0</span><span class="punctuation">,</span> <span class="number">5.0</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#123;</span><span class="attr">&quot;_index&quot;</span><span class="punctuation">:</span> <span class="string">&quot;test123&quot;</span><span class="punctuation">,</span><span class="attr">&quot;_id&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span><span class="attr">&quot;id&quot;</span><span class="punctuation">:</span> <span class="number">4</span><span class="punctuation">,</span><span class="attr">&quot;classe&quot;</span><span class="punctuation">:</span> <span class="string">&quot;FAQ&quot;</span><span class="punctuation">,</span><span class="attr">&quot;sentence&quot;</span><span class="punctuation">:</span> <span class="string">&quot;我是第四个文档&quot;</span><span class="punctuation">,</span><span class="attr">&quot;vector&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="number">1.0</span><span class="punctuation">,</span> <span class="number">2.0</span><span class="punctuation">,</span> <span class="number">3.0</span> <span class="number">4.0</span><span class="punctuation">,</span> <span class="number">5.0</span><span class="punctuation">]</span><span class="punctuation">&#125;</span><span class="punctuation">&#125;</span></span><br><span class="line"><span class="comment">// ...</span></span><br></pre></td></tr></table></figure>

<p>若使用 <code>Python</code> 批量导入，命令行 <code>helpers.bulk(es, json_text)</code> ，<code>json_text</code> 格式同上 <code>example.json</code></p>
<h3 id="查询文档"><a href="#查询文档" class="headerlink" title="查询文档"></a>查询文档</h3><h4 id="通过id查询文档"><a href="#通过id查询文档" class="headerlink" title="通过id查询文档"></a>通过id查询文档</h4><p><code>id</code> 为 <code>3</code> 时：</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">GET http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_doc/<span class="number">3</span>?pretty</span><br></pre></td></tr></table></figure>

<h4 id="通过计算余弦相似度检索文档"><a href="#通过计算余弦相似度检索文档" class="headerlink" title="通过计算余弦相似度检索文档"></a>通过计算余弦相似度检索文档</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">    <span class="string">&quot;script_score&quot;</span>: &#123;</span><br><span class="line">      <span class="string">&quot;query&quot;</span> : &#123;<span class="string">&quot;match_all&quot;</span>: &#123;&#125;&#125;,</span><br><span class="line">      <span class="string">&quot;script&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;source&quot;</span>: <span class="string">&quot;cosineSimilarity(params.query_vector, &#x27;vector&#x27;) + 1.0&quot;</span>, </span><br><span class="line">        <span class="string">&quot;params&quot;</span>: &#123;</span><br><span class="line">          <span class="string">&quot;query_vector&quot;</span>: [<span class="number">4</span>, <span class="number">3.4</span>, -<span class="number">0.2</span>, <span class="number">0.1</span>, -<span class="number">3</span>]  </span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="通过简单的字符串匹配查询文档"><a href="#通过简单的字符串匹配查询文档" class="headerlink" title="通过简单的字符串匹配查询文档"></a>通过简单的字符串匹配查询文档</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;match&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;sentence&quot;</span>: <span class="string">&quot;我想要查询的文档&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="通过阈值筛选"><a href="#通过阈值筛选" class="headerlink" title="通过阈值筛选"></a>通过阈值筛选</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;min_score&quot;</span>: <span class="number">0.5</span></span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="regexp">//</span> ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="每页展示的结果数（response-“hits”-“hits”-的数目）"><a href="#每页展示的结果数（response-“hits”-“hits”-的数目）" class="headerlink" title="每页展示的结果数（response[“hits”][“hits”]的数目）"></a>每页展示的结果数（response[“hits”][“hits”]的数目）</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;size&quot;</span>: <span class="number">50</span> <span class="regexp">//</span> 默认<span class="number">10000</span></span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="regexp">//</span> ...</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="通过某个item筛选"><a href="#通过某个item筛选" class="headerlink" title="通过某个item筛选"></a>通过某个item筛选</h4><p><code>item</code> 为 <code>enabled_status</code>，且 <code>enabled_status</code> 是<code>boolean</code> 时</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;bool&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;filter&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;item&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;enabled_status&quot;</span>: true</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>item</code> 为 <code>classe</code>，且 <code>classe</code> 包含关键词 <code>FAQ</code> 时:</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;item&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;classe&quot;</span>: <span class="string">&quot;FAQ&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>item</code> 为 <code>classe</code>，且 <code>classe</code> 包含多个关键词 <code>FAQ</code> <code>greeting</code> 时:</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;items&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;classe&quot;</span>: [<span class="string">&quot;FAQ&quot;</span>, <span class="string">&quot;greeting&quot;</span>]</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="余弦相似度-筛选条件-组合查询"><a href="#余弦相似度-筛选条件-组合查询" class="headerlink" title="余弦相似度+筛选条件 组合查询"></a>余弦相似度+筛选条件 组合查询</h4><figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">POST http:<span class="regexp">//</span><span class="number">127.0</span>.<span class="number">0.1</span>:<span class="number">9200</span><span class="regexp">/test123/</span>_search</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    <span class="string">&quot;size&quot;</span>: <span class="number">100</span>,</span><br><span class="line">    <span class="string">&quot;min_score&quot;</span>: <span class="number">1.5</span>,</span><br><span class="line">    <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;script_score&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;query&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;bool&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;filter&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;term&quot;</span>: &#123;</span><br><span class="line">                            <span class="string">&quot;enabled_status&quot;</span>: true</span><br><span class="line">                        &#125;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;script&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;scource&quot;</span>: <span class="string">&quot;cosineSimilarity(params.query_vector, &#x27;vector&#x27;) + 1.0&quot;</span>,</span><br><span class="line">                <span class="string">&quot;params&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;query_vector&quot;</span>: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>]</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title>Git 的使用</title>
    <url>/2022/01/06/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/Git-%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
    <content><![CDATA[<h1 id="Git-的使用"><a href="#Git-的使用" class="headerlink" title="Git 的使用"></a>Git 的使用</h1><h3 id="Git-常用命令"><a href="#Git-常用命令" class="headerlink" title="Git 常用命令"></a>Git 常用命令</h3><p>git diff 颜色：<br>git diff 时增删代码颜色相同想要更改颜色：git config –global color.ui true</p>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title>HTTP和gRPC的区别</title>
    <url>/2019/01/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/HTTP%E5%92%8CgRPC%E7%9A%84%E5%8C%BA%E5%88%AB/</url>
    <content><![CDATA[<h1 id="HTTP和gRPC的区别"><a href="#HTTP和gRPC的区别" class="headerlink" title="HTTP和gRPC的区别"></a>HTTP和gRPC的区别</h1><ol>
<li><code>http</code>和<code>grpc</code>都是通过<code>tcp</code>运行。</li>
<li>不同的是<code>grpc</code>会将信息转换成二进制格式，因此传输更快，所需内存更小，效率更高。<br>但是因为<code>grpc</code>中，双方都需要编写相同的<code>.proto</code>，彼此传输的格式要固定，因此要求相对更严格一些。而http则没有这种要求，双方并没有格式的约定，所以<code>http</code>也更大一些，速度相对更慢一些。</li>
<li><code>http</code>通过<code>&quot;\\r\\n&quot;</code>来分隔信息。<code>grpc</code>则要通过<code>.proto</code>文件里的<code>message</code>严格规范信息的数目、类型等。</li>
</ol>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>http</tag>
        <tag>grpc</tag>
      </tags>
  </entry>
  <entry>
    <title>JMETER-参数解析与压测步骤</title>
    <url>/2022/07/01/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/JMETER-%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%8E%8B%E6%B5%8B%E6%AD%A5%E9%AA%A4/</url>
    <content><![CDATA[<h1 id="JMETER-参数解析与压测步骤"><a href="#JMETER-参数解析与压测步骤" class="headerlink" title="JMETER-参数解析与压测步骤"></a>JMETER-参数解析与压测步骤</h1><h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><h3 id="解释"><a href="#解释" class="headerlink" title="解释"></a>解释</h3><p><img src="/2022/07/01/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/JMETER-%E5%8F%82%E6%95%B0%E8%A7%A3%E6%9E%90%E4%B8%8E%E5%8E%8B%E6%B5%8B%E6%AD%A5%E9%AA%A4/JMeter%E5%8F%82%E6%95%B0%E9%85%8D%E7%BD%AE%E7%A4%BA%E4%BE%8B.png" alt="参数展示"></p>
<ul>
<li>Number of Thread：该线程组包括的线程数,可理解为虚拟用户数</li>
<li>Ramp-up Period：即设置线程数在多少秒内启动完毕，即如果线程数设置为10，而此项设置也设置为5，那么会每隔10&#x2F;5&#x3D;2秒,启动一个线程.</li>
<li>Loop Count：即设置的线程数循环的次数，如果勾选永远，则会一直循环（注意：如果勾选了永远且调度器配置中设置了持续时间，则会在持续时间到达之后结束循环）.</li>
<li>Delay Thread creation until needed：此选项和Ramp-up 时间（秒）设置配合使用，举例说明：如过设置线程数为10，Ramp-up 时间（秒）时间为100，则如果不勾选此项则此次测试会每隔10s创建并启动一个线程，那么100s后会有1-10个线程在运行；但是如果勾选此项，那么线程组会每隔10s创建一个线程但并不启动，而是会等待10个线程都创建好之后同时启动.</li>
<li>Scheduler：勾选此项则打开调度器配置</li>
<li>duration：即本线程组测试的持续时间，到时间后则停止此次测试，注意这个时间设置不要设置的比Ramp-up 时间（秒）小，如果勾选了循环次数中的永远，那么测试一样会在此持续时间到达后结束；</li>
<li>start up delay：此项设置为在我们启动测试后多久时间开始创建线程组，通常用于定时</li>
<li>Listner：较常用的是View Results Tree和Aggregate Report</li>
<li>terminer中可以使用GUI，生成HTML报告，更方便分析，目前还没有弄通…</li>
</ul>
<h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><ol>
<li>先需要找到最大TPS(Transactions Per Second)，即JMETER结果表格中的Thoughout</li>
<li>每个事物如果只请求一次，则TPS&#x3D;QPS</li>
<li>因为实际线上会是多个用户同时请求，而非一个用户多次请求，所以压测是，我们主要通过逐步增加线程数（Number of Threads），模拟实际线上用户数的增加，来找到最高TPS</li>
<li>线程循环次数（loop count）无需过多改变和设置的太高，因为同一用户的调用次数往往是非常有限的</li>
</ol>
<h2 id="压测"><a href="#压测" class="headerlink" title="压测"></a>压测</h2><h3 id="Windows界面压测"><a href="#Windows界面压测" class="headerlink" title="Windows界面压测"></a>Windows界面压测</h3><p>简单的压测可以按如下步骤进行：</p>
<ol>
<li>设置第一次压测的线程数（Number of Threads），如20</li>
<li>设置Ramp-Up Period，可以根据线程数来设置，主要保证此参数要小于整体压测时间（Duration）</li>
<li>每个用户调用次数Loop Count可以设置成持续调用（Forever），并勾选Scheduler</li>
<li>Scheduler Configuration里主要要设置Duration，即压测持续时间。这个时间一定要大于线程启动时间（Ramp-Up Period），即【Duration】 - 【Ramp-Up Period】 &#x3D; 【所有线程启动后希望压测的时间】</li>
</ol>
<h3 id="Linux压测"><a href="#Linux压测" class="headerlink" title="Linux压测"></a>Linux压测</h3><ol>
<li>在有界面可视化的 <code>JMETER</code> 上（如 <code>Windows</code> 系统上），配置好需要压测的参数</li>
<li>储存成 <code>test.jmx</code> 文件</li>
<li>将 <code>test.jmx</code> 文件传输到 <code>Linux</code> 上</li>
<li>进入 <code>Linux</code> 上 <code>JMETER</code> 的 <code>bin</code> 文件夹下，运行 <code>jmeter -n -t /path/to/your/test.jmx -l /path/to/results/file.jtl</code></li>
<li>将 <code>file.jtl</code> 传到 <code>Windows</code> 上，导入 <code>JMETER</code> 中即可</li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>TPS、线程数、响应时间等之间的分析见博客<a href="https://www.cnblogs.com/happyliuyi/p/10755837.html">happy刘艺<br>性能测试入门（五）实践得出系统最佳线程数设置(压测最佳线程数&lt;真实设置的线程数量&lt;内存极限线程数)</a>和博客<a href="https://cloud.tencent.com/developer/article/1771176">【修正版】QPS、TPS、RT、并发数、吞吐量理解和性能优化深入思考</a></p>
<p>结果大概就一个表格，可以参见博客 <a href="https://blog.csdn.net/weixin_42579328/article/details/108243059">Jmeter使用教程（四）—测试结果分析</a> </p>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>JMETER</tag>
        <tag>压测</tag>
      </tags>
  </entry>
  <entry>
    <title>Linux 常用命令</title>
    <url>/2021/10/07/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/Linux-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
    <content><![CDATA[<h1 id="Linux-常用命令"><a href="#Linux-常用命令" class="headerlink" title="Linux 常用命令"></a>Linux 常用命令</h1><ol>
<li>快速查看端口占用情况<code>lsof -i tcp:8080</code>或者<code>netstat -nlp</code></li>
<li>查看PID具体信息<code>ll /proc/PID</code></li>
<li>同样，查看CPU和内存信息可以使用<code>cat /proc/cupinfo</code> 和 <code>cat /proc/meminfo</code>。更详细CPU信息查询可以使用<code>cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c </code>和<code>cat /proc/cpuinfo | grep physical | uniq -c</code></li>
<li>查看显卡信息可以使用命令<code>lspci | grep -i vga</code></li>
<li>查看磁盘使用情况<code>df -k</code>或<code>df -m</code>，分别以kb或mb为单位，查看磁盘使用量</li>
<li>GPU当前使用状况：<code>nvidia-smi</code></li>
<li>每10秒，更新一次GPU情况：<code>watch -n 10 nvidia-smi</code></li>
<li><code>free -h</code>看内存情况</li>
</ol>
<p>参考文章：<a href="https://cloud.tencent.com/developer/article/1721406">Linux系统查看CPU、机器型号、内存等信息</a></p>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title>protobuf</title>
    <url>/2019/01/16/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/protobuf/</url>
    <content><![CDATA[<h1 id="protobuf"><a href="#protobuf" class="headerlink" title="protobuf"></a>protobuf</h1><ul>
<li><code>protobuf</code>：把消息序列化的工具</li>
<li><code>rpc</code>： 远程方法调用</li>
<li><code>redis</code>：缓存</li>
<li><code>mysql</code>：持久储存</li>
</ul>
<h4 id="搭建微服务实例"><a href="#搭建微服务实例" class="headerlink" title="搭建微服务实例"></a>搭建微服务实例</h4><p><a href="https://blog.goodaudience.com/ml-client-server-using-grpc-in-python-3cba7693d1f5">ML Client&#x2F;Server Using gRPC in Python – Good Audience</a></p>
<h4 id="安装Protocol-Buffers："><a href="#安装Protocol-Buffers：" class="headerlink" title="安装Protocol Buffers："></a>安装Protocol Buffers：</h4><ol>
<li>ruby -e “$(curl -fsSL <a href="https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;">https://raw.githubusercontent.com/Homebrew/install/master/install)&quot;</a></li>
<li>brew install protobuf</li>
<li>安装gRPC：pip install grpcio</li>
<li>安装gRPC：pip install grpcio-tools</li>
</ol>
<span id="more"></span>

<h4 id="编译proto"><a href="#编译proto" class="headerlink" title="编译proto"></a>编译proto</h4><figure class="highlight jboss-cli"><table><tr><td class="code"><pre><span class="line">python -m grpc_tools.protoc -I <span class="string">./protos</span> <span class="params">--python_out=</span>. <span class="params">--grpc_python_out=</span>. <span class="string">./protos/intent.proto</span></span><br></pre></td></tr></table></figure>

<ul>
<li><code>-I</code><br>指定生成的<code>pb2.py</code>和<code>pb2_grpc.py</code>文件的储存路径</li>
<li><code>./protos/intent.proto</code><br>需要编译的<code>.proto</code>文件的位置</li>
</ul>
<p>如果文件原来的位置如下：</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="bullet">- </span>protos</span><br><span class="line"><span class="bullet">- </span>intent.proto</span><br><span class="line"><span class="bullet">- </span>client.py</span><br><span class="line"><span class="bullet">- </span>server.py</span><br><span class="line"><span class="bullet">- </span>predict.py</span><br></pre></td></tr></table></figure>
<p>在根目录上运行</p>
<figure class="highlight awk"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I .<span class="regexp">/protos/</span> --python_out=. --grpc_python_out=. .<span class="regexp">/protos/</span>price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="bullet">- </span>protos</span><br><span class="line"><span class="bullet">- </span>intent.proto</span><br><span class="line"><span class="bullet">- </span>client.py</span><br><span class="line"><span class="bullet">- </span>server.py</span><br><span class="line"><span class="bullet">- </span>predict.py</span><br><span class="line"><span class="bullet">- </span>intent<span class="emphasis">_pb2.py</span></span><br><span class="line"><span class="emphasis">- intent_pb2_</span>grpc.py</span><br></pre></td></tr></table></figure>

<p>在根目录上运行</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">python3 -m grpc_tools.protoc -I . <span class="attribute">--python_out</span>=. <span class="attribute">--grpc_python_out</span>=. ./protos/price.proto</span><br></pre></td></tr></table></figure>
<p>后会变成：</p>
<figure class="highlight asciidoc"><table><tr><td class="code"><pre><span class="line"><span class="bullet">- </span>protos</span><br><span class="line"><span class="bullet">- </span>intent.proto</span><br><span class="line"><span class="bullet">- </span>intent<span class="emphasis">_pb2.py</span></span><br><span class="line"><span class="emphasis">- intent_pb2_</span>grpc.py</span><br><span class="line"><span class="bullet">- </span>client.py</span><br><span class="line"><span class="bullet">- </span>server.py</span><br><span class="line"><span class="bullet">- </span>predict.py</span><br></pre></td></tr></table></figure>
<p>没弄明白为什么，不过为了方便函数之间的调用，一般使用第一种方法。</p>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>protobuf</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown基础语法</title>
    <url>/2020/02/19/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/Markdown%E5%9F%BA%E7%A1%80%E8%AF%AD%E6%B3%95/</url>
    <content><![CDATA[<h1 id="Markdown基础语法"><a href="#Markdown基础语法" class="headerlink" title="Markdown基础语法"></a>Markdown基础语法</h1><h2 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h2><ol>
<li>安装插件<code>Markdown All in One</code>，包含了最常用的<code>Markdown</code>优化。</li>
<li>安装插件<code>Markdown Preview Github Styling</code>，专门针对<code>github pages</code>的预览，功能有限，但是正是我需要的</li>
<li>如果不是针对<code>github</code>的预览，则可以安装<code>Markdown Preview Enhanced</code>，这个插件应用更普遍</li>
</ol>
<span id="more"></span>

<h2 id="Markdown语法"><a href="#Markdown语法" class="headerlink" title="Markdown语法"></a>Markdown语法</h2><ol>
<li><p><strong>标题</strong>：标题有共有六个等级，在前面加上一到六个 “#”</p>
</li>
<li><p><strong>正文</strong>：正文中想要换行，必须要多跳一行，如果在代码中只换一行，那么其实没有换行</p>
</li>
<li><p><strong>代码</strong>：正文中的代码块，在前后加上”&#96;”，如果是一段代码段落，前后分别加上”&#96;&#96;&#96;”</p>
<p> <strong>注：可以根据不同的语言配置不同的代码着色</strong></p>
 <figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wenzi</span>(<span class="params">self, n</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>列表</strong></p>
<p> 有序列表：输入数字 + 一个点”.” + 一个空格</p>
<p> 无序列表：输入”-“&#x2F;“*”&#x2F;“+” + 一个空格</p>
<ul>
<li>无序列表<ul>
<li>与前面的表示符号无关<ul>
<li>只与缩紧行数相关</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>加粗、倾斜和删除</strong></p>
<p> <strong>加粗</strong></p>
<p> <em>倾斜</em></p>
<p> <del>删除线</del></p>
</li>
<li><p><strong>引用</strong></p>
<blockquote>
<p>此处是引用</p>
</blockquote>
<blockquote>
<p>此处是多层引用第一层</p>
<p>此处是多层引用第二层</p>
</blockquote>
<blockquote>
<p>多层嵌套第一层</p>
<blockquote>
<p>多层嵌套第二层</p>
<blockquote>
<p>多层嵌套第三层</p>
</blockquote>
</blockquote>
</blockquote>
</li>
<li><p><strong>插入连接</strong></p>
<p> <a href="https://markdown-zh.readthedocs.io/en/latest/">Markdown中文文档</a></p>
</li>
<li><p><strong>插入图片</strong></p>
<ul>
<li>可以在图片上传到<code>github</code>后，用github上图片的地址链接，这样网页上可以正常显示</li>
<li>可以把包含图片的文件夹docs放到本地&#x2F;hexo&#x2F;public&#x2F;images里，用相对路径&#x2F;public&#x2F;images&#x2F;docs&#x2F;images.jpg来调用</li>
<li>更多方式参考<a href="https://fuhailin.github.io/Hexo-images/">这个文章</a></li>
</ul>
</li>
<li><p><strong>文字的个性设置</strong>：可以直接用html语法对正文进行编辑，达到想要的展示效果</p>
<ul>
<li><p>居中：</p>
  <center>这一行居中</center>
</li>
<li><p>换色 + 变化大小：</p>
<p>  接下来就是见证奇迹的时刻<br>  <font color="#989898"> 我爱用的注释颜色 </font><br>  <font color="#FF0000"> 我可以设置这一句的颜色哈哈 </font><br>  <font size=6> 我还可以设置这一句的大小嘻嘻 </font><br>  <font size=5 color="#FF0000"> 我甚至可以设置这一句的颜色和大小呵呵</font> </p>
</li>
<li><p>分段、分行<br>  分段：<code>&lt;p&gt;&lt;/p&gt;</code><br>  分行：<code>&lt;br&gt;</code></p>
</li>
<li><p>有序、无序列表</p>
  <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>无序列表<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">ol</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span>有序列表<span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p><strong>数学公式</strong>：主题目录下<code>/hexo/theme/next/_config.yml</code>设置<code>mathjax</code>里的<code>enable: true</code><br>并且在需要使用数学公式的博客里打开公式开关：</p>
<figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">title:</span> <span class="string">index.html</span></span><br><span class="line"><span class="attr">date:</span> <span class="number">2016-12-28 21:01:30</span></span><br><span class="line"><span class="attr">tags:</span></span><br><span class="line"><span class="attr">mathjax:</span> <span class="literal">true</span></span><br><span class="line"><span class="meta">---</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.w3cschool.cn/lme/q92a1srq.html">Markdown 语法简介</a></p>
]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
  <entry>
    <title>使用HEXO和GitHub搭建博客</title>
    <url>/2020/02/19/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/%E4%BD%BF%E7%94%A8HEXO%E5%92%8CGitHub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/</url>
    <content><![CDATA[<h1 id="使用HEXO和GitHub搭建博客"><a href="#使用HEXO和GitHub搭建博客" class="headerlink" title="使用HEXO和GitHub搭建博客"></a>使用HEXO和GitHub搭建博客</h1><h3 id="基本需求"><a href="#基本需求" class="headerlink" title="基本需求"></a>基本需求</h3><ol>
<li><p><code>github</code>上创建仓库，仓库名必须严格是<code>username.github.io</code></p>
</li>
<li><p>创建本地写博客的hexo文件夹MyHexo，然后进入文件夹，执行命令行<code>hexo init</code>，如果报错，会提示输入<code>npm install hexo --save</code></p>
</li>
<li><p>进入根目录下的<code>_config.yml</code>，增加</p>
 <figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">deploy:</span></span><br><span class="line"><span class="symbol">  type:</span> git</span><br><span class="line"><span class="symbol">  repository:</span> git@github.com:username/username.github.io.git</span><br><span class="line"><span class="symbol">  branch:</span> master</span><br></pre></td></tr></table></figure>
</li>
<li><p>执行命令<code>hexo g</code>，如果报错，一般是因为没有安装<code>git</code>，执行<code>npm install hexo-deployer-git --save</code>安装<code>hexo</code>下的<code>git</code>，然后重新<code>hexo g</code></p>
</li>
<li><p>执行<code>hexo d</code>，如果没有关联<code>git</code>和<code>hexo</code>，会自动提醒输入</p>
 <figure class="highlight rust"><table><tr><td class="code"><pre><span class="line">Username <span class="keyword">for</span> <span class="symbol">&#x27;https</span>:<span class="comment">//github.com&#x27;:</span></span><br><span class="line">Password <span class="keyword">for</span> <span class="symbol">&#x27;https</span>:<span class="comment">//github.com&#x27;:</span></span><br></pre></td></tr></table></figure>
<p> 这里我用了<code>ssh</code>钥匙。</p>
<ul>
<li>执行命令<code>ssh-keygen -t rsa -C github_de_email@gmail.com</code></li>
<li>进入<code>~/.ssh</code>复制文件<code>id_rsa.pub</code>里的公匙，进入到<code>github</code>里的<code>setting</code>，进入<code>SSH and GPG keys</code>，新建SSH钥匙<code>new SSH key</code>，粘贴公匙，<code>title</code>可以随意取或者不取，然后确定</li>
<li>运行<code>ssh -T git@github.com</code>，如果看到<code>Hi username! You&#39;ve successfully authenticated, but GitHub does not provide shell access.</code>证明SSH连接成功。</li>
</ul>
</li>
<li><p>自己博客的域名为 <a href="https://shijiaod.github.io/">https://shijiaod.github.io/</a><br>有时<code>hexo d</code>后页面没有变化，可以尝试重启电脑…</p>
</li>
</ol>
<span id="more"></span>

<h4 id="踩过的坑之血与泪的教训"><a href="#踩过的坑之血与泪的教训" class="headerlink" title="踩过的坑之血与泪的教训"></a>踩过的坑之血与泪的教训</h4><ol>
<li><p><strong>⚠️ 注 ⚠️</strong>：硬是弄了一天才发现，是仓库名见错了，所以怎么<code>hexo d</code>都报错，说<code>ERROR: Repository not found.</code>，后来发现仓库名后面应该跟上<code>github.io</code>，即完整仓库名应该是<code>username.github.io</code>而不是<code>username</code>！</p>
</li>
<li><p>当时在<code>hexo d</code>一直报错要输入<code>username</code>和<code>password</code>时，尝试使用<code>git remote add origin</code>来连接<code>github</code>，但是更换到正确的仓库名后，发现自己删除掉以前建立的<code>origin</code>也不影响，可见这里其实不需要更多的步骤</p>
</li>
<li><p>执行命令<code>ssh -T git@github.com</code>后出现</p>
 <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">The authenticity of host <span class="string">&#x27;github.com (13.229.188.59)&#x27;</span> can<span class="string">&#x27;t be established.</span></span><br><span class="line"><span class="string">RSA key fingerprint is SHA256:nThbg6kXUpJWGl7E1IGOCspRomTxdCARLviKw6E5SY8.</span></span><br><span class="line"><span class="string">Are you sure you want to continue connecting (yes/no)?</span></span><br></pre></td></tr></table></figure>
<p> 是因为没有配置<code>ssh</code>，或者本地缺少一个文件夹（什么文件夹我也不清楚…）<br> 如果是没有配置<code>ssh</code>见上方如何配置，如果是后者，输入<code>yes</code>，而非回车或者<code>y</code></p>
</li>
<li><p>在升级npm，安装hexo下的git，重新安装hexo时，都会报错</p>
 <figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">No receipt <span class="keyword">for</span> <span class="string">&#x27;com.apple.pkg.CLTools_Executables&#x27;</span> found at <span class="string">&#x27;/&#x27;</span>.</span><br><span class="line"></span><br><span class="line">No receipt <span class="keyword">for</span> <span class="string">&#x27;com.apple.pkg.DeveloperToolsCLILeo&#x27;</span> found at <span class="string">&#x27;/&#x27;</span>.</span><br><span class="line"></span><br><span class="line">No receipt <span class="keyword">for</span> <span class="string">&#x27;com.apple.pkg.DeveloperToolsCLI&#x27;</span> found at <span class="string">&#x27;/&#x27;</span>.</span><br><span class="line"></span><br><span class="line">gyp: No Xcode or CLT version detected!</span><br><span class="line">gyp ERR! configure error</span><br><span class="line">gyp ERR! stack Error: `gyp` failed with <span class="built_in">exit</span> code: 1</span><br><span class="line">gyp ERR! stack     at ChildProcess.onCpExit (/usr/local/lib/node_modules/npm/node_modules/node-gyp/lib/configure.js:351:16)</span><br><span class="line">gyp ERR! stack     at ChildProcess.emit (events.js:210:5)</span><br><span class="line">gyp ERR! stack     at Process.ChildProcess._handle.onexit (internal/child_process.js:272:12)</span><br><span class="line">gyp ERR! System Darwin 19.3.0</span><br><span class="line">gyp ERR! <span class="built_in">command</span> <span class="string">&quot;/usr/local/bin/node&quot;</span> <span class="string">&quot;/usr/local/lib/node_modules/npm/node_modules/node-gyp/bin/node-gyp.js&quot;</span> <span class="string">&quot;rebuild&quot;</span></span><br><span class="line">gyp ERR! cwd /Users/shijiaodeng/Documents/MyBlog/shijiaod/node_modules/nunjucks/node_modules/fsevents</span><br><span class="line">gyp ERR! node -v v12.13.1</span><br><span class="line">gyp ERR! node-gyp -v v5.0.5</span><br><span class="line">gyp ERR! not ok</span><br></pre></td></tr></table></figure>
<p> 是升级了mac的原因（到版本10.15.3），所以下载并安装Xcode并同意它的相关协议，问题就解决了。具体详见<a href="https://segmentfault.com/a/1190000021394623?utm_source=tag-newest">这片文章</a></p>
</li>
<li><p>报错404<br>一般404会是在初建博客是，因为仓库名设置的问题（见 1.）导致。<br>后期还碰到404，在检查仓库名没有问题+本地localhost:4000展示完全正常后，找到了可能是 <code>Source</code> 选择的问题。</p>
<ul>
<li>进入 <code>Settings</code>， 点击左边列表中的 <code>Pages</code> 进入 <code>GitHub Pages</code></li>
<li>看下方的 <code>Source</code> 是否为空 <img src="/2020/02/19/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/%E4%BD%BF%E7%94%A8HEXO%E5%92%8CGitHub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/Soure%E8%AE%BE%E7%BD%AE%E5%9B%BE1.png" alt="Soure设置图1"></li>
<li>完成设置并点击 <code>Save</code> ，过一会刷新页面即可 <img src="/2020/02/19/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%92%8C%E7%B3%BB%E7%BB%9F/%E4%BD%BF%E7%94%A8HEXO%E5%92%8CGitHub%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A2/Soure%E8%AE%BE%E7%BD%AE%E5%9B%BE2.png" alt="Soure设置图2"></li>
</ul>
</li>
</ol>
<h3 id="内容更丰富的博客"><a href="#内容更丰富的博客" class="headerlink" title="内容更丰富的博客"></a>内容更丰富的博客</h3><h4 id="博客主题"><a href="#博客主题" class="headerlink" title="博客主题"></a>博客主题</h4><ol>
<li><p><code>hexo</code>根目录下安装主题nexT<code>git clone https://github.com/theme-next/hexo-theme-next themes/next</code></p>
</li>
<li><p><code>hexo</code>根目录下修改文件<code>_config.yml</code>中的主题为<code>nexT</code>：<code>theme: next</code></p>
</li>
<li><p>到next主题下更改配置文件<code>/hexo/theme/next/_config.yml</code>中的<code>scheme: pisce</code>，里面有四种主题可以选，<code>pisce</code>是经典的旁边有小栏框的格式</p>
</li>
<li><p>我喜欢把<code>sidebar</code>放在右边:<code>/hexo/theme/next/_config.yml</code>中改为</p>
 <figure class="highlight yml"><table><tr><td class="code"><pre><span class="line"><span class="attr">sidebar:</span></span><br><span class="line"><span class="comment"># Sidebar Position.</span></span><br><span class="line"><span class="comment"># position: left</span></span><br><span class="line"><span class="attr">position:</span> <span class="string">right</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>进入主题目录下的languages文件夹中，<code>cp zh-CN.yml zh-Hans.yml</code>，然后再进入<code>hexo</code>根目录下修改<strong>语言</strong>、名字等一些基本信息（我也不知道为什么要改成<code>zh-Hans</code>而不是直接用<code>zh-CN</code>…）</p>
 <figure class="highlight vbnet"><table><tr><td class="code"><pre><span class="line"># Site</span><br><span class="line"><span class="symbol">title:</span> 诗娇的博客</span><br><span class="line"><span class="symbol">subtitle:</span> <span class="comment">&#x27;&#x27;</span></span><br><span class="line"><span class="symbol">description:</span> <span class="comment">&#x27;&#x27;</span></span><br><span class="line"><span class="symbol">keywords:</span></span><br><span class="line"><span class="symbol">author:</span> Shijiao DENG</span><br><span class="line"><span class="symbol">language:</span> zh-Hans</span><br><span class="line"><span class="symbol">timezone:</span> <span class="comment">&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>在<code>/hexo/theme/next/_config.yml</code>中配置<code>avatar</code>设置图像</p>
 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">avatar:</span></span><br><span class="line">  <span class="attr">url:</span> <span class="comment">#/images/avatar.gif  #头像图片路径 图片放置在hexo/public/images</span></span><br><span class="line">  <span class="attr">rounded:</span> <span class="literal">false</span>  <span class="comment">#是否显示圆形头像，true表示圆形，false默认</span></span><br><span class="line">  <span class="attr">opacity:</span> <span class="number">0.7</span>  <span class="comment">#透明度0~1之间</span></span><br><span class="line">  <span class="attr">rotated:</span> <span class="literal">false</span>  <span class="comment">#是否旋转 true表示旋转，false默认</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h4><ol>
<li><p>进入主题目录下的<code>_config.yml</code>修改主页目录，可以更改配置里的上下顺序来更改他们在主页各自排列的顺序，比如<code>about</code>本来在<code>home</code>下面，我们把它移到了最下面</p>
 <figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">menu:</span></span><br><span class="line"><span class="symbol">  home:</span> / || home</span><br><span class="line">  <span class="meta">#about: /about/ || user</span></span><br><span class="line"><span class="symbol">  tags:</span> <span class="keyword">/tags/</span> || tags</span><br><span class="line"><span class="symbol">  categories:</span> <span class="keyword">/categories/</span> || th</span><br><span class="line"><span class="symbol">  archives:</span> <span class="keyword">/archives/</span> || archive</span><br><span class="line"><span class="symbol">  schedule:</span> <span class="keyword">/schedule/</span> || calendar</span><br><span class="line">  <span class="meta">#sitemap: /sitemap.xml || sitemap</span></span><br><span class="line">  <span class="meta">#commonweal: /404/ || heartbeat</span></span><br><span class="line"><span class="symbol">  about:</span> <span class="keyword">/about/</span> || user</span><br></pre></td></tr></table></figure>
<p> 也可以自己增加一个目录内容，不过这些配置都要与主题目录下的languages文件中对应的<code>yml</code>文档里配置相关联。比如你在站点根目录中的配置文件设置<code>language</code>为<code>zh-Hans</code>，那么就要进入到主题目录下的languages文件中修改<code>zh-Hans.yml</code>，这样才能显示出菜单项新增的中文内容（以something为例子）</p>
</li>
<li><p>前面通过修改next主题下的<code>_config.yml</code>文件中的<code>menu</code>选项，可以在主页面的菜单栏添加”标签”选项，但是此时点击”标签”，跳转的页面会显示”page not found”。此时我们要新建一个页面</p>
<p> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A201.png" alt="图一"><br> 在新建的<code>index.md</code>文件中添加<code>type: &quot;tags&quot;</code><br> <img src="/images/image/hexo%E6%90%AD%E5%BB%BA%E5%8D%9A%E5%AE%A202.png" alt="图二"><br> <code>categories</code>等同理</p>
</li>
<li><p>设置头像：</p>
 <figure class="highlight gradle"><table><tr><td class="code"><pre><span class="line">avatar:</span><br><span class="line">  url: <span class="regexp">/images/</span>avatar.gif  #头像图片路径 图片放置在<span class="keyword">next</span><span class="regexp">/source/im</span>ages</span><br><span class="line">  rounded: <span class="keyword">false</span>  #是否显示圆形头像，<span class="keyword">true</span>表示圆形，<span class="keyword">false</span>默认</span><br><span class="line">  opacity: <span class="number">0.7</span>  #透明度<span class="number">0</span>~<span class="number">1</span>之间</span><br><span class="line">  rotated: <span class="keyword">false</span>  #是否旋转 <span class="keyword">true</span>表示旋转，<span class="keyword">false</span>默认</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>/hexo/theme/next/_config.yml</code>的<code>menu_settings</code>如果设置<code>icon: false</code>则无前面的图标，<code>badges: true</code>则标签都会显示数字</p>
</li>
<li><p>社交设置<code>/hexo/theme/next/_config.yml</code>里的<code>social</code></p>
</li>
<li><p>自动在文章中生成目录，在文章最前面加入<code>[toc]</code>，具体设置如下，但自己试了没有效果</p>
 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">toc:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">false</span> <span class="comment">#是否启动侧边栏</span></span><br><span class="line">  <span class="attr">number:</span> <span class="literal">true</span>  <span class="comment">#自动将列表编号添加到toc。</span></span><br><span class="line">  <span class="attr">wrap:</span> <span class="literal">false</span> <span class="comment">#true时是当标题宽度很长时，自动换到下一行</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="添加站内搜索"><a href="#添加站内搜索" class="headerlink" title="添加站内搜索"></a>添加站内搜索</h4><ol>
<li>安装站内搜索插件 <figure class="highlight ada"><table><tr><td class="code"><pre><span class="line">npm install hexo-generator-searchdb <span class="comment">--save</span></span><br></pre></td></tr></table></figure></li>
<li>在根目录下的<code>_config.yml</code>添加 <figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">search:</span></span><br><span class="line"><span class="symbol">  path:</span> search.xml</span><br><span class="line"><span class="symbol">  field:</span> post</span><br><span class="line"><span class="symbol">  format:</span> html</span><br><span class="line"><span class="symbol">  limit:</span> <span class="number">10000</span></span><br></pre></td></tr></table></figure></li>
<li>在<code>themes/next/_config.yml</code>文件中搜索<code>local_search</code>,进行设置 <figure class="highlight yaml"><table><tr><td class="code"><pre><span class="line"><span class="attr">local_search:</span></span><br><span class="line">  <span class="attr">enable:</span> <span class="literal">true</span>  <span class="comment"># 设置为true</span></span><br><span class="line">  <span class="attr">trigger:</span> <span class="string">auto</span>  <span class="comment"># auto /  manual，auto 自动搜索、manual：按回车[enter ]键手动搜索</span></span><br><span class="line">  <span class="attr">top_n_per_article:</span> <span class="number">3</span> <span class="comment"># 每篇博客显示搜索的结果数</span></span><br><span class="line">  <span class="attr">unescape:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure></li>
</ol>
<h4 id="不显示整篇文章"><a href="#不显示整篇文章" class="headerlink" title="不显示整篇文章"></a>不显示整篇文章</h4><p>在文章中想要显示的部分后面加上<code>&lt;!--more--&gt;</code>，从这里开始，后面文章会隐藏，只会显示”阅读全文”按钮</p>
<h4 id="更多惊喜"><a href="#更多惊喜" class="headerlink" title="更多惊喜"></a>更多惊喜</h4><p>还有很多细节，可参见以下两个网站：</p>
<p><a href="https://www.jianshu.com/p/3a05351a37dc">网站一</a></p>
<p><a href="https://eirunye.github.io/2018/09/15/Hexo%E6%90%AD%E5%BB%BAGitHub%E5%8D%9A%E5%AE%A2%E2%80%94%E6%89%93%E9%80%A0%E7%82%AB%E9%85%B7%E7%9A%84NexT%E4%B8%BB%E9%A2%98%E2%80%94%E9%AB%98%E7%BA%A7%E2%80%94%E5%9B%9B/#more">网站二</a></p>
<h2 id="如何把本地写好的-md文件推倒github网站上"><a href="#如何把本地写好的-md文件推倒github网站上" class="headerlink" title="如何把本地写好的.md文件推倒github网站上"></a>如何把本地写好的.md文件推倒github网站上</h2><h3 id="传统方式"><a href="#传统方式" class="headerlink" title="传统方式"></a>传统方式</h3><ul>
<li>进入到根目录（<code>/Documents/xxxx.github.io/</code>）下，输入命令行<code>hexo g</code> (<code>hexo generate</code>)生成静态页面</li>
<li>输入命令<code>hexo d</code> (<code>hexo deploy</code>)上传到<code>github</code>上<br>这时就可以在网站上看到刚刚写的博客了。<br>如果想要先本地预览效果，可以使用<code>hexo s</code> (<code>hexo server</code>)</li>
</ul>
<h3 id="针对我个人的本地编写文件并推到github上"><a href="#针对我个人的本地编写文件并推到github上" class="headerlink" title="针对我个人的本地编写文件并推到github上"></a>针对我个人的本地编写文件并推到github上</h3><ul>
<li>进入到（<code>/Documents/MyBlog/hexo</code>）下</li>
<li>执行 <code>hexo new &quot;name of file&quot;</code>，生成新的文件</li>
<li>对文件进行编辑</li>
<li>输入命令行 <code>hexo g</code></li>
<li>输入命令 <code>hexo d</code></li>
</ul>
<h3 id="绑定域名"><a href="#绑定域名" class="headerlink" title="绑定域名"></a>绑定域名</h3><h3 id="添加评论等"><a href="#添加评论等" class="headerlink" title="添加评论等"></a>添加评论等</h3><h3 id="增加归档页面文章数目"><a href="#增加归档页面文章数目" class="headerlink" title="增加归档页面文章数目"></a>增加归档页面文章数目</h3>]]></content>
      <categories>
        <category>常用软件和系统</category>
      </categories>
      <tags>
        <tag>hexo</tag>
        <tag>github</tag>
        <tag>博客</tag>
      </tags>
  </entry>
  <entry>
    <title>SVM核函数本质</title>
    <url>/2020/02/19/%E6%95%B0%E5%AD%A6/SVM%E6%A0%B8%E5%87%BD%E6%95%B0%E6%9C%AC%E8%B4%A8/</url>
    <content><![CDATA[<h1 id="SVM核函数本质"><a href="#SVM核函数本质" class="headerlink" title="SVM核函数本质"></a>SVM核函数本质</h1><p>核函数的本质</p>
<ol>
<li>实际中，我们会经常遇到线性不可分的样例，此时，我们的常用做法是把样例特征映射到高维空间中去(如上文2.2节最开始的那幅图所示，映射到高维空间后，相关特征便被分开了，也就达到了分类的目的)；</li>
<li>但进一步，如果凡是遇到线性不可分的样例，一律映射到高维空间，那么这个维度大小是会高到可怕的(如上文中19维乃至无穷维的例子)。那咋办呢？</li>
<li>此时，核函数就隆重登场了，核函数的价值在于它虽然也是讲特征进行从低维到高维的转换，但核函数绝就绝在它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就如上文所说的避免了直接在高维空间中的复杂计算。</li>
</ol>
<p><a href="https://blog.csdn.net/u010199356/article/details/88836026">核函数解释博客</a><br>很有意思，有空可以自己试试用matlab画图看看</p>
<p><a href="https://zhuanlan.zhihu.com/p/77750026">SVM推导过程</a><br>也可以看看李航的统计书里的推导</p>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>核函数</tag>
        <tag>svm</tag>
      </tags>
  </entry>
  <entry>
    <title>斐波那契数列</title>
    <url>/2020/07/07/%E6%95%B0%E5%AD%A6/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/</url>
    <content><![CDATA[<h1 id="斐波那契数列"><a href="#斐波那契数列" class="headerlink" title="斐波那契数列"></a>斐波那契数列</h1><p>斐波那契数列（Fibonacci sequence），又称黄金分割数列、因数学家列昂纳多·斐波那契（Leonardoda Fibonacci）以兔子繁殖为例子而引入，故又称为“兔子数列”，指的是这样一个数列：$1、1、2、3、5、8、13、21、34、……$在数学上，斐波那契数列以如下被以递推的方法定义：$F(1)&#x3D;1$，$F(2)&#x3D;1$, $F(n)&#x3D;F(n - 1)+F(n - 2)（n ≥ 3，n ∈ N*）$在现代物理、准晶体结构、化学等领域，斐波纳契数列都有直接的应用，为此，美国数学会从 1963 年起出版了以《斐波纳契数列季刊》为名的一份数学杂志，用于专门刊载这方面的研究成果。</p>
<p><b>斐波那契弧线<b></p>
<p><img src="/2020/07/07/%E6%95%B0%E5%AD%A6/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91.jpeg" alt="fibonacci"></p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#################### 输出第n个斐波那契数 ####################</span></span><br><span class="line"><span class="meta">@fn_timer</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fibonacci1</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="comment">## 这种方法，每次都要嵌套循环，不能记录上一次的结果之间用，所以时间复杂度非常高！！</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> n == <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> fib(n-<span class="number">1</span>) + fib(n-<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> fib(n)</span><br><span class="line"></span><br><span class="line"><span class="meta">@fn_timer</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fibonacci2</span>(<span class="params">n</span>):</span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    i = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &lt;= n:</span><br><span class="line">        l.append(l[i-<span class="number">1</span>]+l[i-<span class="number">2</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> l[i-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fibonacci1(<span class="number">20</span>))</span><br><span class="line"><span class="built_in">print</span>(fibonacci2(<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#################### 输出前n个斐波那契数 ####################</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fibonacci</span>(<span class="params">n</span>):</span><br><span class="line">    l = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    i = <span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; n:</span><br><span class="line">        l.append(l[i-<span class="number">1</span>]+l[i-<span class="number">2</span>])</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> l</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fibonacci(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>斐波那契数列</tag>
        <tag>黄金分割</tag>
        <tag>兔子数列</tag>
      </tags>
  </entry>
  <entry>
    <title>欧几里得算法--求最大公约数</title>
    <url>/2020/07/07/%E6%95%B0%E5%AD%A6/%E6%AC%A7%E5%87%A0%E9%87%8C%E5%BE%97%E7%AE%97%E6%B3%95-%E6%B1%82%E6%9C%80%E5%A4%A7%E5%85%AC%E7%BA%A6%E6%95%B0/</url>
    <content><![CDATA[<h1 id="欧几里得算法"><a href="#欧几里得算法" class="headerlink" title="欧几里得算法"></a>欧几里得算法</h1><p>欧几里德算法是用来求两个正整数最大公约数的算法。是由古希腊数学家欧几里德在其著作《The Elements》中最早描述了这种算法,所以被命名为欧几里德算法。扩展欧几里德算法可用于RSA加密等领域。</p>
<p>假如需要求 1997 和 615 两个正整数的最大公约数,用欧几里德算法，是这样进行的：</p>
<figure class="highlight basic"><table><tr><td class="code"><pre><span class="line"><span class="symbol">1997 </span>/ <span class="number">615</span> = <span class="number">3</span> (余 <span class="number">152</span>)</span><br><span class="line"><span class="symbol">615 </span>/ <span class="number">152</span> = <span class="number">4</span>(余<span class="number">7</span>)</span><br><span class="line"><span class="symbol">152 </span>/ <span class="number">7</span> = <span class="number">21</span>(余<span class="number">5</span>)</span><br><span class="line"><span class="symbol">7 </span>/ <span class="number">5</span> = <span class="number">1</span> (余<span class="number">2</span>)</span><br><span class="line"><span class="symbol">5 </span>/ <span class="number">2</span> = <span class="number">2</span> (余<span class="number">1</span>)</span><br><span class="line"><span class="symbol">2 </span>/ <span class="number">1</span> = <span class="number">2</span> (余<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<p>至此，最大公约数为1</p>
<p>以除数和余数反复做除法运算，当余数为 0 时，取当前算式除数为最大公约数，所以就得出了 1997 和 615 的最大公约数 1。</p>
<h3 id="计算证明"><a href="#计算证明" class="headerlink" title="计算证明"></a>计算证明</h3><p>其计算原理依赖于下面的定理：</p>
<p>定理：两个整数的最大公约数等于其中较小的那个数和两数相除余数的最大公约数。最大公约数（Greatest Common Divisor）缩写为GCD。<br>$gcd(a,b) &#x3D; gcd(b,a \mod b)$ (不妨设$a&gt;b$ 且$r&#x3D;a \mod b$ ,$r$不为$0$)</p>
<h4 id="证法一"><a href="#证法一" class="headerlink" title="证法一"></a>证法一</h4><ul>
<li><p>正</p>
<ul>
<li>$a$可以表示成$a &#x3D; kb + r$（$a$，$b$，$k$，$r$皆为正整数，且$r&lt;b$），则$r &#x3D; a \mod b$</li>
<li>假设$d$是$a$,$b$的一个公约数，记作$d|a$,$d|b$，即$a$和$b$都可以被$d$整除。</li>
<li>而$r &#x3D; a - kb$，两边同时除以$d$，$r&#x2F;d&#x3D;a&#x2F;d-kb&#x2F;d&#x3D;m$，由等式右边可知$m$为整数，因此$d|r$</li>
<li>因此$d$也是$b$,$a \mod b$的公约数</li>
</ul>
</li>
<li><p>反</p>
<ul>
<li>假设$d$是$b$,$a \mod b$的公约数, 则$d|b$,$d|(a-k*b)$,$k$是一个整数。</li>
<li>进而$d|a$，因此$d$也是$a$,$b$的公约数</li>
<li>因此$(a,b)$和$(b,a \mod b)$的公约数是一样的，其最大公约数也必然相等</li>
</ul>
</li>
<li><p>得证</p>
</li>
</ul>
<h4 id="证法二"><a href="#证法二" class="headerlink" title="证法二"></a>证法二</h4><p>假设c &#x3D; gcd(a,b),则存在m,n，使a &#x3D; mc, b &#x3D; nc;<br>令r &#x3D; a mod b，即存在k，使r &#x3D; a-kb &#x3D; mc - knc &#x3D; (m-kn)c;<br>故gcd(b,a mod b) &#x3D; gcd(b,r) &#x3D; gcd(nc,(m-kn)c) &#x3D; gcd(n,m-kn)*c;<br>假设d &#x3D; gcd(n,m-kn), 则存在x,y, 使n &#x3D; xd, m-kn &#x3D; yd; 故m &#x3D; yd+kn &#x3D; yd+kxd &#x3D; (y+kx)d;<br>故有a &#x3D; mc &#x3D; (y+kx)dc, b &#x3D; nc &#x3D; xdc; 可得 gcd(a,b) &#x3D; gcd((y+kx)dc,xdc) &#x3D; dc;<br>由于gcd(a,b) &#x3D; c, 故d &#x3D; 1;<br>即gcd(n,m-kn) &#x3D; 1, 故可得gcd(b,a mod b) &#x3D; c;<br>故得证gcd(a,b) &#x3D; gcd(b,a mod b).<br>注意:两种方法是有区别的。</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p>自己写的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gcd</span>(<span class="params">a, b</span>):</span><br><span class="line">    c = a%b</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">print</span>(b) <span class="keyword">if</span> c==<span class="number">0</span> <span class="keyword">else</span> gcd(b, c)</span><br><span class="line"></span><br><span class="line">gcd(<span class="number">1997</span>, <span class="number">615</span>)</span><br></pre></td></tr></table></figure>

<p>百度百科上的</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">gcd</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">while</span> a != <span class="number">0</span>:</span><br><span class="line">        a, b = b % a, a</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">print</span>(b)</span><br><span class="line"></span><br><span class="line">gcd(<span class="number">1997</span>, <span class="number">615</span>)</span><br></pre></td></tr></table></figure>

<h2 id="扩展欧几里得算法"><a href="#扩展欧几里得算法" class="headerlink" title="扩展欧几里得算法"></a>扩展欧几里得算法</h2><p>扩展欧几里得算法（英语：Extended Euclidean algorithm）是欧几里得算法（又叫辗转相除法）的扩展。已知整数$a$、$b$，扩展欧几里得算法可以在求得$a$、$b$的最大公约数的同时，能找到整数$x$、$y$（其中一个很可能是负数），使它们满足贝祖等式</p>
<p>$ax+by&#x3D;gcd(a,b)$</p>
<p>如果a是负数，可以把问题转化成$|a|(-x)+by&#x3D;gcd(|a|,b)$，然后令$x^{‘}&#x3D;(-x)$。</p>
<p>扩展欧几里得算法可以用来计算模反元素(也叫模逆元)，而模反元素在RSA加密算法中有举足轻重的地位。</p>
<h3 id="代码-1"><a href="#代码-1" class="headerlink" title="代码"></a>代码</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ext_euclid</span>(<span class="params">a, b</span>):     </span><br><span class="line">    <span class="keyword">if</span> b == <span class="number">0</span>:         </span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span>, <span class="number">0</span>, a     </span><br><span class="line">    <span class="keyword">else</span>:         </span><br><span class="line">        x, y, q = ext_euclid(b, a % b) </span><br><span class="line">        <span class="comment"># q = gcd(a, b) = gcd(b, a%b)   </span></span><br><span class="line"></span><br><span class="line">        <span class="comment">## 这一行怎么求的，有时间可以论证一下      </span></span><br><span class="line">        x, y = y, (x - (a // b) * y)     </span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x, y, q</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>欧几里得</tag>
        <tag>最大公约数</tag>
      </tags>
  </entry>
  <entry>
    <title>牛顿法--优化算法</title>
    <url>/2020/07/07/%E6%95%B0%E5%AD%A6/%E7%89%9B%E9%A1%BF%E6%B3%95--%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h1 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h1><p>牛顿迭代法（Newton’s method）又称为牛顿-拉夫逊（拉弗森）方法（Newton-Raphson method），它是牛顿在17世纪提出的一种在实数域和复数域上近似求解方程的方法。</p>
<p>牛顿法迭代法解非线性方程，是把非线性方程 $f(x)&#x3D;0$ 线性化的一种近似方法。把 $f(x)$ 在点 $x_0$ 的某领域内展开成泰勒级数 $f(x)&#x3D;f(x_0)+f^{‘}(x_0)(x-x_0)+ \frac{f^{‘’}(x_0)(x-x_0)^2}{2!}+…+ \frac{f^{n}(x_0)(x-x_0)^n}{n!}+R_n(x)$ ，取其线性部分，即展开式的前两项 $f(x_0)+f^{‘}(x_0)(x-x_0)$ ，令其等于0，以此作为非线性方程 $f(x)&#x3D;0$ 的近似方程，若 $f^{‘}(x_0) \neq 0$ ，则解为 $x_1&#x3D;x_0- \frac {f(x_0)} {f^{‘}(x_0)}$ ，这样我们得到牛顿迭代法的一个迭代关系式</p>
<p>Python代码实例展示求解方程 $(x-3)^3&#x3D;0$ 的根</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> (x-<span class="number">3</span>)**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fd</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*(x-<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">newtonMethod</span>(<span class="params">n, assum</span>):</span><br><span class="line">    time = n</span><br><span class="line">    x = assum</span><br><span class="line">    Next = <span class="number">0</span></span><br><span class="line">    A = f(x)</span><br><span class="line">    B = fd(x)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;A = &#x27;</span> + <span class="built_in">str</span>(A) + <span class="string">&#x27;,B = &#x27;</span> + <span class="built_in">str</span>(B) + <span class="string">&#x27;,time = &#x27;</span> + <span class="built_in">str</span>(time))</span><br><span class="line">    <span class="keyword">if</span> f(x) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> time, x</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        Next = x - A/B</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Next x = &#x27;</span> + <span class="built_in">str</span>(Next))</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 1.</span></span><br><span class="line">    <span class="comment">## 这里设置的迭代条件不是当前值和最优值之间的距离，而是相比于上一个值的变化要小于某个值</span></span><br><span class="line">    <span class="comment">## 但我们要求的就是最优值，所以我们并不知道最优值的具体值，不能的到他们的距离</span></span><br><span class="line">    <span class="comment">## 所以要通过每次A的变化，来估计最优值</span></span><br><span class="line">    <span class="keyword">if</span> A - f(Next) &lt; <span class="number">1e-6</span>: </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Meet f(x) = 0, x = &quot;</span> + <span class="built_in">str</span>(Next))</span><br><span class="line"></span><br><span class="line">    <span class="comment">## 2.</span></span><br><span class="line">    <span class="comment"># ## 所以这里其实是不是也可以通过x的变化，来估计最优值？</span></span><br><span class="line">    <span class="comment"># if A/B &lt; 1e-4:</span></span><br><span class="line">    <span class="comment">#     print(&quot;Meet f(x) = 0, x = &quot; + str(Next))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">## 3.</span></span><br><span class="line">    <span class="comment"># ## 这里当然也能用 if time &gt; m 来设置跳出循环的条件，但是相对 A - f(Next) 得到的值会更不精确一些</span></span><br><span class="line">    <span class="comment"># if time &gt; 10:</span></span><br><span class="line">    <span class="comment">#     print(&quot;Meet f(x) = 0, x = &quot; + str(Next))</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> newtonMethod(n+<span class="number">1</span>, Next)</span><br><span class="line"></span><br><span class="line">newtonMethod(<span class="number">0</span>, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>牛顿法</tag>
        <tag>优化算法</tag>
      </tags>
  </entry>
  <entry>
    <title>白化和归一化</title>
    <url>/2020/02/19/%E6%95%B0%E5%AD%A6/%E7%99%BD%E5%8C%96%E5%92%8C%E5%BD%92%E4%B8%80%E5%8C%96/</url>
    <content><![CDATA[<h1 id="白化和归一化"><a href="#白化和归一化" class="headerlink" title="白化和归一化"></a>白化和归一化</h1><h3 id="白化"><a href="#白化" class="headerlink" title="白化"></a>白化</h3><p>白话的目的就是去除输入数据的冗余信息。经过白化处理后，生成新数据集满足两个条件：</p>
<ul>
<li>特征相关性较低</li>
<li>特征具有相同的方差</li>
</ul>
<p>白化算法实现的过程：</p>
<ul>
<li>第一步是进行PCA操作，将数据降维</li>
<li>第二步是对新的坐标进行归一化操作</li>
</ul>
<h3 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h3><p>数据归一化的目的就是为了把不同来源的数据统一到同一数量级(一个参考坐标系下)，这样使得比较起来更加有意义。归一化使得后面数据的处理更为方便，它有两大有点：</p>
<ul>
<li>归一化提高梯度下降求最优解的速度</li>
<li>归一化有可能提高精度</li>
</ul>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>看了下博客，都是抄来抄去，没有明确解释两者区别的，个人认为区别如下：</p>
<ul>
<li>白化里会用来归一化来统一降维后数据的量级，但是归一化不会用到白化</li>
<li>白化的主要目的是特征提取，尽量获取相互独立的特征进行分析</li>
<li>归一化的主要目的是统一特征数据的量级（取值范围）</li>
</ul>
]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>白化</tag>
        <tag>归一化</tag>
      </tags>
  </entry>
  <entry>
    <title>AM-softmax</title>
    <url>/2020/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/AM-softmax/</url>
    <content><![CDATA[<h1 id="AM-softmax"><a href="#AM-softmax" class="headerlink" title="AM-softmax"></a>AM-softmax</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>基于已有的L-softmax, A-softmax，作者提出了AM-softmax，一个适用于分类模型的loss计算方法。主要用于增加类间距离，减小类内距离</p>
<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><h3 id="反向传播求导简单"><a href="#反向传播求导简单" class="headerlink" title="反向传播求导简单"></a>反向传播求导简单</h3><p>反向传播对x求导结果为1</p>
<h3 id="公式更直观"><a href="#公式更直观" class="headerlink" title="公式更直观"></a>公式更直观</h3><p>相对于L-softmax，AM-softmax的参数简单直观</p>
<h3 id="加法"><a href="#加法" class="headerlink" title="加法"></a>加法</h3><p>使用加法$cos(&#x2F;delta)-m$，不同于L-softmax中使用乘法cos(m*&#x2F;delta)，超参数直接作用于cos(&#x2F;delta)更好，因为：cos(&#x2F;delta)和delta虽然是一一对应，但是变化趋势不同，cos(&#x2F;delta)在pai处更密集。因此在乘法中，得到wf后需要进行反余弦计算，因此会消耗更多的算力</p>
<h3 id="两个超参数"><a href="#两个超参数" class="headerlink" title="两个超参数"></a>两个超参数</h3><p>e的右上角公式为s(cos(&#x2F;delta)-m)</p>
<ul>
<li>其中m是用来增加类间距离，强行使两个类的边缘样本有一定的距离而更不容易混淆</li>
<li>反向传播是，x的导数为1，前面增加一个s则反向求导后gradient会变为s，使用一个s是为了让好样本的梯度更大，来减小类内样本的方差，也就是更快的减小类内距离</li>
</ul>
<h3 id="Feature-Normalization"><a href="#Feature-Normalization" class="headerlink" title="Feature Normalization"></a>Feature Normalization</h3><p>对于质量较好的样本，不需要进行feature normalization。<br>但是对于质量较差的样本，使用feature normalization后，效果会有提升。<br>因为质量越差，特征归一化的值越小，因此反向传播中受到的关于越大（1&#x2F;alpha），所以特征归一化更适合图像质量差的任务</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>am-softmax</tag>
      </tags>
  </entry>
  <entry>
    <title>Bert中的一些重点</title>
    <url>/2020/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Bert/</url>
    <content><![CDATA[<h1 id="Bert中的一些重点"><a href="#Bert中的一些重点" class="headerlink" title="Bert中的一些重点"></a>Bert中的一些重点</h1><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h3><p>Bert中使用的是Layer Normalization，源于对batch normalization的改进。<br>batch normalization的优点：</p>
<ul>
<li>加快神经网络训练时间</li>
<li>对学习率有更好的容忍</li>
<li>重新拉正数据分布，避免进入激活函数的饱和区域，缓解梯度消失<br>batch normalization的缺点：</li>
<li>对batch size的大小比较敏感</li>
<li>在RNN和LSTM等序列模型中效果不太好<br>针对以上两个BN的缺点，提出了LN，较好的解决了上面两个问题</li>
</ul>
<span id="more"></span>

<h3 id="position-embedding"><a href="#position-embedding" class="headerlink" title="position embedding"></a>position embedding</h3><p>同transformer里的position embedding不同，这里的position embedding需要训练获取。<br>transformer里的position embedding用的的正弦版本。两者各有优劣<br>正弦：</p>
<ul>
<li>一定位置间的embedding可以被彼此线性表示</li>
<li>相比训练好的position embedding，正弦版本能允许更长的输入序列<br>训练的position embedding</li>
<li>相比正弦的常量表示，学习来的embedding表示能力更强。加上bert后续需要做ner对位置上的字符进行状态预测，因此一个可以学习的参数来表达位置，更合理</li>
</ul>
<h3 id="gelu"><a href="#gelu" class="headerlink" title="gelu"></a>gelu</h3><p>Bert Transfromer结构中使用了这个激活函数—gelu（Gaussian error linear units，高斯误差线性单元），Gelu在论文中已经被验证，是一种高性能的神经网络激活函数，因为GELU的非线性变化是一种符合预期的随机正则变换方式（这句话，说实话，我翻译自原论文，具体怎么理解呢？我自己是如下理解的）。</p>
<p>激活函数的作用：给网络模型加入非线性因子，这个非线性因子的实际操作就是在wx+b这样的线下变化后面加入一个非线性变化的函数fun。<br>Gelu的操作方式：Gelu怎么完成非线性变换的呢？引入这样的变化函数：<br>        公式中x是自己，P(X&lt;&#x3D;x)决定x中有多少信息保留 ，并且由于P是服从高斯分布的，也就满足了非线性的特征，并且更加符合数据的分布预期。</p>
<h3 id="special-tokens"><a href="#special-tokens" class="headerlink" title="special tokens"></a>special tokens</h3><p>在BERT的使用中，我们对<code>[SEP]</code>和<code>[CLS]</code>者两个特殊tokens并不陌生。但是在查看<code>vocab.txt</code>时会发现，在文件的开头有若干<code>[unused*]</code>特殊token，比如在<code>bert-uncased-base</code>模型中有994<code>[unused*]</code>个特殊token。</p>
<p>这些token都是未经训练的，是BERT预留出来添加词汇的标记，我们可以用他们来指代任何字符，为下游fineturn任务引入先验知识预先提供占位符</p>
<pre><code>![unused](unused.jpeg)
</code></pre>
<p>通常我们如果想要在使用预训练模型的基础上，在BERT里加入额外的字词信息是比较困难的，通俗的做法是</p>
<ol>
<li><p>将BERT输出的<code>embedding</code>和额外信息的<code>embedding</code>进行<code>concat</code>来整合</p>
<p> <img src="/2020/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Bert/combiner-model.jpeg" alt="combiner-model"></p>
</li>
<li><p>或者修改<code>vocab.txt</code>文档，将词表维度从N扩展到M (: 可以参考<code>Hugging Face</code>官方<code>resize token embeddings</code>函数来实现)</p>
<ol>
<li>在词表（vocab.txt）中添加若干个自定义的特殊 tokens，词表大小由 N 增大到 M</li>
<li>新建一个 M 维的 embedding layer</li>
<li>将 BERT 原来的 N 维 embedding layer 中的 pretrained weights，按照词表顺序，复制到新的 M 维 embedding layer 中</li>
<li>替换掉 BERT 原来的 N 维 embedding layer。</li>
</ol>
</li>
</ol>
<p>但如果有了<code>[unused*]</code>预先占位，我们就可以将<code>vocab.txt</code>词表中的这些token替换成自定义的token，如<code>[Paragraph=N]</code>，这样诸如<code>[Paragraph=N]</code>的token会和<code>[CLS]</code>和<code>[SEP]</code>等meta-token一样，不会被WordPiece tokenizer分开，后续可以直接使用<code>[Paragraph=N]</code>等的embedding，而不需要引入更多维度的embedding向量了。</p>
<h3 id="warm-up"><a href="#warm-up" class="headerlink" title="warm-up"></a>warm-up</h3><h3 id="dropout"><a href="#dropout" class="headerlink" title="dropout"></a>dropout</h3><h2 id="获取向量"><a href="#获取向量" class="headerlink" title="获取向量"></a>获取向量</h2><p>bert中有很多种获取向量的方法，具体如下：</p>
<p><strong>init</strong>()：重头戏，模型的构建在此完成，三步走完成。<br>主要分为三个模块：embeddings、encoder 和 pooler。<br>首先构建输入，包括 input_ids、input_mask 等。其次进入 embeddings 模块，进行一系列 embedding 操作，涉及 embedding_lookup() 和 embedding_postprocessor() 两个函数。<br>然后进入 encoder 模块，就是 transformer 模型和 attention 发挥作用的地方了，主要涉及 transformer_model() 函数，得到 encoder 各层输出。<br>最后进入 pooler 模块，只取 encoder 最后一层的输出的第一个 token 的信息，送入到一个大小为 hidden_size 的全连接层，得到 pooled_output，这就是最终输出了。</p>
<ul>
<li>get_pooled_output(self)：获取 pooler 的输出。</li>
<li>get_sequence_output(self)：获取 encoder 最后的隐层输出，输出大小为 [batch_size, seq_length, hidden_size]。</li>
<li>get_all_encoder_layers(self)：获取 encoder 中所有层。返回大小应该是 [num_hidden_layers, batch_size, seq_length, hidden_size]。</li>
<li>get_embedding_output(self)：获取对 input_ids 的 embedding 结果，大小为 [batch_size, seq_length, hidden_size]，这是 word embedding、positional embedding、token type embedding（论文中的 segment embedding）和 layer normalization 一系列操作的结果，也是 transformer 的输入。</li>
<li>get_embedding_table(self)：获取 embedding table，大小为 [vocab_size, embedding_size]，即词汇表中的词对应的 embedding。</li>
</ul>
<p>详情见<a href="https://zhuanlan.zhihu.com/p/140718739">Bert是如何构建模型的</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>K-fold交叉验证</title>
    <url>/2022/07/12/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/K-fold%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81/</url>
    <content><![CDATA[<h1 id="K-fold交叉验证"><a href="#K-fold交叉验证" class="headerlink" title="K-fold交叉验证"></a>K-fold交叉验证</h1><h2 id="交叉验证的定义"><a href="#交叉验证的定义" class="headerlink" title="交叉验证的定义"></a>交叉验证的定义</h2><p>交叉验证（Cross Validation），有的时候也称作循环估计（Rotation Estimation），是一种统计学上将数据样本切割成较小子集的实用方法，该理论是由Seymour Geisser提出的。<br>在给定的建模样本中，拿出大部分样本进行建模型，留小部分样本用刚建立的模型进行预报，并求这小部分样本的预报误差，记录它们的平方加和。这个过程一直进行，直到所有的样本都被预报了一次而且仅被预报一次。把每个样本的预报误差平方加和，称为PRESS(predicted Error Sum of Squares)。</p>
<h3 id="K-fold-cross-validation"><a href="#K-fold-cross-validation" class="headerlink" title="K-fold cross-validation"></a>K-fold cross-validation</h3><p>K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的</p>
<h2 id="K-fold交叉验证的目的是什么"><a href="#K-fold交叉验证的目的是什么" class="headerlink" title="K-fold交叉验证的目的是什么"></a>K-fold交叉验证的目的是什么</h2><p>如果用一句话来总结：交叉验证是为了评估模型的稳定性，而不是为了从 <code>K</code> 个方程式(model)中选择一个。</p>
<p>具体见下方百度里的解释：</p>
<blockquote>
<p>用交叉验证的目的是为了得到可靠稳定的模型。在建立PCR 或PLS 模型时，一个很重要的因素是取多少个主成分的问题。用cross validation 校验每个主成分下的PRESS值，选择PRESS值小的主成分数。或PRESS值不再变小时的主成分数。<br>常用的精度测试方法主要是交叉验证，例如10折交叉验证(10-fold cross validation)，将数据集分成十份，轮流将其中9份做训练1份做验证，10次的结果的均值作为对算法精度的估计，一般还需要进行多次10折交叉验证求均值，例如：10次10折交叉验证，以求更精确一点。<br>交叉验证有时也称为交叉比对，如：10折交叉比对</p>
</blockquote>
<p>也可以参考此文章，解释的非常透彻： <a href="https://stats.stackexchange.com/questions/52274/how-to-choose-a-predictive-model-after-k-fold-cross-validation">How to choose a predictive model after k-fold cross-validation?</a></p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTM</title>
    <url>/2019/12/06/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/LSTM/</url>
    <content><![CDATA[<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="LSTM来源"><a href="#LSTM来源" class="headerlink" title="LSTM来源"></a>LSTM来源</h2><p>LSTM由RNN变化而来，所以在介绍LSTM前，我们先了解一下RNN。</p>
<p>RNN是一类用来处理序列数据的神经网络，当前状态不仅受到当前输入$x_t$的影响，也受前面状态$h_{t-1}$的影响</p>
<p>$h_t&#x3D;\delta (W^{h_{t-1}} \cdot h_{t-1} + W^{x_t} \cdot x_t)$<br>$y&#x3D;\delta (W^y \cdot h_t)$</p>
<pre><code>ignore bais here
</code></pre>
<p>且神经元是共享权重W的，即:<br>$W^{h_{t-1}}&#x3D;W^{h_{t}}$<br>$W^{x_{t-1}}&#x3D;W^{x_t}$<br>$W^{y-1}&#x3D;W^y$</p>
<span id="more"></span>

<h2 id="RNN问题"><a href="#RNN问题" class="headerlink" title="RNN问题"></a>RNN问题</h2><h3 id="长期依赖"><a href="#长期依赖" class="headerlink" title="长期依赖"></a>长期依赖</h3><p>RNN在实际应用中又一个很大的问题，就是长期依赖性问题，即一个输出$y_t$主要受到其前面几个输入和状态的影响，对较长时间前的输入和状态则非常不敏感，这是由于反向传播中梯度消失引起的。<br>为了解决这一问题，我们引入了LSTM（门机制）。</p>
<h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>我们在计算RNN反向传播的梯度时会发现，因为RNN是一个共享权重的神经网络，所以当权重W可以特征分解为$W&#x3D;Q \cdot \Lambda \cdot Q^T$且Q为正交矩阵时，则梯度包含特征值矩阵的幂次方，因此当特征值&gt;1时，则会梯度爆炸，反之则会梯度消失。<br>梯度爆炸比较容易察觉，一般表现为loss变为Nan<br>梯度消失则相对比较难以察觉，一般表现为loss几乎不变，但是loss几乎不变不代表就是产生了梯度消失</p>
<h2 id="LSTM基本描述"><a href="#LSTM基本描述" class="headerlink" title="LSTM基本描述"></a>LSTM基本描述</h2><p>LSTM一定程度上解决了RNN梯度消失和梯度爆炸的问题，得益于三个门的引入</p>
<h3 id="LSTM的三个门"><a href="#LSTM的三个门" class="headerlink" title="LSTM的三个门"></a>LSTM的三个门</h3><p>在LSTM中引入了三个门：输入门，输出门和遗忘门</p>
<p>遗忘门：控制了上一个cell的状态$C_{t-1}$中有多少信息进入到当前状态$C_t$中<br>输入门：控制了有多少信息会被保存到cell状态$C_t$中<br>输出门：控制了当前cell的状态$C_t$(cell state)有多少信息会输出到下一个神经元（即隐藏单元输出hidden state）</p>
<p><img src="/images/image/LSTM3-chain.png" alt="LSTM"></p>
<h3 id="LSTM特点"><a href="#LSTM特点" class="headerlink" title="LSTM特点"></a>LSTM特点</h3><p>LSTM一定程度上将RNN中反向传播中梯度中的叠成关系转换成叠加关系，所以缓解了梯度消失和梯度爆炸的问题。</p>
<h3 id="LSTM和GRU的比较"><a href="#LSTM和GRU的比较" class="headerlink" title="LSTM和GRU的比较"></a>LSTM和GRU的比较</h3><p>LSTM有三个门，GRU只有两个（更新门update和相关门revelance），所以LSTM的运算更为复杂。<br>GRU是在LSTM后出来的，为了缓解LSTM的计算问题，但是在一些实际应用中，GRU的效果并不比LSTM差，所以应用越来越广泛。</p>
<p><img src="/images/image/GRU.png" alt="GRU"></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ol>
<li><p>关于RNN的长期依赖性问题，看了很多文章，一直不能确到底是因</p>
<ul>
<li>$h_t&#x3D;Q^T \cdot \Lambda^t \cdot Q \cdot h_{(0)}$导致的在<strong>前向传播</strong>中$h_{(0)}$对$h_t$的影响因为幂次方的关系而非常小</li>
<li>还是因为梯度消失导致的长期依赖关系.</li>
</ul>
<p> 直到看到Andrew Ng在<a href="https://www.coursera.org/learn/nlp-sequence-models/lecture/PKMRR/vanishing-gradients-with-rnns">视频</a>中明确指出：RNN的长期依赖性问题来源于<strong>梯度消失</strong>，并且在<a href="http://pelhans.com/2019/04/24/deepdive_tensorflow-note9/#%E9%95%BF%E6%9C%9F%E4%BE%9D%E8%B5%96%E9%97%AE%E9%A2%98">文章</a>中看到公式$\Delta_{h^{(0)}}$的求解，才理解：RNN的长期依赖性问题来源于反向传播中因共享权重W产生的$\Delta$的幂次方，造成了梯度消失，而使得当前状态对前面序列的依赖过小。</p>
</li>
<li><p>在LSTM中有几种激活函数：</p>
<ul>
<li>sigmoid主要用在门的计算中，因为sigmoid很容易趋近于0或者1，所以使得门可以控制保存（等于1时）或不保存（等于0时）信息</li>
<li>softmax一般用于输出y之前</li>
<li>tanh用于求$\tilde{C}^t$和$h_{t}$的时候</li>
</ul>
</li>
<li><p>hexo中并不支持LaTex格式写公式的问题见<a href="http://stevenshi.me/2017/06/26/hexo-insert-formula/">此文章</a></p>
</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>长期依赖性</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>SBert</title>
    <url>/2020/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/SBert/</url>
    <content><![CDATA[<h1 id="SBert"><a href="#SBert" class="headerlink" title="SBert"></a>SBert</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>SBert针对句子匹配问题，提出了一种新的想法。没有很复杂的公式，可以理解为，是一些常见的计算进行各种融合的实战经验总结。<br>主要思想也是对bert进行fine tuning，来获取语义上有意义的空间向量<br>    语义上有意义：论文中有解释，是指句子意思相近的句子在空间向量上也相近</p>
<span id="more"></span>

<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><h3 id="速度快"><a href="#速度快" class="headerlink" title="速度快"></a>速度快</h3><p>论文中提出了，相对于通常使用的NSP，即concat sent1和sent2来判断两个句子是否相似，我们将两个句子独立分开输入到同一bert模型（两个句子使用的参数相同）中，再将得到的两个向量进行计算，要快上很多倍，主要原因如下：</p>
<ol>
<li>假设有10000个样本<ol>
<li>如果用NSP句子对计算，那么可以相互匹配出10000*10000个句子对，去除重复的也有一半，所以样本数上计算量很大</li>
<li>如果将句子分开过bert模型，那么10000个样本只用过10000次bert模型，计算量会少很多</li>
</ol>
</li>
<li>每个样本过bert模型的计算量也会不同<ol>
<li>如果用NSP句子对，假设两个句子的长度分别为m和n，那么concate之后进行self attention，就是O（（m+n）^2）的时间复杂度</li>
<li>如果两个句子独立进行self attention，两个句子的时间复杂度也是O（m^2） while m &gt;&#x3D; n</li>
</ol>
</li>
</ol>
<h3 id="pooling"><a href="#pooling" class="headerlink" title="pooling"></a>pooling</h3><p>在bert的embedding输出层，接了一个pooling。<br>作者比较了三种pooling方法，一种直接使用CLS的结果，一种使用mean，一种使用max。使用mean的效果最好<br>作者并没有在论文中详细阐述这个mean的pooling是作用在哪个维度上的，我个人推测可能是对seq_length做的mean，这样才能和cls层出来的embedding维度相同<br>找到一个解释是：</p>
<ul>
<li>MEAN策略，计算各个token输出向量的平均值代表句子向量。各个token的向量维度一样，即把所有token的embeding相加，然后处以token数即得到平均，也就是这个句子的句向量</li>
<li>MAX策略，取所有输出向量各个维度的最大值代表句子向量。各个token有很多个维度，每个维度上比较多个token当前维度的值，并取最大，然后获得句向量<br>所以以上都是对seq_length这一行进行了pooling，将其变为了1</li>
</ul>
<p>具体cnn里filter和pooling后的维度见<a href="https://mp.weixin.qq.com/s/vq4-4M46okms5_sxw9PBQA">实战TextCNN 文本分类</a></p>
<h3 id="针对不同的数据提出了三种loss"><a href="#针对不同的数据提出了三种loss" class="headerlink" title="针对不同的数据提出了三种loss"></a>针对不同的数据提出了三种loss</h3><h4 id="classification-objective-function"><a href="#classification-objective-function" class="headerlink" title="classification objective function"></a>classification objective function</h4><p>在将两个句子的embedding，u和v，concate时，作者实验了很多种不同的方法<br>比如：（u,v），（|u-v|），（u,v,|u-v|）等等，可以到时候放个结果图过来，论文中的table6。<br>作者最后发现（u,v,|u-v|）这种concate方式效果最好</p>
<h4 id="regression-objective-function"><a href="#regression-objective-function" class="headerlink" title="regression objective function"></a>regression objective function</h4><p>计算两个句子的余弦相似性，然后使用均方差loss</p>
<h4 id="triplet-objective-function"><a href="#triplet-objective-function" class="headerlink" title="triplet objective function"></a>triplet objective function</h4><p>如果是有正负样本的数据，可以使用triplet loss<br>max(dist(sa-sp)-dist(sa-sn)+margin, 0)<br>这里dist()计算的两者欧式距离，并且把margin设为1</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>SBert</tag>
      </tags>
  </entry>
  <entry>
    <title>T5</title>
    <url>/2020/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/T5/</url>
    <content><![CDATA[<h1 id="T5"><a href="#T5" class="headerlink" title="T5"></a>T5</h1><h2 id="分词"><a href="#分词" class="headerlink" title="分词"></a>分词</h2><p>1、中文分词(Chinese Word Segmentation) 指的是将一个汉字序列切分成一个一个单独的词。分词就是将连续的字序列按照一定的规范重新组合成词序列的过程 </p>
<span id="more"></span>

<p>2、现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法</p>
<pre><code>1. 基于字符串匹配的分词方法：这种方法又叫做机械分词方法，它是按照一定的策略将待分析的汉字串与一个“充分大的”机器词典中的词条进行配，若在词典中找到某个字符串，则匹配成功（识别出一个词）
    1）正向最大匹配法（由左到右的方向）
    2）逆向最大匹配法（由右到左的方向）:
    3）最少切分（使每一句中切出的词数最小）
    4）双向最大匹配法（进行由左到右、由右到左两次扫描）
2. 基于统计模型的分词方法
    1) 基于 N-gram 语言模型的分词方法
3. 基于序列标注的分词方法
    1) 基于 HMM 的分词方法
    2) 基于 CRF 的分词方法
    3) 基于词感知机的分词方法
    4) 基于深度学习的端到端的分词方法
</code></pre>
<h3 id="基于规则或词典"><a href="#基于规则或词典" class="headerlink" title="基于规则或词典"></a>基于规则或词典</h3><h5 id="1-正向最大匹配思想MM"><a href="#1-正向最大匹配思想MM" class="headerlink" title="1 正向最大匹配思想MM"></a>1 正向最大匹配思想MM</h5><p>从左向右取待切分汉语句的m个字符作为匹配字段，m为大机器词典中最长词条个数。<br>查找大机器词典并进行匹配：<br>若匹配成功，则将这个匹配字段作为一个词切分出来。<br>若匹配不成功，则将这个匹配字段的最后一个字去掉，剩下的字符串作为新的匹配字段，进行再次匹配，重复以上过程，直到切分出所有词为止。<br>举个栗子：<br>现在，我们要对“南京市长江大桥”这个句子进行分词，根据正向最大匹配的原则：</p>
<p>先从句子中拿出前5个字符“南京市长江”，把这5个字符到词典中匹配，发现没有这个词，那就缩短取字个数，取前四个“南京市长”，发现词库有这个词，就把该词切下来；<br>对剩余三个字“江大桥”再次进行正向最大匹配，会切成“江”、“大桥”；<br>整个句子切分完成为：南京市长、江、大桥；</p>
<h5 id="2-逆向最大匹配算法RMM"><a href="#2-逆向最大匹配算法RMM" class="headerlink" title="2 逆向最大匹配算法RMM"></a>2 逆向最大匹配算法RMM</h5><p>该算法是正向最大匹配的逆向思维，匹配不成功，将匹配字段的最前一个字去掉，实验表明，逆向最大匹配算法要优于正向最大匹配算法。</p>
<p>还是那个栗子：</p>
<p>取出“南京市长江大桥”的后四个字“长江大桥”，发现词典中有匹配，切割下来；<br>对剩余的“南京市”进行分词，整体结果为：南京市、长江大桥</p>
<h5 id="3-双向最大匹配法-Bi-directction-Matching-method-BM"><a href="#3-双向最大匹配法-Bi-directction-Matching-method-BM" class="headerlink" title="3 双向最大匹配法(Bi-directction Matching method,BM)"></a>3 双向最大匹配法(Bi-directction Matching method,BM)</h5><p>双向最大匹配法是将正向最大匹配法得到的分词结果和逆向最大匹配法的到的结果进行比较，从而决定正确的分词方法。</p>
<p>据SunM.S. 和 Benjamin K.T.（1995）的研究表明，中文中90.0％左右的句子，正向最大匹配法和逆向最大匹配法完全重合且正确，只有大概9.0％的句子两种切分方法得到的结果不一样，但其中必有一个是正确的（歧义检测成功），只有不到1.0％的句子，或者正向最大匹配法和逆向最大匹配法的切分虽重合却是错的，或者正向最大匹配法和逆向最大匹配法切分不同但两个都不对（歧义检测失败）。这正是双向最大匹配法在实用中文信息处理系统中得以广泛使用的原因所在。</p>
<p>正向最大匹配法，最终分词结果为：“南京市长&#x2F;江&#x2F;大桥”，其中，总分词数3个，单个词为1。</p>
<p>逆向最大匹配法，最终分词结果为：“南京市&#x2F;长江大桥”，其中，总分词数2个，单个词为0。</p>
<p>选择标准：</p>
<p>首先看两种方法结果的分词数，分词数越少越好；<br>分词数相同的情况下，看单个词的数量，越少越好；<br>因此最终输出为逆向结果。</p>
<h5 id="4-最佳匹配（OM，分正向和逆向）"><a href="#4-最佳匹配（OM，分正向和逆向）" class="headerlink" title="4 最佳匹配（OM，分正向和逆向）"></a>4 最佳匹配（OM，分正向和逆向）</h5><p>对分词词典按词频大小顺序排列，并注明长度，降低时间复杂度。</p>
<p>优点：易于实现<br>缺点：匹配速度慢。对于未登录词的补充较难实现。缺乏自学习。</p>
<h5 id="5-逐词遍历法"><a href="#5-逐词遍历法" class="headerlink" title="5 逐词遍历法"></a>5 逐词遍历法</h5><p>这种方法是将词库中的词由长到短递减的顺序，逐个在待处理的材料中搜索，直到切分出所有的词为止。</p>
<h5 id="等等"><a href="#等等" class="headerlink" title="等等"></a>等等</h5><h5 id="Trie树（前缀树）"><a href="#Trie树（前缀树）" class="headerlink" title="Trie树（前缀树）"></a>Trie树（前缀树）</h5><h3 id="基于统计的分词"><a href="#基于统计的分词" class="headerlink" title="基于统计的分词"></a>基于统计的分词</h3><h5 id="N-gram模型思想"><a href="#N-gram模型思想" class="headerlink" title="N-gram模型思想"></a>N-gram模型思想</h5><p>模型基于这样一种假设，第n个词的出现只与前面N-1个词相关，而与其它任何词都不相关，整句的概率就是各个词出现概率的乘积。</p>
<p>我们给定一个词，然后猜测下一个词是什么。当我说“艳照门”这个词时，你想到下一个词是什么呢？我想大家很有可能会想到“陈冠希”，基本上不会有人会想到“陈志杰”吧，N-gram模型的主要思想就是这样的。</p>
<p>对于一个句子T，我们怎么算它出现的概率呢？假设T是由词序列W1,W2,W3,…Wn组成的，那么P(T)&#x3D;P(W1W2W3…Wn)&#x3D;P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1)<br>但是这种方法存在两个致命的缺陷：一个缺陷是参数空间过大，不可能实用化；另外一个缺陷是数据稀疏严重。为了解决这个问题，我们引入了马尔科夫假设：一个词的出现仅仅依赖于它前面出现的有限的一个或者几个词。如果一个词的出现仅依赖于它前面出现的一个词，那么我们就称之为bigram。即</p>
<p>P(T) &#x3D;P(W1W2W3…Wn)&#x3D;P(W1)P(W2|W1)P(W3|W1W2)…P(Wn|W1W2…Wn-1)<br>≈P(W1)P(W2|W1)P(W3|W2)…P(Wn|Wn-1)</p>
<p>如果一个词的出现仅依赖于它前面出现的两个词，那么我们就称之为trigram。</p>
<p>在实践中用的最多的就是bigram和trigram了，而且效果很不错。高于四元的用的很少，因为训练它需要更庞大的语料，而且数据稀疏严重，时间复杂度高，精度却提高的不多。一般的小公司，用到二元的模型就够了，像Google这种巨头，也只是用到了大约四元的程度，它对计算能力和空间的需求都太大了。</p>
<p>详细讲解和代码见<a href="https://codle.net/chinese-word-cutter-1/">n-gram</a></p>
<h5 id="HMM、CRF-模型思想"><a href="#HMM、CRF-模型思想" class="headerlink" title="HMM、CRF 模型思想"></a>HMM、CRF 模型思想</h5><p>以往的分词方法，无论是基于规则的还是基于统计的，一般都依赖于一个事先编制的词表(词典)，自动分词过程就是通过词表和相关信息来做出词语切分的决策。与此相反，</p>
<p>基于字标注（或者叫基于序列标注）的分词方法实际上是构词方法，即把分词过程视为字在字串中的标注问题。</p>
<p>由于每个字在构造一个特定的词语时都占据着一个确定的构词位置(即词位)，假如规定每个字最多只有四个构词位置：即B(词首)，M (词中)，E(词尾)和S(单独成词)，那么下面句子(甲)的分词结果就可以直接表示成如(乙)所示的逐字标注形式：</p>
<ol>
<li>(甲)分词结果：／上海／计划／N／本／世纪／末／实现／人均／国内／生产／总值／五千美元／ </li>
<li>(乙)字标注形式：上／B海／E计／B划／E N／S 本／s世／B 纪／E 末／S 实／B 现／E 人／B 均／E 国／B 内／E生／B产／E总／B值／E 五／B千／M 美／M 元／E 。／S</li>
</ol>
<p>首先需要说明，这里说到的“字”不只限于汉字。考虑到中文真实文本中不可避免地会包含一定数量的非汉字字符，本文所说的“字”，也包括外文字母、阿拉伯数字和标点符号等字符。所有这些字符都是构词的基本单元。当然，汉字依然是这个单元集合中数量最多的一类字符。</p>
<p>把分词过程视为字的标注问题的一个重要优势在于，它能够平衡地看待词表词和未登录词的识别问题。</p>
<p>在这种分词技术中，文本中的词表词和未登录词都是用统一的字标注过程来实现的。在学习架构上，既可以不必专门强调词表词信息，也不用专门设计特定的未登录词(如人名、地名、机构名)识别模块。这使得分词系统的设计大大简化。在字标注过程中，所有的字根据预定义的特征进行词位特性的学习，获得一个概率模型。然后，在待分字串上，根据字与字之间的结合紧密程度，得到一个词位的标注结果。最后，根据词位定义直接获得最终的分词结果。总而言之，在这样一个分词过程中，分词成为字重组的简单过程。在学习构架上，由于可以不必特意强调词表词的信息，也不必专门设计针对未登录词的特定模块，这样使分词系统的设计变得尤为简单。</p>
<p>2001年Lafferty在最大熵模型（MEM）和隐马尔科夫模型（HMM）的基础上提出来了一种无向图模型–条件随机场（CRF）模型，它能在给定需要标记的观察序列的条件下，最大程度提高标记序列的联合概率。常用于切分和标注序列化数据的统计模型。</p>
<h2 id="T5中的分词"><a href="#T5中的分词" class="headerlink" title="T5中的分词"></a>T5中的分词</h2><p>T5中的分词用的是google的sentencepiece分词。类似于BPE，找出频率最高的词组合作为词典进行分词。<br>bert4keras中为了效率，不是按照原mt5的字典来进行的分词，而且作者用自己的预料进行了后处理。<br>经测试，发现分词效果并不是非常客观。<br>因此可以尝试训练一套自己领域的并且适合T5中使用的model和词典来进行分词。具体如何实现，可以看完T5论文后再进行</p>
<ol>
<li>T5中使用词典，那么还需要模型的概率预测来进行分词吗？</li>
</ol>
<h2 id="Greedy-Decoding-VS-Beam-Search"><a href="#Greedy-Decoding-VS-Beam-Search" class="headerlink" title="Greedy Decoding VS Beam Search"></a>Greedy Decoding VS Beam Search</h2><h3 id="Greedy-search"><a href="#Greedy-search" class="headerlink" title="Greedy search"></a>Greedy search</h3><p>一个自然的想法是贪心搜索(greedy search)，即decoder的每一步都选择最可能的单词，最后得到句子的每一个单词都是每一步认为最合适的单词。但这样并不保证整个句子的概率是最大的，即不能保证整个句子最合适。实际上，贪心搜索的每一步搜索都处理成仅仅与前面刚生成的一个单词相关，类似于马尔科夫假设。这显然是不合理的，具体来说，贪心搜索到的句子𝑦概率是使得下式概率最大：</p>
<p>𝑃(𝑦|𝑥)&#x3D;∏𝑛𝑘&#x3D;1𝑝(𝑦𝑘|𝑥,𝑦𝑘−1)</p>
<p>而实际上，根据全概率公式计算得到𝑃(𝑦|𝑥)为：</p>
<p>𝑃(𝑦|𝑥)&#x3D;∏𝑛𝑘&#x3D;1𝑝(𝑦𝑘|𝑥,𝑦1,𝑦2,…,𝑦𝑘−1)</p>
<h3 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h3><p>译为束搜索。思想是，每步选取最可能的𝑘个结果，再从最后的𝑘个结果中选取最合适的句子。𝑘称为beam size。</p>
<p>具体做法是：</p>
<p>首先decoder第一步搜索出最可能的𝑘个单词，即找到𝑦11,𝑦12,…,𝑦1𝑘，他们的概率𝑝(𝑦11|𝑥),…,𝑝(𝑦1𝑘|𝑥)为最大的𝑘个。</p>
<p>进行第二步搜索，分别进行𝑘个模型副本的搜索。每个副本𝑖，根据上一步选取的单词𝑦1𝑖，选取概率最大的𝑘个结果𝑦21,𝑦22,…,𝑦2𝑘。这样，就有了𝑘∗𝑘个可能的结果，从这些结果中选择𝑘个概率最大的结果，即𝑝(𝑦1𝑖|𝑥)∗𝑝(𝑦2𝑗|𝑥,𝑦1𝑖)最大的𝑘个结果。</p>
<p>进行第三步搜索，从第二步中确定的𝑘个结果出发，再进行𝑘个模型副本的搜索，直到最后一步，从最后的𝑘个结果中选取概率最大者。</p>
<p>显然，若𝑘&#x3D;1则为贪心搜索，𝑘越大则占用内存越大，计算代价越大，实际应用中取10即可。</p>
<p>另外，可以发现概率的连乘使得概率越来越小，很可能溢出，为了保证模型的稳定性，常对概率连乘计算+log变为加法。</p>
<p>𝑃(𝑦|𝑥)&#x3D;𝑙𝑜𝑔(∏𝑛𝑘&#x3D;1𝑝(𝑦𝑘|𝑥,𝑦1,𝑦2,…,𝑦𝑘−1))</p>
<h3 id="改进Beam-search"><a href="#改进Beam-search" class="headerlink" title="改进Beam search"></a>改进Beam search</h3><h4 id="length-penalty"><a href="#length-penalty" class="headerlink" title="length penalty"></a>length penalty</h4><p>从Beam search的搜索过程中可以发现，Beam search偏向于找到更短的句子，也就是说，如果搜索过程中有一支搜索提前发现了&lt;𝐸𝑂𝑆&gt;,而另外𝑘−1支继续搜索找到其余更长的结果，那么由于概率连乘(或log连加)，越长的结果概率肯定越小。因此有必要进行模型修正，即进行长度归一化，具体来说，即：</p>
<p>选择概率𝑃(𝑦|𝑥)&#x3D;1𝑛𝑙𝑜𝑔(∏𝑛𝑘&#x3D;1𝑝(𝑦𝑘|𝑥,𝑦1,𝑦2,…,𝑦𝑘−1))最大的句子，式中，𝑛为该结果序列长度。</p>
<p>另外，实践中还做了如下修正：</p>
<p>𝑃(𝑦|𝑥)&#x3D;1𝑛𝛼𝑙𝑜𝑔(∏𝑛𝑘&#x3D;1𝑝(𝑦𝑘|𝑥,𝑦1,𝑦2,…,𝑦𝑘−1))</p>
<p>式中，超参数𝛼取0.6或者0.7比较合适。</p>
<h4 id="Grid-Beam-Search"><a href="#Grid-Beam-Search" class="headerlink" title="Grid Beam Search"></a>Grid Beam Search</h4><p>结合约束条件，选择模型生成的下一个结果或特定的词语或者短句</p>
<h4 id="Best-First-Beam-Search"><a href="#Best-First-Beam-Search" class="headerlink" title="Best-First Beam Search"></a>Best-First Beam Search</h4><p>优化使用beam search解码的时间。实现比较复杂，细节较多</p>
<h4 id="look-ahead"><a href="#look-ahead" class="headerlink" title="look ahead"></a>look ahead</h4><h4 id="on-the-fly"><a href="#on-the-fly" class="headerlink" title="on the fly"></a>on the fly</h4><h3 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h3><p>对于训练的seq2seq模型，对它输出的句子𝑦，以及实际的句子𝑦∗，若概率𝑦大于𝑦∗，（统计所有句子，平均来说是这个结果），则说明，seq2seq模型出错了。否则，说明，baem search并没有找到最合适的结果，可以考虑增大beam size大小。</p>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>T5</tag>
      </tags>
  </entry>
  <entry>
    <title>Triplet Loss</title>
    <url>/2020/02/19/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/Triplet%20Loss/</url>
    <content><![CDATA[<h1 id="Triplet-Loss"><a href="#Triplet-Loss" class="headerlink" title="Triplet Loss"></a>Triplet Loss</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>Triplet-Loss最先由图像里提出来。为了解决负样本与标准样本过于相似的问题，来增加类间距离，减小类内距离<br>Triplet-Loss通过最小化dist(a,p)-dist(a,n)+margin，来训练出可以更好表征样本的embedding<br>dist()一般使用欧式距离，当然也可以用余弦等其它距离</p>
<span id="more"></span>

<h2 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h2><h3 id="样本"><a href="#样本" class="headerlink" title="样本"></a>样本</h3><p>使用triplet-loss通常需要将样本组合成三元组，&lt;a, p, n&gt;<br>如何组合有很多中方法，最直接的一种，将所有正样本和所有可能的负样本与标准样本一一组成三元组，这样可组合的可能性过多，这种方法并不适用，所以作者提出了另外两种方法offline和online</p>
<h3 id="offline"><a href="#offline" class="headerlink" title="offline"></a>offline</h3><p>每n steps后，使用当前模型，计算出离每个标准问最远的正样本和最近的负样本，然后重新接着训练</p>
<h3 id="online"><a href="#online" class="headerlink" title="online"></a>online</h3><p>从batch-size里选取正样本和负样本（我看很多文章里也写不清楚到底是怎么取，以下是个人理解）<br>每个batch-size里每个类别要取40个样本，然后随机选取一些负样本加入进来。batch-size大小最好为1800<br>我的思考：</p>
<ul>
<li>每个batch-size不需要包含所有类别，可以每次包含k&#x3D;5个类别，每个类别选取40个样本放入，共有200个样本，然后再选取1600个负样本放入</li>
<li>接着对于batch-size中的每一个类别，我们选取40次正样本，然后从剩下的1800-40个负样本中随机选取负样本</li>
<li>每个类别我们总共组成40个三元组，这个batch-size我们会组成200个三元组</li>
<li>之所以要每个类别要选取40个正样本，是为了保证此类别中正样本可选取的数量不要太少，因为在组成三元组样本时，经常会碰到负样本过多的问题，这里强行规定的一定量的正样本是的两种样本比在batch-size中不至于太失衡</li>
</ul>
<h3 id="semi-hard"><a href="#semi-hard" class="headerlink" title="semi-hard"></a>semi-hard</h3><p>在论文中，作者解释如果每次都取hardest sample，即离标准样本最近的负样本，容易导致模型过早限于局部最小值（如果learning rate没足够大到能跳出局部最小区域，就会造成loss不下降的问题），因此我们尝试使用semi-hard sample</p>
<ul>
<li>hardest sample为距离标准样本最近的负样本（有可能距离小于正样本到标准样本的距离）</li>
<li>semi sample为，比正样本距离标准样本的距离很近但稍远一点点的负样本。这类样本虽然不是最难的负样本，但也是很难分的样本，因为他们距离标准样本的距离与正样本相似，因此容易混淆</li>
</ul>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><ul>
<li>为了让SGD更快下降，batch-size越小越好</li>
<li>为了让triplet loss效果更好，batch-size越大越好</li>
<li>作者选择了batch-size&#x3D;1800</li>
<li>作者这里margin设为0.2</li>
</ul>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>triplet loss</tag>
      </tags>
  </entry>
  <entry>
    <title>梯度消失和梯度爆炸</title>
    <url>/2019/12/07/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8/</url>
    <content><![CDATA[<h1 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h1><h2 id="什么是梯度消失-x2F-梯度爆炸"><a href="#什么是梯度消失-x2F-梯度爆炸" class="headerlink" title="什么是梯度消失&#x2F;梯度爆炸"></a>什么是梯度消失&#x2F;梯度爆炸</h2><p>梯度消失或梯度爆炸是指在神经网络权重更新的反向传播中，$\frac{\partial Loss}{\partial W}$过小或过大的现象</p>
<h2 id="梯度消失-x2F-梯度爆炸会产生什么问题"><a href="#梯度消失-x2F-梯度爆炸会产生什么问题" class="headerlink" title="梯度消失&#x2F;梯度爆炸会产生什么问题"></a>梯度消失&#x2F;梯度爆炸会产生什么问题</h2><p>梯度爆炸会导致权重的更新过大（表现为LOSS出现Nan）<br>梯度消失会导致权重的更新过慢或者几乎不更新</p>
<span id="more"></span>

<h2 id="为什么会有梯度消失和梯度爆炸"><a href="#为什么会有梯度消失和梯度爆炸" class="headerlink" title="为什么会有梯度消失和梯度爆炸"></a>为什么会有梯度消失和梯度爆炸</h2><p>一种是任务梯度消失&#x2F;梯度爆炸的原因是反向传播过程中的叠成导致的<br>另一种认为反向传播不该背这个锅，反向传播求导只是一种求最优解的方式，最根本的原因还是网络结构的问题（线性和非线性公式连成组合）</p>
<blockquote>
<p>看到<a href="https://www.zhihu.com/question/34878706">有个文章</a>说我们不应该纠结于梯度消失和梯度爆炸，应该从不同的角度看LSTM，比如选择性、信息不变性等等，觉得这是个很好的想法。当然，他对于RNN和DNN中梯度消失和梯度爆炸的看法也相对详细正确，下面会细说。</p>
</blockquote>
<p>无论是因为网络结构还是反向传播，不可否认的是梯度消失和梯度传播确实来源于对权重求导后很多小于1的数值的连乘，下面我门主要看一下为什么会产生这些小于1的数值。</p>
<h3 id="几个激活函数"><a href="#几个激活函数" class="headerlink" title="几个激活函数"></a>几个激活函数</h3><p>在进入反向传播求导前，先认识一下神经网络中我们经常用到的激活函数</p>
<h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p>sigmoid 常见于CNN中，在RNN和LSTM中一般不会用到sigmoid<br>sigmoid的公式为$S(x)&#x3D;\frac{1}{1+e^{-x}}$<br>其函数和导数图如下</p>
<p><img src="/./images/image/sigmoid.png" alt="sigmoid"></p>
<h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p>LSTM和RNN中会用到tanh，详情可以看上一篇文章<br>tanh的公式为$tanh(x)&#x3D;\frac{e^{x}-e^{-x}}{e^{x}+e^{x}}$<br>其函数和导数图如下</p>
<p><img src="/images/image/tanh.png" alt="tanh"></p>
<h4 id="Relu"><a href="#Relu" class="headerlink" title="Relu"></a>Relu</h4><p>Relu主要是为了解决梯度消失和梯度爆炸的问题<br>其公式为：<br>$<br>relu(x)&#x3D;\begin{cases}<br>0, x&lt;0 \\<br>x, x&gt;0<br>\end{cases}<br>$</p>
<p>其函数和导数图如下</p>
<p><img src="/images/image/relu.jpg" alt="relu"></p>
<p>relu的优点：</p>
<ul>
<li>解决了梯度消失&#x2F;梯度爆炸的问题</li>
<li>计算速度快</li>
<li>加速了网络训练（这点我有点怀疑，是指计算速度快呢还是指效果更好，更容易收敛找到最优解呢？如果是第二点，如何证明的？）<br>relu缺点：</li>
<li>负数部分恒为0，导致一些神经元无法激活（但是有很多relu的改进版已经解决了这个问题，比如leakrelu）</li>
<li>输出不是以0为中心</li>
</ul>
<blockquote>
<p><strong>问题：如果大于0的函数都返回其本身，不是失去了使用非线性函数的意义吗？</strong></p>
</blockquote>
<h3 id="为什么会产生梯度消失和梯度爆炸"><a href="#为什么会产生梯度消失和梯度爆炸" class="headerlink" title="为什么会产生梯度消失和梯度爆炸"></a>为什么会产生梯度消失和梯度爆炸</h3><p>DNN和RNN中的梯度消失和梯度爆炸是不同的</p>
<h4 id="DNN"><a href="#DNN" class="headerlink" title="DNN"></a>DNN</h4><p>写比较麻烦，下次有时间再看看专门推一遍公式写一遍吧，这里直接贴吧：</p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播w导数解释"></p>
<p><img src="/images/image/nn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADb%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="nn反向传播b导数解释"></p>
<p>没有手动推导，所以曲折的理解过程如下：</p>
<ol>
<li><p>我当时看到两个博客里的公式不一样很困惑，后来发现是自己粗心，因为第一个是对W求导，第二个是对b求导。但是看到很多博客里其实都写的是第二种情况。<br>其实更应该考虑的是第一种情况，因为在神经网络中，bias是为了防止wx等于零起的辅助作用，真正有意义的是讨论W。</p>
</li>
<li><p>虽然真正有意义的是讨论W，但是图二中对b求导的公式其实和真正对w求导的公式差不多，因为他们前面的所有结果都是相同的，唯一不同的是最后一步对b求导结果是1，而对w求导结果是$\delta’(z)$，也就是图一中的$f_1$。所以图二中的公式可以看作是图一公式的详细版。所以在反向传播中，决定导数数值的不仅是激活函数的函数，也有W本身。</p>
</li>
</ol>
<p>如上所示，我们可以看到$loss$对$W_1$求导，其实是$\delta’(z) \cdot w_i$的连乘，在上面我们已经了解到，sigmoid和tanh的导数，都小于零，w一般都会初始化为均值为0，方差为1的数值，我们可以有$\vert \delta’(z) \cdot w_i \vert&lt;1$($\vert \delta’(z) \cdot w_i \vert&lt;\frac{1}{4}$当激活函数为sigmoid时)。因此会有梯度消失。</p>
<p>当然，当w特别大是，我们会有$\vert \delta’(z) \cdot w_i \vert&gt;1$，就会导致梯度爆炸。</p>
<p>从上我们可以看到DNN中的梯度爆炸或者梯度消失，本质是大于1或者小于1的数的<strong>连乘</strong>（注意这里不是幂次方，RNN中才是幂次方，下面会细说）导致的。</p>
<h4 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h4><p>RNN中很重要的一点是权重共享。一样贴图</p>
<p><img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%ADw%E5%AF%BC%E6%95%B0%E8%A7%A3%E9%87%8A.png" alt="rnn反向传播w导数解释"></p>
<p>这里有几个点需要解释：</p>
<ul>
<li><p>为什么DNN中就连乘，但是RNN的导数中有加法呢？<br>DNN中写出的公式是<strong>每一层</strong>$w_i$前的求导公式<br>RNN这里写出的是<strong>所有层数</strong>的$w_x$前的求导公式的和，因为RNN是权重共享，所以对$W_x$的更新就是每一层需要对$W_x$更新的和。<br>因此第三层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%89%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第三层导数">，第二层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%BA%8C%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第二层导数">，第一层$W_x$前的导数是<img src="/images/image/rnn%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%AC%AC%E4%B8%80%E5%B1%82%E5%AF%BC%E6%95%B0.png" alt="rnn反向传播第一层导数"></p>
</li>
<li><p>公式中$\frac{\partial S_i}{\partial S_i-1}$和DNN中的$\frac{\partial f_i}{\partial f_i-1}$是一样的性质，唯一不同的是这里$\frac{\partial S_i}{\partial S_i-1}&#x3D;\frac{\partial S_i-1}{\partial S_i-2}&#x3D;tanh’\cdot W_s$，所以这里其实是一个小于1的数的<strong>幂次方</strong></p>
</li>
</ul>
<h4 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h4><p>关于DNN和RNN的梯度消失和梯度爆炸，<a href="https://www.zhihu.com/question/34878706">这里</a>解释的非常详细。</p>
<p>我也自己来总结以下加深印象</p>
<ul>
<li><p>DNN中每层都有不同的参数，所以每一层都各是各的梯度。RNN中因为权重共享，所以梯度为所有层数梯度的总和（上面提到过）</p>
</li>
<li><p><strong>每一层中</strong>：DNN是小于1的梯度的<strong>连乘</strong>，而RNN中因为$\frac{\partial S_i}{\partial S_i-1}&#x3D;\frac{\partial S_i-1}{\partial S_i-2}$的关系，所以是<strong>幂次方</strong></p>
</li>
<li><p>RNN中的梯度不会真正的消失，它是每一层梯度的和，RNN中的梯度消失主要是指：离的越远，梯度的传递越弱，所以梯度被邻近的梯度主导，导致模型难以学到远距离信息的长期依赖性问题</p>
</li>
</ul>
<h4 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h4><p>附加一点LSTM的</p>
<ul>
<li><p>在LSTM中，有很多条路径，cell state这条路径扮演的角色有点像ResNet残差连接，使前面的数据可以不受中间数据的影响传输到后面。</p>
</li>
<li><p>如果去除cell state这条路径，LSTM中该消失还是消失，该爆炸也爆炸，但是有了cell state，<em>梯度&#x3D;消失梯度+cell state的梯度</em>，我们看到如果cell state的梯度不消失，就可以一定程度上缓解梯度消失，在第一版没有遗忘门的LSTM中，这里的cell state的梯度&#x3D;1，后来加入了一个遗忘门，但是因为tanh的取值在-1和1之间，并且多数情况趋于1或者-1，所以加入遗忘门后，LSTM也可以一定程度上缓解梯度消失问题。</p>
</li>
</ul>
<h5 id="为什么说是缓解呢"><a href="#为什么说是缓解呢" class="headerlink" title="为什么说是缓解呢"></a>为什么说是缓解呢</h5><ul>
<li><p>我们不能保证每一层都有前面传过来的残差连接，即我们不能保证哪几层cell state的门就一定是0（怎么解释有待再详细思考）</p>
</li>
<li><p>cell state趋于1或者-1，且有不趋于1和-1的时候，这个时候梯度也是小于1的数</p>
</li>
<li><p><strong>那如果每一层的门的数值不一样，是不是也会导致$\frac{\partial S_i}{\partial S_i-1} \neq \frac{\partial S_i-1}{\partial S_i-2}$？所以这里其实也不是幂次方的关系？</strong></p>
</li>
</ul>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><h3 id="梯度爆炸"><a href="#梯度爆炸" class="headerlink" title="梯度爆炸"></a>梯度爆炸</h3><ol>
<li><p>梯度爆炸一般的解决方法是使用gradient clipping(梯度裁剪)，即通过设置梯度的值域范围来限制梯度过大。</p>
</li>
<li><p>L2正则化：$Loss&#x3D;(y-W^tx)^2+\alpha \cdot \Vert W \Vert^2$，如果权重过大，可以通过Loss的减小来一定程度控制W过大。</p>
</li>
</ol>
<h3 id="梯度消失"><a href="#梯度消失" class="headerlink" title="梯度消失"></a>梯度消失</h3><p>在神经网络中，梯度消失出现的更多一些，也更难解决。</p>
<ol>
<li><p>用tanh取代sigmoid激活函数。因为tanh导数的范围是-1到1，相对sigmoid的导数0到$\frac{1}{4}$，tanh可以相对sigmoid在一定程度缓解梯度消失</p>
</li>
<li><p>使用relu激活函数，当x&gt;0时，relu导数是1。但是觉得他会引起一些其他问题，并且在x&gt;0时为线性函数，所以有点失去了使用激活函数的意义，上面已经提到过了</p>
</li>
<li><p>batchnorm(简称BN)非常复杂，以后专门弄一篇研究一下。在反向传播中$\delta’(z) \cdot w_i$有W的存在，我们通过强行拉回偏离的W的分布到均值为0方差为1的正太分布上，消除了W带来的放大和缩小的问题，来一定程度缓解梯度消失（梯度爆炸）的问题</p>
<p> 有文章说这里是通过BN改变激活函数的输入，使其落在激活函数比较敏感的区域，让输入的微小变化造成Loss较大的变化来缓解梯度消失，这一点还要详细思考一下。</p>
<p> a. 我们用BN对数据进行缩放，强行改变数据输入，是否会对模型有影响？</p>
<p> b. BN中，如果batch size过小，会导致数据不准确，过大会对内存有要求</p>
<p> c. 这里norm的是输入x还是权重w？：两个都不是，而是激活函数的输入$Wx+b$</p>
</li>
<li><p>残差结构：LSTM中cell state的应用，就类似于残差结构，可以一定程度上让在序列前面的信息不受影响的（较完整的）传输到后面的序列中</p>
</li>
<li><p>谨慎选择随机初始化权重，如xavier初始化</p>
</li>
</ol>
<h3 id="备注"><a href="#备注" class="headerlink" title="备注"></a>备注</h3><p>非常感谢<a href="https://zhuanlan.zhihu.com/p/76772734">这篇文章</a>，很全面的解释了很多我的疑惑。</p>
<h3 id="hexo-markdown"><a href="#hexo-markdown" class="headerlink" title="hexo-markdown"></a>hexo-markdown</h3><ol>
<li><p>又一个新发现，在markdown中，如果使用<code>$</code>，公式会和字在同一层展示，如果使用<code>$$</code>，公式会自己单独用一行展示。如下：</p>
<ul>
<li><code>$</code>：$y&#x3D;f(x)$</li>
<li><code>$$</code>：$$y&#x3D;f(x)$$</li>
</ul>
</li>
<li><p>尝试了很多插入图片的方法。只有复制github上图片连接最好用，这样在markdown，首页和文章中都可以正确显示图片。如果只是在<code>_config.yml</code>中修改<code>post_asset_folder:true</code>，再使用相对路径来显示图片，那么图片只能在文章首页正确显示，但是不能在文章中正确显示，并且在markdown中的相对路径和在网页上的，不一定是一样的。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
      <tags>
        <tag>LSTM</tag>
        <tag>RNN</tag>
        <tag>梯度消失</tag>
        <tag>梯度爆炸</tag>
        <tag>DNN</tag>
      </tags>
  </entry>
  <entry>
    <title>ChitChat任务</title>
    <url>/2021/08/19/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/ChitChat%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何<ol>
<li>闲聊原本数据是120万左右，后来经过数据清洗后，剩下80万左右的数据。</li>
<li>数据清洗：<ol>
<li>删除重复对话</li>
<li>将多轮对话剪切成单轮对话</li>
<li>删除回复过短的回答，或者不需要的回答&#x2F;问题（如 字符，笑脸符等）</li>
<li>删除过长对话（30字以内）</li>
<li>统一回复（如 哈哈哈哈哈哈 &#x3D;&gt; 哈哈哈哈哈，如 我不知道啊，哈哈哈 &#x3D;&gt; 我不知道啊）、</li>
<li>用规则过滤了一遍脏话</li>
</ol>
</li>
</ol>
</li>
</ol>
<p><img src="/image.jpg"></p>
<span id="more"></span>

<ol>
<li><p>背景如何</p>
</li>
<li><p>模型如何</p>
<ol>
<li>“问候”和“再见”等，是用规则写的，大致过滤一遍输入</li>
<li>如果没有匹配到“问候”或者“再见”，就会进入到模型生成回复</li>
<li>模型是seq2seq+attention</li>
<li>attention是哪个attention，对结果影响并不大</li>
<li>把数据分为几块，比如5-10个字的句子一起，10-20个字的句子一起，等等，然后让句子从短到长进行输入，一是让训练更稳定，二是能一定程度上加快速度</li>
</ol>
</li>
<li><p>准确率如何</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>无意义回答过多 &#x3D;&gt; 加入anti-LM</li>
<li>相似回答过多 &#x3D;&gt; 加入余弦相似到loss中，有一定的效果，但是很难统计，因为可能对某个测试集，效果明显，对某些测试集，效果就不是那么明显</li>
<li>简单问题正确回答的百分比 &#x3D;&gt; “问候”等对话进行了规则处理</li>
<li>过于简短回答的百分比 &#x3D;&gt; 数据集上进行了处理</li>
</ol>
<p>还有一个隐藏问题，生成式模型，一般都有training和inference时，输入不匹配的问题，即training时输入的是正确答案的编码，inference输入的是模型的上一步产生的输入<br>解决方法：</p>
<ol>
<li>只用输出作为下一步的输入，很难训练出语句通顺的句子，因此可以将两种方式交替训练</li>
<li>引入对齐模型alignment model。通过计算输入位置s和输出位置t的匹配程度，获取一个匹配程度分布的矩阵（attention），ast计算了在当前状态t，相对于前一个状态t-1，不同encode中的hs的重要程度。此矩阵衡量了输出单词t和单词s的对齐程度。上下文向量ct来源于ast，表示t时刻，encoder根据对齐程度得到的期望上下文向量</li>
</ol>
</li>
<li><p>怎么解决（trick或者模型）</p>
</li>
<li><p>为什么这么解决</p>
</li>
<li><p>效果怎么样<br>闲聊的评估是带有非常大的主观性的，而且场景不同，用户不同，对相同结果的满意度也会不一样，所以我们对模型的评估希望从更客观、更基础的地方来进行评估和改进：</p>
<ol>
<li>无意义回答的频率&#x2F;百分比</li>
<li>相似回答的频率&#x2F;百分比</li>
<li>正确回答的百分比</li>
<li>简单问题正确回答的百分比</li>
<li>相同问题，是否能给出相同答案</li>
<li>过于简短回答的百分比</li>
<li>对话内容大多无意义，无法深入交谈具体事情</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li>模型优化</li>
<li>数据集质量优化+数据集扩充</li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
</li>
</ol>
<p>闲聊：</p>
<ol>
<li>SF的闲聊中使用的是人工评价指标</li>
<li>虽然ppl被认为是可以评估生成的文本，但其实效果并不是特别好<ol>
<li>Perplexity的影响因素（这些是听报告了解的）：<ol>
<li>训练数据集越大，PPL会下降得更低，1billion dataset和10万dataset训练效果是很不一样的；</li>
<li>数据中的标点会对模型的PPL产生很大影响，一个句号能让PPL波动几十，标点的预测总是不稳定；</li>
<li>预测语句中的“的，了”等词也对PPL有很大影响，可能“我借你的书”比“我借你书”的指标值小几十，但从语义上分析有没有这些停用词并不能完全代表句子生成的好坏。</li>
</ol>
</li>
<li>需要具体了解一下ppl，出自哪里，怎么计算，如何评价生成的文本而非模型</li>
<li>可以看一下上次做的ppt</li>
</ol>
</li>
<li>20年google出了另一种评价指标BLEURT</li>
<li><a href="https://cloud.tencent.com/developer/news/468354">人机对话关键技术及挑战</a><br>文章提到了好几个闲聊重的优化技术，比如learning to start for sequence to sequence,Retrieval-Enhanced Adversarial Training For Neural Response Generation, etc.</li>
</ol>
<p>好好研究一下<a href="https://www.infoq.cn/profile/90B7FCCBE83037/publish">滴滴技术</a></p>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
  </entry>
  <entry>
    <title>DM任务</title>
    <url>/2021/08/19/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/DM%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何</li>
<li>背景如何</li>
<li>模型如何</li>
<li>准确率如何</li>
<li>碰到哪些问题</li>
<li>怎么解决（trick或者模型）</li>
<li>为什么这么解决</li>
<li>效果怎么样</li>
<li>还有哪些可以改进的</li>
<li>现有哪些更好的方法吗（新的论文）</li>
</ol>
<span id="more"></span>

<p>对话机器人中碰到的问题：</p>
<ol>
<li>解决语言的多样性和歧义性问题</li>
<li>槽位模块中如何提高抽取模型的复用性</li>
<li>解决实体消歧问题</li>
<li>上下文理解</li>
<li>场景可移植性</li>
<li>多轮（ner可以加到多轮里，比如学生教学场景，可以聊ner和dm结合rasa的设计）</li>
</ol>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
  </entry>
  <entry>
    <title>FAQ</title>
    <url>/2019/12/08/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/FAQ/</url>
    <content><![CDATA[<h1 id="FAQ"><a href="#FAQ" class="headerlink" title="FAQ"></a>FAQ</h1><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>为了降本增效，我们规划了八大任务型场景，并以FAQ为辅，构建智能在线客服。总场景覆盖率是87.6%，其中FAQ覆盖率为16.7%</p>
<h2 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h2><p>｜顶层分类器｜ &#x3D;&gt; ｜FAQ｜+｜任务型场景｜+｜闲聊｜</p>
<span id="more"></span>

<h2 id="FAQ-1"><a href="#FAQ-1" class="headerlink" title="FAQ"></a>FAQ</h2><p>出去任务型囊括的几大场景外，还有一些并不足以构建场景，但是用户会有疑问的一些问题，我们通过FAQ进行回答。</p>
<h3 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h3><p>总共大概9W数据，3W原始数据，5.6W扩充数据，总共800多个类别。扩充数据大部分是通过人工术语扩充，即针对一个标准问题，我们要扩充为不同的句式问法。<br>测试集在扩充数据以前，按分布取的8000样本。</p>
<p>（吴恩达的那个测试集数量决定的方法，在这里不太实用，因为数据不够，所以直接取25%也是不错的选择，5%是照顾一下样本少的类别）</p>
<h3 id="清洗"><a href="#清洗" class="headerlink" title="清洗"></a>清洗</h3><p>连续的数字统一清洗为连续的0，单位也要统一</p>
<p>长短句：</p>
<ul>
<li>有不少长句是因为地址过长，所以我们会提取很长的地址，来做数据增广，让模型更多的学会句式，而忽略长地址的影响</li>
<li>有不少长句是因为用户不满，抱怨的句子，这时需要通过顶层分类器，或者任务型场景里的分类器，分到“抱怨”这个类别，然后转人工</li>
<li>还有一些长句，我们会人工进行压缩，只取有用的部分</li>
</ul>
<p>问题：</p>
<ul>
<li>很多非常长的句子，哪怕不是抱怨，我们也会转人工</li>
<li>几次说一个问题，或者对上一个问题追问，我们在FAQ中目前无法解决。比如“我要寄快递”这句话我们回复完了后，如果用户再说一句“加急”，如果是在FAQ中，我们会不能够回答“加急”，会让用户再次组织语言，或者转人工</li>
</ul>
<h3 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h3><p>扩充数据大部分是通过人工术语扩充，即针对一个标准问题，我们要扩充为不同的句式问法。</p>
<ul>
<li>句式&#x2F;问法扩充</li>
<li>句子顺序交换</li>
<li>过采样</li>
</ul>
<h3 id="词向量"><a href="#词向量" class="headerlink" title="词向量"></a>词向量</h3><p>使用的是jieba分词（jieba分词的逻辑是什么样的？效果为什么这么好？）</p>
<p>经过比较后，我们选用公司内部领域内的词向量，效果要比word2vec要好，OOV会少一些，而且不允许词向量微调效果要更好。词向量的效果也比字向量的好。</p>
<p>什么模型中比较的结果？每个模型都是这个比较结果吗？</p>
<ul>
<li>初步设置了一个baseline，用的孪生网络 LSTM + cosine similarty</li>
<li>在这个模型中比较的结果，然后应用到后面的模型中</li>
</ul>
<p>word2vec向量如何得到的</p>
<ul>
<li>有 skip-gram 和 CBOW 两种</li>
</ul>
<p>训练加速方法的逻辑是什么</p>
<ul>
<li>负采样<ul>
<li>取一个正样本，5个负样本，计算得到正样本和不得到负样本的概率</li>
<li>负样本是通过词频大小来取的</li>
<li>num_word&#x2F;total_num_word 来得到每个词的词频占比，然后在[0, 1]中随机选择数字</li>
</ul>
</li>
<li>分层 softmax<ul>
<li>哈夫曼树</li>
<li>最小路径权重（词频）</li>
</ul>
</li>
</ul>
<p>用的什么训练</p>
<ul>
<li>gensim.word2vec</li>
</ul>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>800多个类是怎么分的？</p>
<ul>
<li>人工分的</li>
<li>很大程度上依赖于答案，所以业务同事直接给的类别和答案</li>
</ul>
<p>数据是怎么样的？</p>
<ul>
<li>ID，标准样本，答案，类别</li>
<li>样本最后都按ID以向量的形式储存，选定匹配的序号后，可以直接从取出向量计算</li>
<li>只需要把用户的输入重新过一遍网络来获取他的向量</li>
</ul>
<p>用了哪些模型</p>
<ul>
<li>SVM</li>
<li>LSTM + attention，[a, a_chapeau, a-a_chapeau, a*a_chapeau] + LSTM + CNN &#x3D;&gt; “分类” and “fc &#x3D;&gt; similary”</li>
<li>am-softmax + LSTM</li>
</ul>
<p>分别效果如何</p>
<ul>
<li>am-softmax + LSTM 的效果最好</li>
<li>SVM次之</li>
<li>LSTM + attention 最差</li>
</ul>
<p>为什么</p>
<ul>
<li>SVM本身分类效果就很好，配合if-idf + word2vec的特征提取，泛化效果特别好，但是需要大量内存和时间</li>
<li>为了加强句子的匹配，我们尝试了更复杂的模型</li>
<li>LSTM + attention模型过拟合很严重，而且泛化能力较弱，分类效果没有SVM好，最后匹配结果的准确度也下降很多</li>
<li>最后我们尝试了 word2vec + am-softmax + LSTM，既可以分类，也可以匹配，还可以进行多标签分类，结果也达到了期望的效果<ul>
<li>我们真正要做的是特征提取模型</li>
<li>用分类作为训练方案</li>
<li>最后对模型提取的特征进行对比排序</li>
<li>使用 triplet-loss 增加的类间距，能更好的找到阈，很好的增加了直答的比例，但是对于一些多标签的分类，</li>
</ul>
</li>
</ul>
<p>有没有别的模型的想法？还有哪些可以改进的地方？</p>
<p>重难点是什么？</p>
<ul>
<li>SVM虽然分类效果很好，但是可以用来做匹配的信息过少（只有 word embedding），或者需要重新设计一个模型计算相似度，比较复杂</li>
<li>第二种方法更适合文本的直接匹配，不适合先分类+匹配的做法，且计算量很大</li>
<li>am-softmax 主要是在正负样本的选择上</li>
</ul>
<p>如何进行的错误样本分析？</p>
<p>是否引入了熵</p>
<ul>
<li>二分类器，也可以引入熵</li>
<li>二分类器，还可以是多标签的</li>
</ul>
<p>如何处理多意图问题&#x2F;引入了多标签吗？sigmoid</p>
<ul>
<li>本身是一个二分类问题</li>
<li>在计算的过程中，就是看当前样本是正样本还是负样本</li>
<li>所以相当于有800多个二分类器？？？</li>
</ul>
<p>整体准确率要达到多少？</p>
<p>是否使用了学习曲线</p>
<p>有没有过拟合</p>
<ul>
<li>有</li>
</ul>
<p>loss如何计算的</p>
<ul>
<li>用d_an,d_ap,y，计算出am-softmax</li>
</ul>
<p>是否用了shuffle</p>
<ul>
<li>用了</li>
</ul>
<p>dropout多少</p>
<ul>
<li>0.4（原文0.5）</li>
</ul>
<p>为什么加入高斯噪音？</p>
<ul>
<li>model文件里Model的self.noise</li>
</ul>
<p>triplet-loss里的距离为什么用欧式距离？</p>
<ul>
<li>计算两个句子的距离用的是欧式距离</li>
<li>triplet文件里TripletLoss的dist_mat&#x3D;euclidean_dist</li>
</ul>
<p>如何计算相似度的？</p>
<ul>
<li>先用欧式距离计算d_an,d_ap，加入y计算他们的am-softmax</li>
<li>相似度就是用的欧式距离</li>
</ul>
<p>还可以怎样计算相似度</p>
<ul>
<li>内积</li>
<li>余弦</li>
<li>曼哈顿</li>
<li>多层感知器网络（MLP）</li>
</ul>
<p>为什么相似度用的欧式距离？有没有尝试过别的方法？</p>
<ul>
<li>最先尝试的内积和余弦，但是效果没有欧式距离好</li>
<li>后来也尝试过曼哈顿</li>
</ul>
<h3 id="效果"><a href="#效果" class="headerlink" title="效果"></a>效果</h3><p>覆盖率，成功率，减少客服80%咨询时间，平均降低人工咨询市场50s</p>
<h3 id="难点"><a href="#难点" class="headerlink" title="难点"></a>难点</h3><p>样本不均衡</p>
<ul>
<li>数据增广</li>
<li>采样<ul>
<li>过采样：复制 + SMOTE过采样 + 加入噪音等生成数据</li>
<li>欠采样：删除数据 + 将多样本类的样本分为N分，只取每份的中心数据作为样本</li>
</ul>
</li>
<li>loss权重调节</li>
<li>组合集成方法，组合多个分类器，每个分类器每个类的数据是随机分成的<strong>少数类数据的量</strong></li>
<li>特征选择（CNN其实就是做了特征选择）</li>
</ul>
<p>准确率如何提升</p>
<ul>
<li>错误样本分析<ul>
<li>数据清洗，特征提取，比如“？”</li>
</ul>
</li>
<li>改善模型</li>
<li>改善loss使用am-softmax</li>
<li>引入熵</li>
<li>多标签分类</li>
</ul>
<p>多意图分类+错分类，如何解决？</p>
<ul>
<li>多标签分类</li>
<li>使用sigmoidcrossentropywithlogits</li>
<li>引入熵</li>
<li>引入am-softmax</li>
</ul>
<p>过拟合</p>
<ul>
<li>dropout</li>
<li>early stopping</li>
<li>shuffle</li>
</ul>
<p>上线后的问题？</p>
<ul>
<li>新的术语，</li>
<li>不存在FAQ里的问题，</li>
<li>闲聊，</li>
<li>抱怨，</li>
<li>多轮（信息分几次发送）</li>
<li>未登录词</li>
</ul>
<h3 id="还有哪些可以改进的点"><a href="#还有哪些可以改进的点" class="headerlink" title="还有哪些可以改进的点"></a>还有哪些可以改进的点</h3><ul>
<li>多轮，这是个大问题（在FAQ也做信息提取，保存信息）</li>
<li>更好的做长句的扩充和数据收集，让模型对长句的泛化能力更好，比如对长句的判别（加attention+cnn）</li>
<li>阈值问题，并不一定会游泳，但是会想尝试使用交叉法来看一下结果</li>
</ul>
<h3 id="自己觉得骄傲的点有哪些"><a href="#自己觉得骄傲的点有哪些" class="headerlink" title="自己觉得骄傲的点有哪些"></a>自己觉得骄傲的点有哪些</h3>]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>智能客服</tag>
        <tag>CNN</tag>
        <tag>stacking</tag>
        <tag>LR</tag>
        <tag>SVM</tag>
      </tags>
  </entry>
  <entry>
    <title>FAQ任务</title>
    <url>/2021/08/19/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/FAQ%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><h3 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h3><ol>
<li>数据如何<br>10W数据，1600个标准问（5.6w数据是自己造的），每个标准问对应n个相似问<br>最多的能有两千左右的相似样例，较少的只有上百个</li>
</ol>
<p>数据不是标注，是本来业务就已经有的归类数据</p>
<span id="more"></span>

<ol start="2">
<li><p>背景如何</p>
<ul>
<li>一句话进入到系统后，先用faq进行判别，如果是比如「我要寄快递」这个标准问，那么就转入到多轮，如果是其他的faq中对应问题，有两种可能，一种是特别肯定，就直接回复，一种是不太肯定，就推荐top3进行。</li>
<li>faq做的就是对所有问题的初步判断</li>
<li>（这里和vipkid里就不一样了，因为vipkid里用的是bert模型，很难判断第一个回复是不是和第二个回复差距很大，最开始设置的差别是0.0001，后来改用意图分类来进行筛选，如果意图不能判断，再给到faq，提高直答的准确率）</li>
</ul>
</li>
<li><p>模型如何<br>期间尝试过很多模型，用stacking，使用svm，使用双塔模型，使用lstm，最后发现lstm效果最好（在回复比率相当的情况下，直答率最高，较svm高12个百分点，直答准确率要低1个百分点）</p>
<ol>
<li>双塔模型+triplet loss</li>
</ol>
<ul>
<li>其实双塔模型效果也还不错，尤其是优化了triplet loss之后，我们使用正负样本三元组，对于每一个样本，我们取最远的k个正样本，和最近的k个负样本，然后使用triplet loss进行计算</li>
<li>loss中的距离，尝试了常用的余弦距离和欧式距离，发现欧式距离效果更好。</li>
<li>每5个epoch更新一次距离最近的正样本和负样本，直答率确实提升了3%，但是训练时的整体性能下降很严重，对线上影响不大，线上还是过一个模型计算出embedding，然后找出最相似的句子即可（召回+精排，性能允许，直接精排，但是其实可以加一个召回）</li>
</ul>
<ol start="2">
<li>使用am softmax</li>
</ol>
<ul>
<li>为了更好的区分类间距离，我们使用了am softmax，softmax指数右上角本来应该是x^cos(delta)，改为s(cos(delta)-m)，m就是为了增加目标类别间的距离，s是为了控制让好的样本有一个更高的梯度，来进一步缩小类内间的距离</li>
<li>相比triplet loss而言，am softmax再能达到相同效果的情况下，更节省算力</li>
</ul>
<ol start="3">
<li>两者相对简单的使用lsmt效果都有2%左右的提升，但是资源耗费不同</li>
</ol>
<ul>
<li>triplet loss是通过计算相似度训练出得embedding，因此应该更适用于相似度的计算</li>
<li>这里最后还是需要进行分类的，加上am softmax更节省算力，在效果几乎相同的情况下，我们选择am softmax</li>
</ul>
</li>
<li><p>准确率如何<br>最后结果，直答准确率，85.9%，直答率91.3%，默认回复比率2.3%</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>也会有数据问题<ol>
<li>最大的类别有700多个样本，最小的类别只有几十个样本</li>
<li>我们做了数据增广，过采样加上数据增强</li>
</ol>
</li>
<li>召回率和精排效果的提升<ol>
<li>主要是对模型的尝试，希望从模型方面更好的提升直答率和直答准确率</li>
<li>最初用了baseline是svm模型（svm模型对数据样本的分布并不敏感，只取最贴近边界的样本计算loss）</li>
<li>改用lstm后直答率提升了12%到89%，但是准确率下降了1%</li>
<li>改用am softmax整体效果又提升了，增加了阈值之后，直答准确率85.9%，直答率91.3%</li>
</ol>
</li>
<li>某些样本很难区分<ol>
<li>所以尝试了triplet loss和am softmax，针对性解决问题</li>
<li>都有一定的效果提升</li>
</ol>
</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li></li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
</li>
<li><p>碰到哪些问题</p>
</li>
<li><p>样本不平衡 —— 数据增强</p>
</li>
<li><p>提升直答率和精排的准确率 —— 模型的改进</p>
</li>
<li><p>还有哪些可以改进的</p>
</li>
<li><p>现有哪些更好的方法吗（新的论文）<br>SBert</p>
</li>
</ol>
<h3 id="VIPKid"><a href="#VIPKid" class="headerlink" title="VIPKid"></a>VIPKid</h3><ol>
<li><p>数据如何</p>
<ol>
<li>总共2w数据，200个类别</li>
<li>因为用的三元组重新构造的数据，某个样本有多个正样本和负样本。我们在当前类中，匹配当前问题坐在类别中的其他问题为正样本，随机选择其他类的问题为负样本，最后获得80w的三元组数据组对</li>
<li></li>
</ol>
</li>
<li><p>背景如何</p>
<ol>
<li>在intent不能够区分当前语句的意图时，faq进行处理并进行兜底</li>
</ol>
</li>
<li><p>模型如何</p>
<ol>
<li>最开始用的bert模型<ol>
<li>在对数据进行三元组处理以后，就讲数据放入bert中的nsp，进行而分类匹配，这个为基础模型，但是也不是特别差，F1值能到89.6%</li>
</ol>
</li>
<li>因为NSP初衷不是用来分类的，所以我们尝试使用sbert模型，只能bert来做一个embedding的提取<ol>
<li>先是只取的cls的embedding，尝试了triplet loss等通过相似度计算loss的方法，最后发现triplet loss效果最好</li>
<li>之后又我们比较了cls的embedding，第一层输出的embedding，最后一层token的embedding，然后计算相似度进行训练，发现最后一层token的embedding进行concat后计算相似度效果最好，能够再提升1.5个百分点的效果</li>
<li>最后效果是93%</li>
</ol>
</li>
<li></li>
</ol>
</li>
<li><p>准确率如何</p>
</li>
<li><p>碰到哪些问题</p>
</li>
<li><p>怎么解决（trick或者模型）</p>
</li>
<li><p>为什么这么解决</p>
</li>
<li><p>效果怎么样</p>
</li>
<li><p>还有哪些可以改进的</p>
</li>
<li><p>现有哪些更好的方法吗（新的论文）<br>3w数据，200个类，最开始是直接上的bert模型，然后使用nsp来预测两个句子是否是同一类，即对每个类别做个二分类。<br>效果不是特别好，89%<br>分析是bert的NSP并不适合分类，因为我们只是微调，没有重新训练bert，之前都是判别两个句子是否是前后文，与我们的情况不同<br>所以我们尝试使用sbert模型。其实这个模型的主题思想是使用bert来产生embedding，然后在对每个句子的embedding进行softmax，triplet loss等各种尝试处理，最后发现普通的triplet loss效果就不错，能够让f1值达到93%，等于是优化了解码的embedding</p>
</li>
</ol>
<p>Sbert<br>学习曲线<br>bert结果两集分化，如何平衡<br>triplet loss<br>am softmax</p>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
  </entry>
  <entry>
    <title>INTENT任务</title>
    <url>/2021/08/19/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/INTENT%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li>数据如何</li>
</ol>
<h4 id="SF"><a href="#SF" class="headerlink" title="SF"></a>SF</h4><p>共有15000条数据，10个类别（如「信息」，「询问到达时间」，「询问费用」，「询问到达时间——是否」，「询问费用——是否」，「肯定」，「否定」，「」等） + 1个「others」类<br>其中将近5000是数据增强的</p>
<p>样本最多的类别others，样本数可达5000<br>样本最少的类别比如「否定」，样本数只有100多（最后两少的三个类别，取了三倍过采样，过多和过少都会降低F1值）</p>
<span id="more"></span>

<ol start="2">
<li><p>背景如何<br>比如其中一个多轮场景是「寄快递」</p>
</li>
<li><p>模型如何<br>baseline上的是rnn<br>后来改为cnn<br>也同bert比较了，bert出了收敛较cnn更快一些外，最终效果并不比cnn好</p>
</li>
<li><p>准确率如何<br>最后F1达到95%，达到业务要求</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>F1值不够95%<ol>
<li>数据增强</li>
<li>错误样本分析（尤其对准确率很低的类别，针对行进行分析）<ol>
<li>我们通过错误样本分析，找到了几个准确率特别低的样本，进行过采样，提升了效果</li>
</ol>
</li>
</ol>
</li>
<li>样本不均衡的问题<ol>
<li>低样本的三个类别进行了三倍过采样</li>
<li>数据增强（尝试了使用bert的mask进行数据增强）</li>
</ol>
</li>
<li>多轮之间通过意图跳转<ol>
<li>尝试过使用熵来替代概率，效果不好</li>
<li>最后通过调节weight，然后跟踪整体准确率和召回率的变化，提前中断训练，使得others类的准确率能达到99%（因为在调大others的weight后，训练会优先照顾这个类别，因为一开始，others的召回特别低，但是准确率特别高，然后在整体F1值慢慢上升后，others的准确率会开始下降，这时需要终止训练，这里牺牲了整体1个百分点的F1值）</li>
</ol>
</li>
<li>曾经出现过一个句子分属多个类别的情况<ol>
<li>进行数据梳理后，发现两个类别可以分开。对这两个类别的数据重新进行梳理，准确率大幅提升</li>
</ol>
</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li>对于分属多个类别的语句，是否也可以尝试进行多标签分类</li>
<li>更可以改进的是DM的设置<ol>
<li>每个多轮都有一个others，比较累赘，说明每个多轮都需要找过others的准确率，而影响整体的准确率</li>
<li>是否可以通过记住当前多轮所在状态和所在的bot，然后将再此进入多轮的句子进行更精确的分类和进入判别，这样就不用每个多轮里面都加一个others类别了</li>
</ol>
</li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
<ol>
<li>对于分类，cnn和rnn有时候就很够用了，再不行上bert也差不多，然后就是数据的问题了，模型帮助不大</li>
<li>对于对话机器人更好的想法，如上5.2.2</li>
</ol>
</li>
</ol>
<h4 id="VIPKid"><a href="#VIPKid" class="headerlink" title="VIPKid"></a>VIPKid</h4><ol>
<li>数据如何</li>
<li>背景如何</li>
<li>模型如何</li>
<li>准确率如何</li>
<li>碰到哪些问题</li>
<li>怎么解决（trick或者模型）</li>
<li>为什么这么解决</li>
<li>效果怎么样</li>
<li>还有哪些可以改进的</li>
<li>现有哪些更好的方法吗（新的论文）</li>
</ol>
<h2 id="重难点"><a href="#重难点" class="headerlink" title="重难点"></a>重难点</h2><h3 id="Bert数据增强"><a href="#Bert数据增强" class="headerlink" title="Bert数据增强"></a>Bert数据增强</h3><p>分类碰到的问题：</p>
<ol>
<li>使用某个模型前，也可以先做一个学习曲线，判断使用的样本是否足够</li>
<li>进行错误样本分析</li>
<li>调节weight，优化不同类别的presicion</li>
<li>如果某一类别的F1值过低，可以看下是否是样本不足的原因</li>
<li>做数据增强</li>
<li>如果是两个类别经常混淆，可以先将其归为一类，之后在下一层，用规则或模型进行细分<ol>
<li>对下一层分类起，是模型加针对性的数据增强</li>
<li>这里数据增强有用，但是整体数据增强，不一定能增加准确率，比如“蓝屏”，在这里是“noshow”的意思，但是在别的地方有可能是指机器故障（看下数据，找个好点的例子）</li>
</ol>
</li>
<li>叠加分类器</li>
</ol>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
  </entry>
  <entry>
    <title>Kaggle使用和打比赛注意事项</title>
    <url>/2022/07/12/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/Kaggle%E4%BD%BF%E7%94%A8%E5%92%8C%E6%89%93%E6%AF%94%E8%B5%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/</url>
    <content><![CDATA[<h1 id="Kaggle使用和打比赛注意事项"><a href="#Kaggle使用和打比赛注意事项" class="headerlink" title="Kaggle使用和打比赛注意事项"></a>Kaggle使用和打比赛注意事项</h1><h2 id="使用Kaggle上的Notebook训练"><a href="#使用Kaggle上的Notebook训练" class="headerlink" title="使用Kaggle上的Notebook训练"></a>使用Kaggle上的Notebook训练</h2><p>Kaggle上可以免费使用GPU进行模型训练，但输出的数据有保存时效。</p>
<p>可以点击右侧的 <code>Add data</code> &gt; <code>Dataset</code> 上传本地数据，或者 <code>Add data</code> &gt; <code>Competition Data</code> 中获取kaggle上已有的比赛数据</p>
<p>上传数据时，可以一次性upload所有所需数据，输入框 <code>Enter Dataset Title</code> 是为此次上传的所有文件建立的文件夹的名字</p>
<p><img src="/2022/07/12/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/Kaggle%E4%BD%BF%E7%94%A8%E5%92%8C%E6%89%93%E6%AF%94%E8%B5%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/uploaddata.png" alt="upload data"></p>
<p>需注意：</p>
<ol>
<li><p>文件夹中的“-”，”.“等符号会被忽略，所以在代码中引用文件夹名时需注意</p>
</li>
<li><p>如果有重复的数据（如vacab.txt会和平台上默认已有的bert_code里的数据重复），完成时会让选择 <code>create without duplicates</code> 新建文件夹不包含重复数据或 <code>include duplicates</code> 包含重复数据。推荐第二个。</p>
<p> <img src="/2022/07/12/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/Kaggle%E4%BD%BF%E7%94%A8%E5%92%8C%E6%89%93%E6%AF%94%E8%B5%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/duplicate.png" alt="add data"></p>
</li>
<li><p>进行一次<code>save version</code>后，dataset里的数据会消失，需要点击<code>add data</code>，选择<code>your dataset</code>中的数据<code>add</code>重新加进来   </p>
<p><img src="/2022/07/12/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/Kaggle%E4%BD%BF%E7%94%A8%E5%92%8C%E6%89%93%E6%AF%94%E8%B5%9B%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9/adddata.png" alt="add data"></p>
</li>
<li><p>注意具体检查数据数目，确保数据无误</p>
</li>
</ol>
<h2 id="上传比赛结果"><a href="#上传比赛结果" class="headerlink" title="上传比赛结果"></a>上传比赛结果</h2><p>如果要使用notebook上传比赛结果，则需要满足以下步骤和条件：</p>
<ol>
<li>一定保存一版代码： <code>Save Version</code> 并且选择 <code>Save &amp; Run All(Commit)</code></li>
<li>保存和运行前，需要先把数据和模型加载到Data中去，具体操作步骤见上文</li>
<li>运行前需要断开<code>Internet</code>，否则保存后的代码不能被选用上传</li>
<li>要保证运行的代码，有对测试集的预测并按格式输出文件</li>
<li>输出的文件，列名和行数都需要分别和 <code>submission.csv</code> 和 <code>test.csv</code> 保持一致</li>
<li>等程序运行完成后，进入到当前比赛页面下并进入<code>Leaderboard</code></li>
<li>点击<code>Submit Predictions</code>后，选择上面步骤中保存的版本 <code>version X</code> 和输出文件 <code>submission.csv</code></li>
</ol>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>kaggle</tag>
      </tags>
  </entry>
  <entry>
    <title>NER任务</title>
    <url>/2021/08/17/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/NER%E4%BB%BB%E5%8A%A1/</url>
    <content><![CDATA[<h2 id="项目叙述"><a href="#项目叙述" class="headerlink" title="项目叙述"></a>项目叙述</h2><ol>
<li><p>数据如何<br>共有15000条数据，12个类别（如「出发地」，「出发时间」，「类型」，「物品」，「体积」等）</p>
</li>
<li><p>背景如何<br>比如其中一个多轮场景是「寄快递」</p>
</li>
</ol>
<span id="more"></span>

<ol start="3">
<li><p>模型如何<br>先是上的baseline，然后针对问题进行改进的（最开始尝试intent和ner一起做）<br>baseline使用的是lstm+crf，因为一般ner的问题，都是编码获取的信息不够，所以后来上了bilstm+crf，准确率提升了1.5个百分点，也尝试使用bert，发现bert效果并不比bilstm好。之后</p>
</li>
<li><p>准确率如何<br>最后F1值83%</p>
</li>
<li><p>碰到哪些问题</p>
<ol>
<li>整体F1值问题<ol>
<li>F1值不是特别高，刚开始只能到达80%左右</li>
<li>ner一般都是embedding不够丰富，所以我们尝试了bilstm和bert，两个效果差不多，但是bilstm更轻便，能够提升1.5%的F1值</li>
<li>后来又跑了一下当时刚出的lattice lstm，效果确实还是不错的，相对bilstm能提升2个百分点，当然，后来在vipkid又尝试了FLAT，效果更好，相对lattice lstm，还能再提升2个百分点</li>
<li>不同解码方式</li>
<li>CRF&#x2F;指针网络&#x2F;Biaffine</li>
<li>不同解码方式差异有限</li>
</ol>
</li>
<li>有的类别预测不准，有的特别准<ol>
<li>样本不均衡<ol>
<li>本身标注的样本，就会有些不均衡，所以自己按规律造了5000数据</li>
<li>当然，测试集里是原始数据</li>
<li>有一定的效果提升</li>
</ol>
</li>
<li>同样量的样本，有的类别比较难预测（规律难找）<ol>
<li>有的类别虽然样本也不算少，但是本身规律比较难找，比如「明天」，「明天下午两点」，实体变化很大，所以效果不如意</li>
<li>这些特殊类别，我们主要是通过规则加上开源工具来兜底</li>
<li>比如地址，我们本身有很全的字典，如果模型预测概率小于某个阈值，我们就尝试用字典进行匹配</li>
<li>比如时间，我们用开源工具进行兜底</li>
</ol>
</li>
<li>召回不高<ol>
<li>丰富embedding</li>
<li>检查是否有漏标的数据（标注数据的噪音去除）</li>
</ol>
</li>
<li>嵌套实体&#x2F;多种类别实体</li>
<li>实体span过长（规则，引入指针网络）</li>
</ol>
</li>
</ol>
</li>
<li><p>还有哪些可以改进的</p>
<ol>
<li>一个是可以从embedding方面进行改进</li>
<li>另一个是嵌套实体&#x2F;多类别实体的问题解决</li>
</ol>
</li>
<li><p>现有哪些更好的方法吗（新的论文）</p>
</li>
</ol>
<h2 id="重难点"><a href="#重难点" class="headerlink" title="重难点"></a>重难点</h2><h3 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h3><ul>
<li>基于「词」的NER会受到分词的影响。比如需要预测的句子中，目标词汇被错误划分从而导致结果错误。所以目前NER主要还是基于「字」做NER，然后尽量加入「词」信息</li>
<li>LSTM接CRF：在decoder时，对每一步的输出h，进行vitterbi编码</li>
</ul>
<h3 id="Viterbi"><a href="#Viterbi" class="headerlink" title="Viterbi"></a>Viterbi</h3><p>维特比是一个动态规划算法。<br>我们需要找到最大路径，通常需要求出每一层的每一种路径的概率。<br>但是在维特比算法中，我们每次取当前层，节点个数（即可能的状态个数）的路径（每个节点取，到当前层、当前节点，概率最大的路径），因此可以减少很多运算</p>
<h3 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h3><ul>
<li>CRF位条件概率（判别模型）</li>
<li>HMM是联合概率（生成模型），且有独立假设</li>
</ul>
<h3 id="Lattice-LSTM"><a href="#Lattice-LSTM" class="headerlink" title="Lattice LSTM"></a>Lattice LSTM</h3><p>Lattice LSTM在编码时在相应的字信息处加入了词信息，词信息一样有input, cell state，但是没有hidden state(output)，将词信息的cell state和字信息的cell state进行加权求和，获取新的hidden state的计算数据，并未改变原来的decoder结构和维度，因此后面也可以直接接CRF。并且因为加入了词信息，因此准确率会有提升。<br>SF中，准确率提升2%，但是速度会下降一倍。</p>
<h3 id="FLAT"><a href="#FLAT" class="headerlink" title="FLAT"></a>FLAT</h3><ul>
<li>计算了相对位置的四种distance</li>
<li>使用XLNet中attention score的计算方式，来计算相对距离的attention score</li>
</ul>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>ner</tag>
        <tag>lattice</tag>
        <tag>FLAT</tag>
      </tags>
  </entry>
  <entry>
    <title>Chitchat</title>
    <url>/2019/01/16/%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/chitchat/</url>
    <content><![CDATA[<h2 id="Chitchat"><a href="#Chitchat" class="headerlink" title="Chitchat"></a>Chitchat</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p>chitchat持续的时间很长了。每次进步不大，但是至少都在前进。因为持续时间长，每次想法和改动的地方比较多，也是学到了不少东西，想要把这些都记录下来，所以开了这个文档。</p>
<p>chitchat 目前已经自己改进到v1.5了。虽然每一次改动的内容并不大，但是每一次都是一个尝试。</p>
<span id="more"></span>

<h4 id="到目前为止，每个version的改进简介"><a href="#到目前为止，每个version的改进简介" class="headerlink" title="到目前为止，每个version的改进简介"></a>到目前为止，每个version的改进简介</h4><ul>
<li><p>v1.1 是所有以